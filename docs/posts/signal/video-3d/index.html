<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>3D Reconstruction Theory | Home</title>
<meta name="keywords" content="Image Processing, Computer Vision">
<meta name="description" content="Course notes, for personal study and review only.">
<meta name="author" content="Zehua">
<link rel="canonical" href="http://localhost:1313/posts/signal/video-3d/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.3ee48d1db0e8fac5e1cfe277964548e9cae374a212e9a8a76e9e007dde708ede.css" integrity="sha256-PuSNHbDo&#43;sXhz&#43;J3lkVI6crjdKIS6ainbp4Afd5wjt4=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/signal/video-3d/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
    integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
    integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
    crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
    integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
    crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            
            
            delimiters: [
                { left: '$$', right: '$$', display: true },
                { left: '$', right: '$', display: false },
                { left: '\\(', right: '\\)', display: false },
                { left: '\\[', right: '\\]', display: true }
            ],
            
            throwOnError: false
        });
    }); 
</script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap" rel="stylesheet"><meta property="og:title" content="3D Reconstruction Theory" />
<meta property="og:description" content="Course notes, for personal study and review only." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/posts/signal/video-3d/" />
<meta property="og:image" content="http://localhost:1313/images/papermod-cover.png" />
<meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-10-21T16:25:17+01:00" />
<meta property="article:modified_time" content="2024-11-12T17:12:35+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="http://localhost:1313/images/papermod-cover.png" />
<meta name="twitter:title" content="3D Reconstruction Theory"/>
<meta name="twitter:description" content="Course notes, for personal study and review only."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Signal processing",
      "item": "http://localhost:1313/posts/signal/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "3D Reconstruction Theory",
      "item": "http://localhost:1313/posts/signal/video-3d/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "3D Reconstruction Theory",
  "name": "3D Reconstruction Theory",
  "description": "Course notes, for personal study and review only.",
  "keywords": [
    "Image Processing", "Computer Vision"
  ],
  "articleBody": "1. Image Projection Model and 3D Reconstruction Theory 1.1. Concept of SLAM SLAM (Simultaneous Localization and Mapping).\nBy estimating each camera’s position and the scene’s 3D points, achieve scene reconstruction.\n1.2. Reverse 2D Image Extract 3D scene information from a 2D image, i.e., perform $3D$ reconstruction.\nInverse Projection\nThe image is a 2D representation of the 3D scene after projection. To recover 3D information, it is necessary to reverse this projection process.\nEstablish a mathematical model describing how the 3D scene is projected onto the 2D image, then attempt to solve it inversely.\n1.3 Pinhole Camera Model 1.3.1 Model Overview Definition: The pinhole camera model assumes that all light rays pass through a common point, the optical center.\nAdvantages: Simple model, easy to compute inversely, widely used in 3D reconstruction. 1.3.2 Coordinate Systems and Notation Conventions Camera Coordinate System: Origin $O_C$: Optical center, coordinates $(0, 0, 0)$.\nAxis Directions: Establish a right-handed coordinate system, $X_C$ to the right, $Y_C$ downward, $Z_C$ pointing backward (scene depth direction).\nAdvantage: The $Z$ axis points backward, object depth is positive, which is intuitive.\n1.3.3 Projection of 3D Points to Normalized Focal Plane 3D Point Representation:\nPoint $U$: Coordinates $(U_X, U_Y, U_Z)$, representing a 3D point in space.\nNormalized Focal Plane:\nA plane at a distance of $1$ from the optical center $O_C$ ($Z_C = 1$), called the normalized focal plane.\nProject distant 3D points onto this plane.\n1.3.4 Homogeneous Coordinates and Inhomogeneous Coordinates Homogeneous Coordinates:\nDefinition: Adding an extra dimension (usually $1$) to the original coordinates to facilitate projection and transformation.\nRepresentation: For a 2D point $m = (m_X, m_Y)^T$, its homogeneous coordinates are $\\bar{m} = (m_X, m_Y, 1)^T$.\nPurpose: Homogeneous coordinates facilitate matrix operations, especially during projection and transformation processes.\nInhomogeneous Coordinates:\nStandard Cartesian coordinate representation, without extra dimensions.\n1.4 Linear Calibration of the Camera 1.4.1 From Normalized Focal Plane to Image Plane Image Plane:\nCoordinate System: Pixel coordinate system, typically with the image’s top-left corner as the origin, $X$ axis to the right (column index), $Y$ axis downward.\nPurpose: Map points on the normalized focal plane to actual image pixel coordinates.\nLinear Transformation:\nTransformation Formula:\n$$\\begin{cases} P_U = f \\cdot m_X + U_0 \\\\ P_V = f \\cdot m_Y + V_0 \\end{cases}$$ Focal Length $f$\n$m_X$, $m_Y$ are points on the normalized focal plane\nOptical Center Coordinates on the Image Plane $(U_0, V_0)$\n1.4.2 Camera Intrinsic Matrix Represent the above linear transformation in matrix form. $$K = \\begin{pmatrix} f \u0026 0 \u0026 U_0 \\\\ 0 \u0026 f \u0026 V_0 \\\\ 0 \u0026 0 \u0026 1 \\end{pmatrix}$$ Matrix Mapping Relationship: $$\\underline{P} = K \\cdot \\underline{m}$$ Where, $\\underline{P}$ is the homogeneous coordinates of points on the image plane.\n1.4.3 Inverse Process From Image Plane to Normalized Focal Plane: $$\\underline{M} = K^{-1} \\cdot \\underline{P}$$ 1.4.4 Viewing Frustum Represents the spatial range that the camera can see. By converting the four corner points of the image to the normalized focal plane and then connecting them to the optical center, forming a viewing frustum. 1.5 Distortion Modeling and Correction 1.5.1 Sources of Camera Distortion Optical defects in actual camera lenses, especially in wide-angle lenses, causing image distortion, straight lines becoming curved, image edges stretched or compressed. 1.5.2 Distortion Model Distortion Function:\nApply a distortion function to ideal points on the normalized focal plane (from ideal image to distorted image) to obtain distorted points, which are actual $2D$ points on the distorted focal plane.\n$$\\underline{m}_d = d(\\underline{m}, k)$$ Where, $k$ are distortion parameters.\nExample: Polynomial Radial Distortion Model $$M_d = \\left(1 + k_1 \\|m\\|_2^2 + k_2 \\|m\\|_2^4 + \\dots \\right) m$$ Where: $|m|_2^2 = m_x^2 + m_y^2$\n1.6 Implementation of Distortion Correction 1.6.1 Task Description Goal: Correct the distorted actual image to an ideal undistorted image. 1.6.2 Implementation Steps Define Parameters: Ideal camera intrinsic matrix $K_{\\text{ideal}}$; distorted camera intrinsic matrix $K_{\\text{real}}$; distortion parameters $k$.\nFor each ideal image pixel coordinate, perform the following steps:\n​\t1.\tConvert pixel coordinates to normalized focal plane:\n$$\\underline{m} _{\\text{ideal}} = K_{\\text{ideal}}^{-1} \\cdot \\underline{P} _{\\text{ideal}}$$ ​\t2.\tApply distortion function:\n$$\\underline{m}_d = d(\\underline{m} _{\\text{ideal}}, k)$$ ​\t3.\tMap back to actual image coordinate system:\n$$\\underline{P} _{\\text{real}} = K_{\\text{real}} \\cdot \\underline{m}_d$$ ​\t4.\tInterpolation:\nPerform interpolation on $\\underline{P} _{\\text{real}}$ (since coordinates may be non-integer, possibly use bilinear interpolation)\n​\t5.\tGenerate corrected image\n2. 2D Rigid Transformation and Homography 2.1 2D Rigid Transformation 2D rigid transformations include translation and rotation.\n2.1.1 Rotation $$\\mathbf{U}^c = \\overrightarrow{O _c U}^c \\quad \\mathbf{U}^w = \\overrightarrow{O _w U}^w$$ $$\\mathbf{R} _{wc} \\underline{\\mathbf{U}}^c = \\mathbf{R} _{wc} \\cdot \\overrightarrow{O _c U}^c = \\overrightarrow{O _w U}^w$$ Select a vector from one reference frame and then transform it to another coordinate frame.\n$\\mathbf{R} _{wc}$ is an orthogonal matrix.\n2.1.2 Translation $$\\mathbf{T} _{wc} = \\overrightarrow{O _w O _c}^{w}$$ 2.1.3 Rigid Transformation Formula $$\\mathbf{U}^w = \\mathbf{R} _{wc} \\cdot \\mathbf{U}^c + \\mathbf{T} _{wc}$$ Proof:\n$$\\mathbf{R} _{wc} \\cdot \\mathbf{U}^c + \\mathbf{T} _{wc} = \\mathbf{R} _{wc} \\cdot \\overrightarrow{O _c U}^c + \\overrightarrow{O _w O _c}^w = \\overrightarrow{O _c U}^w + \\overrightarrow{O _w O _c}^w = \\overrightarrow{O _w U}^w = \\mathbf{U}^w$$ 2.1.4 Homogeneous Coordinates: $$\\underline{\\mathbf{U}}^w = \\begin{bmatrix} \\mathbf{U}^w \\\\ 1 \\end{bmatrix}$$ $$\\mathbf{M} _{wc} = \\begin{bmatrix} \\mathbf{R} _{wc} \u0026 \\mathbf{T} _{wc} \\\\ 0 \u0026 1 \\end{bmatrix} = \\begin{bmatrix} r _{11} \u0026 r _{12} \u0026 r _{13} \u0026 t _{x} \\\\ r _{21} \u0026 r _{22} \u0026 r _{23} \u0026 t _{y} \\\\ r _{31} \u0026 r _{32} \u0026 r _{33} \u0026 t _{z} \\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\end{bmatrix}$$ 2.1.5 Inverse Transformation: $$\\mathbf{M} _{cw} = \\mathbf{M} _{wc}^{-1}$$ 2.1.6 Composability of Transformations: $$\\mathbf{M} _{ab} \\cdot \\mathbf{M} _{bc} = \\mathbf{M} _{ac}$$ 2.2 Homography 2.2.1 Plane Scene Assumption $$\\mathbf{U} _i^A = z _i^A \\cdot \\underline{\\mathbf{m}} _{Ai}$$ This equation means that the point $\\mathbf{U} _i ^ A$ can be represented by $\\underline{\\mathbf{m}} _{Ai}$ by multiplying its depth, since $\\underline{\\mathbf{m}} _{Ai}$ has unit depth.\n2.2.2 Finding the Correspondence Between $\\underline{\\mathbf{m}} _{Ai}$ and $\\underline{\\mathbf{m}} _{Bi}$ With only this equation, how do we find the correspondence between $\\underline{\\mathbf{m}} _{Ai}$ and $\\underline{\\mathbf{m}} _{Bi}$? In simple terms, how do we perform coordinate correspondence transformations?\n​\t1.\tKey Formula for the Normal\nWe need to recall a property to obtain a key formula involving the normal and the plane.\nIn reference frame $A$, the equation of plane $P$ is: $ax + by + cz + d = 0$, where $a, b, c$ are the components of the plane’s normal vector, and $d$ is a constant representing the relative distance between plane $P$ and the origin $O _A$.\nIn vector form, the plane equation can be simplified to:\n$$\\mathbf{n} _A^T \\mathbf{U} _i^A + d = 0$$ $\\mathbf{n} _A^T$ represents the normal vector of plane $P$ in reference frame $A$.\nThrough the vector form of the plane equation, we obtain a very important formula involving the normal vector.\n​\t2.\tObtaining the Depth Expression Using Variable Substitution\nSubstitute $\\mathbf{U} _i^A = z _i^A \\cdot \\underline{\\mathbf{m}} _{A,i}$ into the above equation:\n$$\\mathbf{n} _A^T \\cdot z _i^A \\cdot \\underline{\\mathbf{m}} _{A,i} + d = 0 \\quad \\Rightarrow \\quad z _i^A = -\\dfrac{d}{\\mathbf{n} _A^T \\cdot \\underline{\\mathbf{m}} _{A,i}}$$ In this way, we have introduced $\\underline{\\mathbf{m}} _{Ai}$, where $z _i^A = -\\dfrac{d}{\\mathbf{n} _A^T \\cdot \\underline{\\mathbf{m}} _{A,i}}$ represents the depth. In other words, by using the two equations of $\\mathbf{U} _i^A$, we have replaced $\\mathbf{U} _i^A$, thus obtaining the depth $z _i^A$. However, this still does not solve the problem $\\Rightarrow$ that is to say, having only the equation related to $\\underline{\\mathbf{m}} _{Ai}$ is not enough; we also need to approach from $\\underline{\\mathbf{m}} _{Bi}$.\n​\t3.\tNext, We Find the Point $\\underline{\\mathbf{m}} _{Bi}$ in Coordinate Frame $B$\nStarting from the rigid transformation formula $\\mathbf{U}^w = \\mathbf{R} _{wc} \\cdot \\mathbf{U}^c + \\mathbf{T} _{wc}$, we can see that projecting from $c$ to $w$ only requires transforming $\\mathbf{U}^c$. In other words, to obtain $\\underline{\\mathbf{m}} _{Bi}$, we only need to perform a rigid transformation on $\\underline{\\mathbf{m}} _{Ai}$.\n$$\\underline{\\mathbf{m}} _{B,i} = \\Pi \\left( \\mathbf{R} _{BA} \\mathbf{U} _i^A + \\mathbf{t} _{BA} \\right)$$ where $\\Pi(\\cdot)$ is the projection function.\n$$\\underline{\\mathbf{m}} _{B,i}= \\Pi \\left( \\mathbf{R} _{BA} \\left( -\\dfrac{d}{\\mathbf{n} _A^T \\cdot \\underline{\\mathbf{m}} _{A,i}} \\right) \\cdot \\underline{\\mathbf{m}} _{A,i} + \\mathbf{t} _{BA} \\right)$$ Multiply both sides of the above equation by $-\\dfrac{\\mathbf{n} _A^T \\cdot \\underline{\\mathbf{m}} _{A,i}}{d}$:\n$$\\underline{\\mathbf{m}} _{B,i}= \\Pi \\left( \\mathbf{R} _{BA} \\cdot \\underline{\\mathbf{m}} _{A,i} - \\dfrac{\\mathbf{n} _A^T \\cdot \\underline{\\mathbf{m}} _{A,i}}{d} \\cdot \\mathbf{t} _{BA} \\right)$$ $$\\underline{\\mathbf{m}} _{B,i} = \\Pi \\left( \\left( \\mathbf{R} _{BA} - \\dfrac{\\mathbf{t} _{BA} \\cdot \\mathbf{n} _A^T}{d} \\right) \\cdot \\underline{\\mathbf{m}} _{A,i} \\right)$$ This establishes the correspondence between point $A$ and point $B$ on their respective normalized planes.\nQuestion: In the above formula, why does multiplying both sides by $-\\dfrac{\\mathbf{n} _A^T \\cdot \\underline{\\mathbf{m}} _{A,i}}{d}$ not change the equation?\nThe projection function $\\Pi(\\cdot)$ is a scale-invariant operation (it only considers direction and relative position, not absolute scale). Therefore, even if we multiply the right side by $-\\dfrac{\\mathbf{n} _A^T \\cdot \\underline{\\mathbf{m}} _{A,i}}{d}$, it does not affect the condition for the equality to hold because the projection result remains the same.\n2.2.3 Finding the Correspondence Between $\\underline{\\mathbf{P}} _{A,i}$ and $\\underline{\\mathbf{P}} _{B,i}$ We know:\n$$\\left\\{ \\begin{aligned} \\underline{\\mathbf{m}} _{A,i} = K _A^{-1} \\cdot \\underline{\\mathbf{P}} _{A,i}\\\\ \\underline{\\mathbf{m}} _{B,i} = K _B^{-1} \\cdot \\underline{\\mathbf{P}} _{B,i} \\end{aligned} \\right.$$ $\\underline{\\mathbf{P}} _{B,i} = K _B \\cdot \\underline{\\mathbf{m}} _{B,i} \\Rightarrow$ Substitute the obtained $\\underline{\\mathbf{m}} _{B,i}$:\n$$\\underline{\\mathbf{P}} _{B,i} = K _B \\cdot \\Pi \\left( \\left( \\mathbf{R} _{BA} - \\dfrac{\\mathbf{t} _{BA} \\cdot \\mathbf{n} _A^T}{d} \\right) \\cdot \\underline{\\mathbf{m}} _{A,i} \\right)$$ $$\\underline{\\mathbf{P}} _{B,i} = K _B \\cdot \\Pi \\left( \\left( \\mathbf{R} _{BA} - \\dfrac{\\mathbf{t} _{BA} \\cdot \\mathbf{n} _A^T}{d} \\right) \\cdot K _A^{-1} \\cdot \\underline{\\mathbf{P}} _{A,i} \\right)$$ Recall the property:\n$$K \\cdot \\Pi \\left( \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} \\right) = \\Pi \\left( K \\cdot \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} \\right)$$ Using this property, we get:\n$$\\underline{\\mathbf{P}} _{B,i} = \\Pi \\left( K _B \\cdot \\left( \\mathbf{R} _{BA} - \\dfrac{\\mathbf{t} _{BA} \\cdot \\mathbf{n} _A^T}{d} \\right) \\cdot K _A^{-1} \\cdot \\underline{\\mathbf{P}} _{A,i}\\right)$$ 2.2.4 Obtaining the Homography Matrix $\\mathbf{H} _{AB}$ Assume:\n$$\\mathbf{H} _{AB} = K _B \\cdot \\left( \\mathbf{R} _{BA} - \\dfrac{\\mathbf{t} _{BA} \\cdot \\mathbf{n} _A^T}{d} \\right) \\cdot K _A^{-1}$$\nTherefore:\n$$\\left\\{ \\begin{aligned} \u0026\\underline{\\mathbf{P}} _{B,i} = \\Pi \\left( \\mathbf{H} _{BA} \\cdot \\underline{\\mathbf{P}} _{A,i} \\right) \\quad \\quad A \\Rightarrow B\\\\ \u0026\\underline{\\mathbf{P}} _{A,i} = \\Pi \\left( \\mathbf{H} _{BA}^{-1} \\cdot \\underline{\\mathbf{P}} _{B,i} \\right) = \\Pi \\left( \\mathbf{H} _{AB} \\cdot \\underline{\\mathbf{P}} _{B,i} \\right) \\quad \\quad B \\Rightarrow A \\\\ \\end{aligned} \\right.$$ Through the homography matrix, we can transform a point from one camera’s image coordinate system to another camera’s image coordinate system, establishing a point mapping relationship.\n2.2.5 Estimating and Solving the Homography Matrix $$\\mathbf{H} _{AB} = \\begin{bmatrix} h _1 \u0026 h _4 \u0026 h _7 \\\\ h _2 \u0026 h _5 \u0026 h _8 \\\\ h _3 \u0026 h _6 \u0026 h _9 \\end{bmatrix}$$ This is a homogeneous matrix with 9 parameters $h _1$ to $h _9$. Homogeneous matrices have redundancy in scale, leading to a loss of degrees of freedom.\nSimple Solution – Parameterization (Parameters to Estimate = Free Parameters) $$\\mathbf{H} _{AB} = \\begin{bmatrix} h _1 \u0026 h _4 \u0026 h _7 \\\\ h _2 \u0026 h _5 \u0026 h _8 \\\\ h _3 \u0026 h _6 \u0026 1 \\end{bmatrix}$$ $$\\mathbf{h} = \\begin{bmatrix} h _1 \\\\ \\vdots \\\\ h _8 \\end{bmatrix}$$ How to Estimate $\\mathbf{h}$?\nIn this case, as long as we know one corresponding point, we can determine $h _1$ to $h _8$. $$\\underline{\\mathbf{P}} _{A,i} = \\Pi \\left( \\begin{bmatrix} h _1 \u0026 h _4 \u0026 h _7 \\\\ h _2 \u0026 h _5 \u0026 h _8 \\\\ h _3 \u0026 h _6 \u0026 1 \\end{bmatrix} \\cdot \\underline{\\mathbf{P}} _{B,i} \\right)$$ Since $\\underline{\\mathbf{P}} _{A,i}$ and $\\underline{\\mathbf{P}} _{B,i}$ are homogeneous coordinates, we expand them:\n$$\\begin{bmatrix} P _{A,i,x} \\\\ P _{A,i,y} \\\\ 1 \\end{bmatrix} = \\Pi \\left( \\begin{bmatrix} h _1 \u0026 h _4 \u0026 h _7 \\\\ h _2 \u0026 h _5 \u0026 h _8 \\\\ h _3 \u0026 h _6 \u0026 1 \\end{bmatrix} \\cdot \\begin{bmatrix} P _{B,i,x} \\\\ P _{B,i,y} \\\\ 1 \\end{bmatrix} \\right)$$ $$\\left\\{ \\begin{aligned} P _{A,i,x} = \\dfrac{h _1 \\cdot P _{B,i,x} + h _4 \\cdot P _{B,i,y} + h _7}{h _3 \\cdot P _{B,i,x} + h _6 \\cdot P _{B,i,y} + 1} \\\\ P _{A,i,y} = \\dfrac{h _2 \\cdot P _{B,i,x} + h _5 \\cdot P _{B,i,y} + h _8}{h _3 \\cdot P _{B,i,x} + h _6 \\cdot P _{B,i,y} + 1} \\end{aligned} \\right.$$ $$\\left\\{ \\begin{aligned} P _{A,i,x} \\cdot \\left( h _3 \\cdot P _{B,i,x} + h _6 \\cdot P _{B,i,y} + 1 \\right) = h _1 \\cdot P _{B,i,x} + h _4 \\cdot P _{B,i,y} + h _7 \\\\ P _{A,i,y} \\cdot \\left( h _3 \\cdot P _{B,i,x} + h _6 \\cdot P _{B,i,y} + 1 \\right) = h _2 \\cdot P _{B,i,x} + h _5 \\cdot P _{B,i,y} + h _8 \\end{aligned} \\right.$$ $$\\begin{bmatrix} P _{B,i,x} \u0026 0 \u0026 -P _{A,i,x} \\cdot P _{B,i,x} \u0026 P _{B,i,y} \u0026 0 \u0026 -P _{A,i,x} \\cdot P _{B,i,y} \u0026 1 \u0026 0 \\\\ 0 \u0026 P _{B,i,x} \u0026 -P _{A,i,y} \\cdot P _{B,i,x} \u0026 0 \u0026 P _{B,i,y} \u0026 -P _{A,i,y} \\cdot P _{B,i,y} \u0026 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} h _1 \\\\ h _2 \\\\ h _3 \\\\ h _4 \\\\ h _5 \\\\ h _6 \\\\ h _7 \\\\ h _8 \\end{bmatrix} = \\begin{bmatrix} P _{A,i,x} \\\\ P _{A,i _y} \\end{bmatrix}$$ Since there are 8 unknowns, we need eight independent linear equations. Each pair of corresponding points provides two corresponding equations (i.e., equations 59), so at least four pairs of corresponding points are required. That is, four matching pairs $\\left( P _{A,i}, P _{B,i} \\right) \\quad i = 1, 2, 3, 4$ are needed.\n$\\Rightarrow \\mathbf{h}^* = \\arg\\min _{\\mathbf{h}} \\sum _{i=1}^{4} \\left\\lVert M _i \\mathbf{h} - P _{A,i} \\right\\rVert _2^2 \\Rightarrow$ Linear Least Squares\n3. Robust Homography Estimation Using the RANSAC Algorithm 3.1 Objective Image Alignment and Stitching: Achieve automatic image stitching by estimating the homography between two images. 3.2 Automatically Establishing Correspondences — SIFT Algorithm Interest Point Detection\nUse algorithms like SIFT to detect feature points in both images (this code is provided by the instructor), eliminating the need to manually label corresponding points and utilizing algorithms to automatically establish correspondences between images.\nTherefore, we can find the most similar point pairs in both images. However, note that the point pairs may not correspond correctly.\nThat is, there may be incorrect matches (outliers). In such cases, we cannot directly use the correspondences. Instead, we will use another algorithm called RANSAC to automatically assess the correctness of corresponding points and obtain the most optimal $H$ matrix to output.\n3.3 Robust Estimation Using the RANSAC Algorithm ​\t1.\tAlgorithm Concept:\nRandom Sample Consensus (RANSAC) is a robust algorithm for estimating model parameters ($H$) in the presence of outliers (incorrect points).\nIt repeatedly performs random sampling to find the best-fitting model.\n​\t2.\tRANSAC Procedure:\nRepeat $N$ times (The number of iterations is determined based on experience or computation):\n​\t1.\tRandomly Select 4 Pairs of Matching Points:\n4 is the minimum number of matching points required to estimate the homography matrix.\nRandomly select four from all matching points. Since it is uncertain which correspondences are correct, a subsequent estimation and evaluation criterion (Euclidean distance) is needed.\n​\t2.\tEstimate the Homography Matrix $H^k$:\nUse the selected 4 pairs of matching points and the $DLT$ algorithm (done in the previous experiment, its purpose and function are to estimate the homography matrix given known corresponding points) to estimate the homography matrix. ​\t3.\tCompute Errors and Evaluate the Model:\nFor all matching points (including those not selected), transform the second image’s point $P _{B _i}$ using the estimated $H^k$, i.e., $H^k P _{B _i}$.\nCalculate the Euclidean distance between the transformed point (estimated point) and the actual point $P _{A _i}$ in the first image.\nDefine the Cost Function: Use a binary kernel function $\\phi _c(d)$ (either 0 or 1):\nWhen the distance $d \u003c \\tau$, consider the match correct, and the cost is 0.\nWhen the distance $d \\geq \\tau$, consider the match incorrect, and the cost is 1.\nTotal Cost: $L^k = \\sum _{i} \\phi _c(|P _{A _i} - H^k P _{B _i}|)$\n​\t4.\tUpdate the Best Model:\nIf the current cost $L^k$ is less than the previous minimum cost $L$, update $L$ and the corresponding $H$.\nFinal Output:\nThe homography matrix $H$ with the minimum cost.\n​\t3.\tSelection of Threshold $\\tau$:\n$\\tau$ is the distance threshold to determine whether a match is an inlier. It is usually chosen based on image resolution and matching accuracy, generally between $0.5$ to $3$ pixels.\nChoosing a too large $\\tau$ increases incorrect matches, while a too small $\\tau$ may ignore correct matches.\n3.4 Why Not Use Traditional Quadratic Cost Functions Sensitivity Issues:\nQuadratic cost functions (such as least squares) are very sensitive to outliers. If a point has a large error, it will cause the cost function value to be excessively large, making the errors of other points irrelevant.\nRobustness:\nBinary kernel functions are insensitive to those extremely large or outlying points (all equal to 1), effectively suppressing the influence of outliers and making the estimation result more robust.\nOther Kernel Functions:\nBesides binary kernel functions, there are other robust kernel functions like the $Huber$ kernel and $Lorentzian$ kernel, which can balance error magnitude and robustness to some extent.\n3.5 Limitations of the RANSAC Algorithm Influence of the Number of Parameters:\nAs the number of model parameters increases, the required number of random samples grows exponentially, significantly increasing computational cost.\nApplicable Range:\nRANSAC is suitable for cases with a small number of parameters, such as line fitting, fundamental matrix estimation, and homography estimation.\n4. Epipolar Geometry in Stereo Vision\nSo far, we have studied the case of planar scenes and used homography to describe the relationship between two views. However, for general three-dimensional scenes, the planar assumption no longer holds. To address this, we introduce epipolar geometry.\n4.1 Epipolar Geometry\nEpipolar geometry can be well explained through a schematic diagram:\n​\t1.\tConsider Two Cameras Located in Reference Frames 1 and 2 Respectively\nThe optical center of Camera 1 is $O_1$, and the optical center of Camera 2 is $O_2$. A point $U$ in space is projected onto the image planes of both cameras, resulting in points $\\underline{m}_1$ and $\\underline{m}_2$.\n​\t2.\tProblem Description:\nIn general cases, we cannot make any assumptions about point $U$ (unlike the previous planar scene).\nWe need to find a method to establish a relationship between $\\underline{m}_1$ and $\\underline{m}_2$ without knowing $U$.\n4.2 Epipolar Plane and Epipolar Lines\n​\t1.\tEpipolar Plane\n$U$ and the optical centers $O_1$, $O_2$ define a plane $\\Rightarrow$ Points $m_1$, $m_2$, $O_1$, $O_2$ are coplanar $\\Rightarrow$ This plane is called the epipolar plane.\nEpipolar Constraint = Coplanarity, meaning $\\underline{m}_1$, $\\underline{m}_2$, $O_1$, $O_2$ are coplanar.\nIn stereo vision, the fundamental matrix $F$ and the essential matrix $E$ both rely on the coplanarity condition for their computation.\n​\t2.\tEpipolar Lines\nThe epipolar plane intersects the image planes of the two cameras, resulting in epipolar lines $l_1$ and $l_2$ respectively.\n$m_2$ is the projection of the three-dimensional point $U$ onto the image plane of the second camera. However, according to the constraints of epipolar geometry, $m_2$ must lie on the epipolar line $l_2$.\n$\\Rightarrow$ Given the position of point $m_1$, the corresponding epipolar line $l_2$ can be determined using the fundamental matrix $F$: $l_2 = F \\cdot m_1$.\nThe fundamental matrix $F$ captures the relative pose and intrinsic parameters between the two cameras. This formula indicates that given point $m_1$, one can compute the epipolar line $l_2$ on which $m_2$ must lie.\n4.3 Epipolar Constraint\nObjective: Utilize the above geometric relationships to formalize the epipolar constraint and establish a mathematical relationship between $m_1$ and $m_2$.\nDefine Vectors:\n$$\\left\\{ \\begin{aligned} \u0026\\mathbf{\\underline{m}_1} \\text{ is the vector from the optical center } O_1 \\text{ to the image point } \\underline{m}_1 \\quad \\overrightarrow{O_1 m_1}^{1} \\\n\u0026\\mathbf{\\underline{m}_2} \\text{ is the vector from the optical center } O_2 \\text{ to the image point } \\underline{m}_2 \\quad \\overrightarrow{O_2 m_2}^{2} \\\n\u0026\\mathbf{t_{12}} = \\overrightarrow{O_1 O_2}^{1} \\text{ is the translation vector between the two camera optical centers}\n\\end{aligned} \\right.$$\nDefine the Normal Vector of the Epipolar Plane:\n$$\\left\\{ \\begin{aligned} \u0026\\text{In reference frame } 1, \\quad \\overrightarrow{\\mathbf{n}_1}^{1} = \\underline{\\mathbf{m}}_1 \\times \\mathbf{t} _{12} \\\n\u0026\\text{In reference frame } 2, \\quad \\overrightarrow{\\mathbf{n}_2}^{2} = \\mathbf{R} _{21} \\overrightarrow{\\mathbf{n}_1}^{1}, \\text{ where } \\mathbf{R} \\text{ is the rotation matrix between the cameras}\n\\end{aligned} \\right.$$\nNote:\nHere, $\\times$ denotes the cross product operation between two vectors. The result of the cross product is a vector perpendicular to both vectors involved in the operation, with its direction determined by the right-hand rule and magnitude equal to the area of the parallelogram formed by the two vectors.\nCoordinate system transformations for the normal vector do not need to consider the translation component because unit normal vectors are not position coordinates. Direction vectors remain unchanged in magnitude during rotation and are unaffected by translation. In summary, normal vectors only consider the rotation matrix, while points need to consider both rotation and translation.\n$$\\overrightarrow{\\mathbf{n}_2}^{2} = \\mathbf{R} _{21} \\cdot \\overrightarrow{\\mathbf{n}_1}^{1} = \\mathbf{R} _{21} \\cdot \\left( \\underline{\\mathbf{m}}_1 \\times \\mathbf{t} _{12} \\right) = \\mathbf{R} _{21} \\cdot \\underline{\\mathbf{m}}_1 \\times \\mathbf{R} _{21} \\cdot \\mathbf{t} _{12}$$ Since we previously know that $\\mathbf{t} _{21} = \\mathbf{R}*{21} \\cdot \\mathbf{t} _{12}$, the above equation becomes\n$$\\overrightarrow{\\mathbf{n}_2}^{2} = \\mathbf{t} _{21} \\times \\left( \\mathbf{R} _{21} \\cdot \\underline{\\mathbf{m}}_1 \\right)$$ Recall Properties of Cross Product Operations:\n$$\\mathbf{a} \\times \\mathbf{b} = \\begin{bmatrix} a_x \\\\ a_y \\\\ a_z \\end{bmatrix} \\times \\begin{bmatrix} b_x \\\\ b_y \\\\ b_z \\end{bmatrix} = \\begin{bmatrix} a_y b_z - a_z b_y \\\\ a_z b_x - a_x b_z \\\\ a_x b_y - a_y b_x \\end{bmatrix} _{3 \\times 1} \\Rightarrow \\left[\\mathbf{a}\\right]_{\\times} = \\begin{bmatrix} 0 \u0026 -a_z \u0026 a_y \\\\ a_z \u0026 0 \u0026 -a_x \\\\ -a_y \u0026 a_x \u0026 0 \\end{bmatrix}$$ $$\\mathbf{a} \\times \\mathbf{b} = \\left[\\mathbf{a}\\right]_{\\times} \\mathbf{b} = \\begin{bmatrix} 0 \u0026 -a_z \u0026 a_y \\\\ a_z \u0026 0 \u0026 -a_x \\\\ -a_y \u0026 a_x \u0026 0 \\end{bmatrix} \\begin{bmatrix} b_x \\\\ b_y \\\\ b_z \\end{bmatrix}$$ Using the above property, we can see that the cross product operation can be transformed into matrix operations. Therefore, using the above property, we obtain:\n$$\\overrightarrow{\\mathbf{n}_2}^{2} = \\mathbf{t} _{21} \\times \\left( \\mathbf{R} _{21} \\cdot \\underline{\\mathbf{m}}_1 \\right) = \\left[ \\mathbf{t} _{21} \\right]_{\\times} \\cdot \\mathbf{R} _{21} \\cdot \\underline{\\mathbf{m}}_1$$ Because $\\overrightarrow{\\mathbf{n}_2}^{2}$ is the normal vector of $\\mathbf{m}_2$, we have\n$\\Rightarrow \\mathbf{m}_2^T \\cdot \\overrightarrow{\\mathbf{n}_2}^{2} = 0$\n$$\\mathbf{m}_2^T \\cdot \\left[ \\mathbf{t} _{21} \\right]_{\\times} \\cdot \\mathbf{R} _{21} \\cdot \\underline{\\mathbf{m}}_1 = 0$$ $$\\mathbf{m}_2^T \\cdot \\left( \\left[ \\mathbf{t} _{21} \\right]_{\\times} \\cdot \\mathbf{R} _{21} \\right) \\cdot \\underline{\\mathbf{m}}_1 = 0$$ 4.4 Essential Matrix (Matrice Essentielle)\n​\t1.\tFormula\nAssume\n$$\\mathbf{E} _{21} = \\left[ \\mathbf{t} _{21} \\right]_{\\times} \\cdot \\mathbf{R} _{21} \\quad \\Rightarrow \\quad \\text{essential matrix}$$ It contains information about the relative rotation $\\mathbf{R}$ and translation $\\mathbf{t}$ between the two cameras.\nThe original equation becomes $ \\underline{\\mathbf{m}}*2^T \\cdot \\mathbf{E} _{21} \\cdot \\underline{\\mathbf{m}}_1 = 0 $\n​\t2.\tDegrees of Freedom\n$$ 5 \\text{ degrees of freedom} \\ \\downarrow\\ 5 \\text{ DoF} \\left( \\begin{array}{c} 3 , \\mathbf{R} _{21} \\quad \\text{rotation} \\ \\quad 2 , \\mathbf{t} _{21} \\quad \\text{translation} \\end{array} \\right)\\ \\downarrow\\ \\quad \\quad | \\mathbf{t} _{21} |_2 \\quad \\text{ unknown}\n$$\nDegrees of Freedom:\n$$\\left\\{ \\begin{aligned} \u0026\\text{The rotation matrix } \\mathbf{R} \\text{ has 3 degrees of freedom} \\\n\u0026\\text{The translation vector } \\mathbf{t} \\text{ has 2 degrees of freedom (since the scale is unknown)} \\\n\u0026\\text{Therefore, } \\mathbf{E} \\text{ has 5 degrees of freedom}\n\\end{aligned} \\right.$$\nDegrees of Freedom (DoF) refer to the number of independent parameters required to describe the essential matrix. In geometry and linear algebra, DoF reflects the number of independent directions or ways a system can vary without constraints.\nThe rotation matrix has 3 degrees of freedom, describing rotation in three-dimensional space.\nThe translation vector theoretically has 3 degrees of freedom in three-dimensional space. However, in the essential matrix, the translation vector typically only considers direction, ignoring magnitude (unknown scale), thus leaving the translation vector with only 2 effective degrees of freedom, describing the direction of translation.\n4.5 Fundamental Matrix (Matrix Fundamental)\nContinuing the transformation of the above formula:\n$$\\underline{\\mathbf{m}}_2^T \\cdot \\mathbf{E} _{21} \\cdot \\underline{\\mathbf{m}}_1 = 0$$ Given:\n$$\\left\\{ \\begin{aligned} \\underline{\\mathbf{m}}_2 = K^{-1} \\cdot \\underline{\\mathbf{P}}_2 \\\n\\underline{\\mathbf{m}}_1 = K^{-1} \\cdot \\underline{\\mathbf{P}}_1\n\\end{aligned} \\right.$$\n$$\\underline{\\mathbf{P}}_2^T \\cdot (K^{-1})^T \\cdot \\mathbf{E} _{21} \\cdot K^{-1} \\cdot \\underline{\\mathbf{P}}_1 = 0$$ When the camera intrinsics are unknown or not considered, we introduce a fundamental matrix $\\mathbf{F}$ to encapsulate $K$.\nAssume $\\mathbf{F} _{21} = (K^{-1})^T \\cdot \\mathbf{E} _{21} \\cdot K^{-1}$\n$$\\mathbf{F} _{21} : \\text{ fundamental matrix} \\quad \\Rightarrow \\quad 7 \\text{ DoF}\\quad \\left\\{ \\begin{aligned} \u0026 \\text{- homogeneous matrix} \\\n\u0026 \\text{- rank}(\\mathbf{F} _{21}) = 2 \\quad \\Rightarrow \\quad \\det(\\mathbf{F} _{21}) = 0\n\\end{aligned} \\right.$$\nProperties:\n$$\\left\\{ \\begin{aligned} \u0026\\text{Homogeneous: The fundamental matrix } \\mathbf{F} \\text{ is a homogeneous matrix, and it can be scaled by any non-zero scalar without changing its properties} \\\n\u0026\\text{Rank Constraint:} \\mathbf{F} \\text{ has rank } 2\n\\end{aligned} \\right.$$\nThe original equation becomes $ \\underline{\\mathbf{P}}*2^T \\cdot \\mathbf{F} _{21} \\cdot \\underline{\\mathbf{P}}_1 = 0 $\n$$\\text{Let:} \\quad \\mathbf{L}_2 = \\mathbf{F} _{21} \\cdot \\underline{\\mathbf{P}}_1 = \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix}$$ $$\\underline{\\mathbf{P}}_2^T \\cdot \\mathbf{L}_2 = 0 \\quad \\Leftrightarrow \\quad a P_{2,x} + b P_{2,y} + c = 0$$ This is the equation of a line on Camera 2’s image plane $\\Rightarrow$ Epipolar Line\n4.6 Estimation of Essential and Fundamental Matrices\nCamera Calibrated $\\Rightarrow$ Estimate the essential matrix $\\mathbf{E}$ (5 degrees of freedom) $\\Rightarrow$ 5-point correspondence algorithm\nCamera Uncalibrated $\\Rightarrow$ Estimate the fundamental matrix $\\mathbf{F}$ (7 degrees of freedom) $\\Rightarrow$ 7-point correspondence algorithm\nSolution $\\Rightarrow$ 8-point correspondence algorithm $\\Rightarrow$ Intentionally ignoring the constraint $\\det(\\mathbf{F}) = 0$\n4.7 Algorithm\nSteps of the 8-Point Correspondence Algorithm:\n​\t1.\tCollect Corresponding Point Pairs:\nAlthough $\\mathbf{F}$ has 7 degrees of freedom, the algorithm ignores the rank-2 constraint. Therefore, at least 8 pairs of corresponding points are required to estimate $\\mathbf{F}$.\n​\t2.\tConstruct a Linear System of Equations\nFor each pair of corresponding points $(\\mathbf{m}_1, \\mathbf{m}_2)$, construct the equation:\n$$\\underline{\\mathbf{m}}_2^T \\cdot \\mathbf{E} _{21} \\cdot \\underline{\\mathbf{m}}_1 = 0$$ $$\\underline{\\mathbf{P}}_2^T \\cdot \\mathbf{F} _{21} \\cdot \\underline{\\mathbf{P}}_1 = 0$$ ​\t3.\tSolve:\nRepresent the system of equations as: $\\underline{\\mathbf{P}}_2^T \\cdot \\mathbf{L}_2 = 0$\n​\t4.\tRANSAC Algorithm Steps\nHandle outliers (incorrect matches) in the set of corresponding points and robustly estimate $\\mathbf{F}$.\nRandom Sampling: Use the 8-point correspondence algorithm to estimate $\\mathbf{F}$.\nModel Evaluation: Use the estimated $\\mathbf{F}$ to calculate the epipolar constraint error for all corresponding point pairs, i.e., the distance from each point to its corresponding epipolar line.\nDetermine Inliers: Based on a set distance threshold, determine which corresponding points are inliers.\nIteration: Repeat the above process until the model with the highest number of inliers is found.\n5. Bundle Adjustment\nBundle adjustment is a technique that simultaneously optimizes camera parameters (including position, orientation, and intrinsic parameters) and the positions of three-dimensional points in the scene.\nIts core idea is to make the optimized model more consistent with actual observations by minimizing the re-projection error of three-dimensional points onto the images.\nRemember these five words: Minimize Re-projection Error\n5.1 Case of Two Cameras\n5.1.1 Data\n$$\\left( P_{A,i}, P_{B,i} \\right) _{i=1,\\dots,N} \\implies N \\text{ correspondences}$$\n5.1.2 Parameters to Estimate\nCamera poses and the three-dimensional point cloud data set.\n$$\\mathbf{R} _{W1} \\quad \\mathbf{t} _{W1} \\quad \\mathbf{R} _{W2} \\quad \\mathbf{t} _{W2} \\quad \\left\\{\\mathbf{U}^w_i \\right\\} _{i=1,\\dots,N}$$ 5.1.3 Loss Function\n$$\\mathcal{L} \\left( \\mathbf{R} _{w1}, \\mathbf{t} _{w1}, \\mathbf{R} _{w2}, \\mathbf{t} _{w2}, \\left\\{ \\mathbf{U}^w_i \\right\\} _{i=1,\\dots,N} \\right) = \\sum_{i=1}^{N} \\left( \\left\\lVert P_{1,i} - K_1 \\Pi \\left( \\mathbf{R} _{w1}^T \\mathbf{U}_i^{w} - \\mathbf{R} _{w1}^T \\mathbf{t} _{w1} \\right) \\right\\rVert_2^2 + \\left\\lVert P_{2,i} - K_2 \\Pi \\left( \\mathbf{R} _{w2}^T \\mathbf{U}_i^{w} - \\mathbf{R} _{w2}^T \\mathbf{t} _{w2} \\right) \\right\\rVert_2^2 \\right)$$ Where:\n$K_A$ and $K_B$ are the intrinsic matrices of cameras $A$ and $B$, respectively.\n$\\Pi(\\cdot)$ is the projection function that projects three-dimensional points onto a two-dimensional plane.\n$\\mathbf{R} _{w1}^T$ and $\\mathbf{R} _{w2}^T$ are equivalent to $\\mathbf{R} _{1w}$ and $\\mathbf{R} _{2w}$, which transform points from the world coordinate system to the camera coordinate system.\n$\\mathbf{R} _{w1}^T \\mathbf{t} _{w1}$ is equivalent to $\\mathbf{t} _{1w}$, representing the translation vector.\n$\\mathbf{U}_i^{1} = \\mathbf{R} _{w1}^T \\cdot \\mathbf{U}_i^{w} - \\mathbf{R} _{w1}^T \\cdot \\mathbf{t} _{w1}$, which transforms $\\mathbf{U}_i^{w}$ to $\\mathbf{U}_i^{1}$, i.e., from the world coordinate system to the camera coordinate system.\nDifference: The image coordinates in camera $A$ or $B$ (actual) minus the estimated image coordinates obtained through three-dimensional space rotation and transformation equals the re-projection error.\n5.2 Case of Multiple Cameras\n5.2.1 Data\nDetected points in each image are:\n$$\\left\\{ \\left\\{ P_{m,i} \\right\\} _{i=1,\\dots,N_m} \\right\\} _{m=1,\\dots,M}$$ These points form tracks across different viewpoints.\nPoints detected by the $m$-th camera, where $N_m$ is the number of points detected by the $m$-th camera.\n$$\\left\\{ \\text{p2d-id}_m, \\ \\text{p3d-id}_m \\right\\} _{m=1,\\dots,M}$$ Where:\n$\\text{p2d-id}_m$ is the index of the two-dimensional point in the image.\n$\\text{p3d-id}_m$ is the index of the corresponding three-dimensional point in the point cloud.\nBoth have dimensions of $C_m \\times 1$.\n5.2.2 Parameters to Estimate\nCamera Extrinsics: $$ \\left\\{ \\left( \\mathbf{R} _{wm}, \\mathbf{t} _{wm} \\right) \\right\\} _{m=1,\\dots,M} $$ Positions of Three-Dimensional Points: $$ \\left\\{ \\mathbf{U}_i^{w} \\right\\} _{i=1,\\dots,N} $$ 5.2.3 Loss Function\nThe cost function extends to calculate errors across all cameras and all detected points, minimizing the distance between projected points and actual observed points:\n$$\\mathcal{L}(x) = \\sum_{m=1}^{M} \\sum_{c=1}^{C_m} \\left\\| P_{m,\\ \\text{p2d-id}_m(c)} - K_m \\Pi \\left( \\mathbf{R} _{wm}^\\top \\mathbf{U} _{\\text{p3d-id}_m(c)}^{w} - \\mathbf{R} _{wm}^\\top \\mathbf{t} _{wm} \\right) \\right\\|_2^2$$ $C_m$ is the number of observations for the $m$-th camera.\n$\\mathbf{U} _{\\text{p3d-id}_m(c)}^{w}$ is the three-dimensional point corresponding to the observation.\nWe can simply simplify the above cost function to:\n$$\\mathcal{L}(x) = \\sum_{i=1}^{N} \\left\\| f_i(x) \\right\\|_2^2 \\quad \\left\\{ \\begin{array}{l} x \\in \\mathbb{R}^D \\\\ f_i : \\mathbb{R}^D \\rightarrow \\mathbb{R}^B \\end{array} \\right.$$ $x$ represents all parameters to be optimized (camera parameters and three-dimensional point coordinates).\n$f_i(x)$ is the $i$-th residual function, representing the re-projection error of the $i$-th observation.\nOur goal is to find $x$ that minimizes $\\mathcal{L}(x)$. This is a non-linear least squares problem, typically solved using iterative methods.\n5.3 Gauss-Newton Algorithm\nAn iterative optimization algorithm used for non-linear least squares problems. $\\Rightarrow$ Iterative $\\quad \\delta_{k+1} = \\delta_k + d_k$ ​\t1.\tLinearization of $f_i$:\n$$f_i(x_k + d_k) \\approx f_i(x_k) + \\mathbf{J}_i(x_k) \\cdot d_k$$ $\\delta x$ is the increment of the parameters to be solved.\nFor each iteration, we perform a Taylor expansion of $f_i(x)$ around the current estimate $x_k$ and ignore higher-order terms.\n$f_i(x_k + d_k) \\in \\mathbb{R}^B$\n$f_i(x_k) \\in \\mathbb{R}^B$\n$\\mathbf{J}_i(x_k) \\in \\mathbb{R}^{B \\times D}$\n$d_k \\in \\mathbb{R}^D$\n​\t2.\tJacobian Matrix:\n$$\\mathbf{J}_i(x_k) = \\frac{\\partial f_i(x_k + d_k)}{\\partial d_k} \\bigg|_{d_k=0}$$ Represents the partial derivatives of the function $f_i$ with respect to $d_k$ at the point $x_k$.\nDescribes the linear rate of change of the function $f_i$ at the point $x_k$.\n​\t3.\tLinear Least Squares:\n$$ L_k(d_k) = \\sum_{i=1}^{N} \\left| f_i(x_k) + \\mathbf{J}_i(x_k) \\cdot d_k \\right|_2^2 $$\n$$\\quad \\mathbf{J}_k = \\begin{bmatrix}\n\\quad J_1(x_k) \\\n\\quad J_2(x_k) \\\n\\quad J_3(x_k) \\\n\\vdots \\\n\\quad J_N(x_k)\n\\end{bmatrix} \\quad \\mathbf{b}_k =\n\\begin{bmatrix}\n\\quad f_1(x_k) \\\n\\quad f_2(x_k) \\\n\\quad \\vdots \\\n\\quad f_N(x_k)\n\\end{bmatrix}$$\n$\\mathbf{b}_k$ is the combination of all residuals.\nThe linear least squares problem becomes:\n$$ L_k(d_k) = \\lVert \\mathbf{b}_k + \\mathbf{J}_k \\cdot d_k \\rVert_2^2\n$$\nBy minimizing $L_k(d_k)$, we obtain the linear system of equations:\n$$\\mathbf{J}_k^\\top \\cdot \\mathbf{J}_k \\cdot d_k = -\\mathbf{J}_k^\\top \\cdot \\mathbf{b}_k \\quad $$ Here, the left matrix $\\mathbf{J}_k^T \\mathbf{J}_k$ is an approximation of the Hessian matrix, and the right vector $-\\mathbf{J}_k^T \\mathbf{b}_k$ is the negative of the gradient.\nSolving this linear system yields the parameter update $d_k$.\n​\t4.\tLevenberg-Marquardt Algorithm\nIntroduces a damping factor $\\lambda$ to the Gauss-Newton algorithm, allowing the optimization process to exhibit the fast convergence of Gauss-Newton when close to the solution and the stability of gradient descent when far from the solution.\nCommonly used for iterative optimization in non-linear least squares problems.\nObjective Function:\n$$L_k(d_k) = \\lVert \\mathbf{b}_k + \\mathbf{J}_k d_k \\rVert_2^2 + \\lambda \\lVert d_k \\rVert_2^2 \\quad \\quad \\Rightarrow (J_k^T J_k + \\lambda I_k) d_k = -J_k^T b_k$$ $\\lambda$ is the damping factor. $$\\begin{cases} \\text{Si } \\lambda = 0 \u0026 \\Rightarrow \\text{Gauss-Newton} \\\n\\text{Si } \\lambda \\rightarrow +\\infty \u0026 \\Rightarrow \\lambda d_k \\rightarrow -J_k^T b_k \\quad \\text{descente de gradient}\n\\end{cases}$$\nIf the new cost function value decreases (indicating an effective update), reduce $\\lambda$ to make the algorithm closer to Gauss-Newton, accelerating convergence.\nIf the cost function value does not decrease, increase $\\lambda$ to make the algorithm closer to gradient descent, ensuring stability.\n5.4 Summary of Algorithm Steps\nIn practical applications, the steps of the Levenberg-Marquardt algorithm are as follows:\n​\t1.\tInitialization:\nSet initial parameters $x$ and damping factor $\\lambda$.\nCompute the initial cost function $L_{\\min}$.\n​\t2.\tIteration:\nCompute Jacobian Matrix $\\mathbf{J}$ and residuals $\\mathbf{b}$.\nSolve Linear System:\n$$ (J^T J + \\lambda I_d) d = -J^T b$$ Update Parameters: $$x' = x + d$$ Compute New Cost Function $L’$ ​\t3.\tEvaluate Update Effectiveness:\nIf $L’ \u003c L_{\\min}$ (cost function decreases):\nAccept the update: $x = x’$, $L_{\\min} = L’$.\nDecrease $\\lambda$: $\\lambda = \\lambda / 2$.\nContinue iteration.\nOtherwise (cost function does not decrease):\nReject the update, do not change $x$.\nIncrease $\\lambda$: $\\lambda = 2\\lambda$.\nCheck if $\\lambda$ exceeds the maximum value; if so, stop iteration.\n​\t4.\tTermination Conditions:\nWhen $\\lambda$ exceeds a preset maximum value or the parameter updates become smaller than a threshold, stop iterating. All content in this document is theoretical and part of the ‘Vidéo 3D - Computer Vision’ course, taught by instructor Guillaume Bourmaud. All rights are reserved by the instructor.\nFor specific experimental content and complete code, please refer to Guillaume Bourmaud’s official website: https://gbourmaud.github.io/teaching/\n",
  "wordCount" : "5522",
  "inLanguage": "en",
  "image": "http://localhost:1313/images/papermod-cover.png","datePublished": "2024-10-21T16:25:17+01:00",
  "dateModified": "2024-11-12T17:12:35+08:00",
  "author":{
    "@type": "Person",
    "name": "Zehua"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/signal/video-3d/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Home",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Home (Alt + H)">Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="http://localhost:1313/zh/" title="中文"
                            aria-label="中文">中文</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="List">
                    <span>List</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives" title="Timeline">
                    <span>Timeline</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about/" title="about">
                    <span>about</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="search (Alt &#43; /)" accesskey=/>
                    <span>search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/signal/">Signal processing</a></div>
    <h1 class="post-title entry-hint-parent">
      3D Reconstruction Theory
    </h1>
    <div class="post-description">
      Course notes, for personal study and review only.
    </div>
    <div class="post-meta"><span title='2024-10-21 16:25:17 +0100 +0100'>October 21, 2024</span>&nbsp;·&nbsp;Zehua

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#1-image-projection-model-and-3d-reconstruction-theory" aria-label="1. Image Projection Model and 3D Reconstruction Theory"><strong>1. Image Projection Model and 3D Reconstruction Theory</strong></a><ul>
                            
                    <li>
                        <a href="#11-concept-of-slam" aria-label="1.1. Concept of SLAM"><strong>1.1. Concept of SLAM</strong></a></li>
                    <li>
                        <a href="#12-reverse-2d-image" aria-label="1.2. Reverse 2D Image"><strong>1.2. Reverse 2D Image</strong></a></li>
                    <li>
                        <a href="#13-pinhole-camera-model" aria-label="1.3 Pinhole Camera Model"><strong>1.3 Pinhole Camera Model</strong></a><ul>
                            
                    <li>
                        <a href="#131-model-overview" aria-label="1.3.1 Model Overview"><strong>1.3.1 Model Overview</strong></a></li>
                    <li>
                        <a href="#132-coordinate-systems-and-notation-conventions" aria-label="1.3.2 Coordinate Systems and Notation Conventions"><strong>1.3.2 Coordinate Systems and Notation Conventions</strong></a></li>
                    <li>
                        <a href="#133-projection-of-3d-points-to-normalized-focal-plane" aria-label="1.3.3 Projection of 3D Points to Normalized Focal Plane"><strong>1.3.3 Projection of 3D Points to Normalized Focal Plane</strong></a></li>
                    <li>
                        <a href="#134-homogeneous-coordinates-and-inhomogeneous-coordinates" aria-label="1.3.4 Homogeneous Coordinates and Inhomogeneous Coordinates"><strong>1.3.4 Homogeneous Coordinates and Inhomogeneous Coordinates</strong></a></li></ul>
                    </li>
                    <li>
                        <a href="#14-linear-calibration-of-the-camera" aria-label="1.4 Linear Calibration of the Camera"><strong>1.4 Linear Calibration of the Camera</strong></a><ul>
                            
                    <li>
                        <a href="#141-from-normalized-focal-plane-to-image-plane" aria-label="1.4.1 From Normalized Focal Plane to Image Plane"><strong>1.4.1 From Normalized Focal Plane to Image Plane</strong></a></li>
                    <li>
                        <a href="#142-camera-intrinsic-matrix" aria-label="1.4.2 Camera Intrinsic Matrix"><strong>1.4.2 Camera Intrinsic Matrix</strong></a></li>
                    <li>
                        <a href="#143-inverse-process" aria-label="1.4.3 Inverse Process"><strong>1.4.3 Inverse Process</strong></a></li>
                    <li>
                        <a href="#144-viewing-frustum" aria-label="1.4.4 Viewing Frustum"><strong>1.4.4 Viewing Frustum</strong></a></li></ul>
                    </li>
                    <li>
                        <a href="#15-distortion-modeling-and-correction" aria-label="1.5 Distortion Modeling and Correction"><strong>1.5 Distortion Modeling and Correction</strong></a><ul>
                            
                    <li>
                        <a href="#151-sources-of-camera-distortion" aria-label="1.5.1 Sources of Camera Distortion"><strong>1.5.1 Sources of Camera Distortion</strong></a></li>
                    <li>
                        <a href="#152-distortion-model" aria-label="1.5.2 Distortion Model"><strong>1.5.2 Distortion Model</strong></a></li></ul>
                    </li>
                    <li>
                        <a href="#16-implementation-of-distortion-correction" aria-label="1.6 Implementation of Distortion Correction"><strong>1.6 Implementation of Distortion Correction</strong></a><ul>
                            
                    <li>
                        <a href="#161-task-description" aria-label="1.6.1 Task Description"><strong>1.6.1 Task Description</strong></a></li>
                    <li>
                        <a href="#162-implementation-steps" aria-label="1.6.2 Implementation Steps"><strong>1.6.2 Implementation Steps</strong></a></li></ul>
                    </li></ul>
                    </li>
                    <li>
                        <a href="#2--2d-rigid-transformation-and-homography" aria-label="2. 2D Rigid Transformation and Homography"><strong>2.  2D Rigid Transformation and Homography</strong></a><ul>
                            
                    <li>
                        <a href="#21-2d-rigid-transformation" aria-label="2.1 2D Rigid Transformation"><strong>2.1 2D Rigid Transformation</strong></a><ul>
                            
                    <li>
                        <a href="#211-rotation" aria-label="2.1.1 Rotation"><strong>2.1.1 Rotation</strong></a></li>
                    <li>
                        <a href="#212-translation" aria-label="2.1.2 Translation"><strong>2.1.2 Translation</strong></a></li>
                    <li>
                        <a href="#213-rigid-transformation-formula" aria-label="2.1.3 Rigid Transformation Formula"><strong>2.1.3 Rigid Transformation Formula</strong></a></li>
                    <li>
                        <a href="#214-homogeneous-coordinates" aria-label="2.1.4 Homogeneous Coordinates:"><strong>2.1.4 Homogeneous Coordinates:</strong></a></li>
                    <li>
                        <a href="#215-inverse-transformation" aria-label="2.1.5 Inverse Transformation:"><strong>2.1.5 Inverse Transformation:</strong></a></li>
                    <li>
                        <a href="#216-composability-of-transformations" aria-label="2.1.6 Composability of Transformations:"><strong>2.1.6 Composability of Transformations:</strong></a></li></ul>
                    </li>
                    <li>
                        <a href="#22-homography" aria-label="2.2 Homography"><strong>2.2 Homography</strong></a><ul>
                            
                    <li>
                        <a href="#221-plane-scene-assumption" aria-label="2.2.1 Plane Scene Assumption"><strong>2.2.1 Plane Scene Assumption</strong></a></li>
                    <li>
                        <a href="#222-finding-the-correspondence-between-underlinemathbfm-_ai-and-underlinemathbfm-_bi" aria-label="2.2.2 Finding the Correspondence Between $\underline{\mathbf{m}} _{Ai}$ and $\underline{\mathbf{m}} _{Bi}$"><strong>2.2.2 Finding the Correspondence Between $\underline{\mathbf{m}} _{Ai}$ and $\underline{\mathbf{m}} _{Bi}$</strong></a></li>
                    <li>
                        <a href="#223-finding-the-correspondence-between-underlinemathbfp-_ai-and-underlinemathbfp-_bi" aria-label="2.2.3 Finding the Correspondence Between $\underline{\mathbf{P}} _{A,i}$ and $\underline{\mathbf{P}} _{B,i}$"><strong>2.2.3 Finding the Correspondence Between $\underline{\mathbf{P}} _{A,i}$ and $\underline{\mathbf{P}} _{B,i}$</strong></a></li>
                    <li>
                        <a href="#224-obtaining-the-homography-matrix-mathbfh-_ab" aria-label="2.2.4 Obtaining the Homography Matrix $\mathbf{H} _{AB}$"><strong>2.2.4 Obtaining the Homography Matrix $\mathbf{H} _{AB}$</strong></a></li>
                    <li>
                        <a href="#225-estimating-and-solving-the-homography-matrix" aria-label="2.2.5 Estimating and Solving the Homography Matrix"><strong>2.2.5 Estimating and Solving the Homography Matrix</strong></a></li></ul>
                    </li></ul>
                    </li>
                    <li>
                        <a href="#3-robust-homography-estimation-using-the-ransac-algorithm" aria-label="3. Robust Homography Estimation Using the RANSAC Algorithm"><strong>3. Robust Homography Estimation Using the RANSAC Algorithm</strong></a><ul>
                            
                    <li>
                        <a href="#31-objective" aria-label="3.1 Objective"><strong>3.1 Objective</strong></a></li>
                    <li>
                        <a href="#32-automatically-establishing-correspondences--sift-algorithm" aria-label="3.2 Automatically Establishing Correspondences — SIFT Algorithm"><strong>3.2 Automatically Establishing Correspondences — SIFT Algorithm</strong></a></li>
                    <li>
                        <a href="#33-robust-estimation-using-the-ransac-algorithm" aria-label="3.3 Robust Estimation Using the RANSAC Algorithm"><strong>3.3 Robust Estimation Using the RANSAC Algorithm</strong></a></li>
                    <li>
                        <a href="#34-why-not-use-traditional-quadratic-cost-functions" aria-label="3.4 Why Not Use Traditional Quadratic Cost Functions"><strong>3.4 Why Not Use Traditional Quadratic Cost Functions</strong></a></li>
                    <li>
                        <a href="#35-limitations-of-the-ransac-algorithm" aria-label="3.5 Limitations of the RANSAC Algorithm"><strong>3.5 Limitations of the RANSAC Algorithm</strong></a>
                    </li>
                </ul>
                </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><h2 id="1-image-projection-model-and-3d-reconstruction-theory"><strong>1. Image Projection Model and 3D Reconstruction Theory</strong><a hidden class="anchor" aria-hidden="true" href="#1-image-projection-model-and-3d-reconstruction-theory">#</a></h2>
<h3 id="11-concept-of-slam"><strong>1.1. Concept of SLAM</strong><a hidden class="anchor" aria-hidden="true" href="#11-concept-of-slam">#</a></h3>
<ul>
<li>
<p>SLAM (Simultaneous Localization and Mapping).</p>
</li>
<li>
<p>By estimating each camera’s position and the scene’s 3D points, achieve scene reconstruction.</p>
</li>
</ul>
<h3 id="12-reverse-2d-image"><strong>1.2. Reverse 2D Image</strong><a hidden class="anchor" aria-hidden="true" href="#12-reverse-2d-image">#</a></h3>
<p>Extract 3D scene information from a 2D image, i.e., perform $3D$ reconstruction.</p>
<p><strong>Inverse Projection</strong></p>
<ul>
<li>
<p>The image is a 2D representation of the 3D scene after projection. To recover 3D information, it is necessary to reverse this projection process.</p>
</li>
<li>
<p>Establish a mathematical model describing how the 3D scene is projected onto the 2D image, then attempt to solve it inversely.</p>
</li>
</ul>
<h3 id="13-pinhole-camera-model"><strong>1.3 Pinhole Camera Model</strong><a hidden class="anchor" aria-hidden="true" href="#13-pinhole-camera-model">#</a></h3>
<h4 id="131-model-overview"><strong>1.3.1 Model Overview</strong><a hidden class="anchor" aria-hidden="true" href="#131-model-overview">#</a></h4>
<p><strong>Definition</strong>: The pinhole camera model assumes that all light rays pass through a common point, the optical center.</p>
<ul>
<li><strong>Advantages</strong>: Simple model, easy to compute inversely, widely used in 3D reconstruction.</li>
</ul>
<h4 id="132-coordinate-systems-and-notation-conventions"><strong>1.3.2 Coordinate Systems and Notation Conventions</strong><a hidden class="anchor" aria-hidden="true" href="#132-coordinate-systems-and-notation-conventions">#</a></h4>
<ul>
<li><strong>Camera Coordinate System</strong>:</li>
</ul>
<p><strong>Origin $O_C$</strong>: Optical center, coordinates $(0, 0, 0)$.</p>
<ul>
<li>
<p><strong>Axis Directions</strong>: Establish a right-handed coordinate system, $X_C$ to the right, $Y_C$ downward, $Z_C$ pointing backward (scene depth direction).</p>
</li>
<li>
<p><strong>Advantage</strong>: The $Z$ axis points backward, object depth is positive, which is intuitive.</p>
</li>
</ul>
<h4 id="133-projection-of-3d-points-to-normalized-focal-plane"><strong>1.3.3 Projection of 3D Points to Normalized Focal Plane</strong><a hidden class="anchor" aria-hidden="true" href="#133-projection-of-3d-points-to-normalized-focal-plane">#</a></h4>
<ul>
<li>
<p><strong>3D Point Representation</strong>:</p>
</li>
<li>
<p><strong>Point $U$</strong>: Coordinates $(U_X, U_Y, U_Z)$, representing a 3D point in space.</p>
</li>
<li>
<p><strong>Normalized Focal Plane</strong>:</p>
</li>
<li>
<p>A plane at a distance of $1$ from the optical center $O_C$ ($Z_C = 1$), called the normalized focal plane.</p>
</li>
<li>
<p>Project distant 3D points onto this plane.</p>
</li>
</ul>
<h4 id="134-homogeneous-coordinates-and-inhomogeneous-coordinates"><strong>1.3.4 Homogeneous Coordinates and Inhomogeneous Coordinates</strong><a hidden class="anchor" aria-hidden="true" href="#134-homogeneous-coordinates-and-inhomogeneous-coordinates">#</a></h4>
<p><strong>Homogeneous Coordinates</strong>:</p>
<ul>
<li>
<p><strong>Definition</strong>: Adding an extra dimension (usually $1$) to the original coordinates to facilitate projection and transformation.</p>
</li>
<li>
<p><strong>Representation</strong>: For a 2D point $m = (m_X, m_Y)^T$, its homogeneous coordinates are $\bar{m} = (m_X, m_Y, 1)^T$.</p>
</li>
</ul>
<p><strong>Purpose</strong>: Homogeneous coordinates facilitate matrix operations, especially during projection and transformation processes.</p>
<ul>
<li>
<p><strong>Inhomogeneous Coordinates</strong>:</p>
</li>
<li>
<p>Standard Cartesian coordinate representation, without extra dimensions.</p>
</li>
</ul>
<h3 id="14-linear-calibration-of-the-camera"><strong>1.4 Linear Calibration of the Camera</strong><a hidden class="anchor" aria-hidden="true" href="#14-linear-calibration-of-the-camera">#</a></h3>
<h4 id="141-from-normalized-focal-plane-to-image-plane"><strong>1.4.1 From Normalized Focal Plane to Image Plane</strong><a hidden class="anchor" aria-hidden="true" href="#141-from-normalized-focal-plane-to-image-plane">#</a></h4>
<ul>
<li>
<p><strong>Image Plane</strong>:</p>
</li>
<li>
<p><strong>Coordinate System</strong>: Pixel coordinate system, typically with the image’s top-left corner as the origin, $X$ axis to the right (column index), $Y$ axis downward.</p>
</li>
<li>
<p><strong>Purpose</strong>: Map points on the normalized focal plane to actual image pixel coordinates.</p>
</li>
<li>
<p><strong>Linear Transformation</strong>:</p>
</li>
<li>
<p><strong>Transformation Formula</strong>:</p>
</li>
</ul>
<div>$$\begin{cases} P_U = f \cdot m_X + U_0 \\ P_V = f \cdot m_Y + V_0 \end{cases}$$</div>
<ul>
<li>
<p><strong>Focal Length $f$</strong></p>
</li>
<li>
<p>$m_X$, $m_Y$ <strong>are points on the normalized focal plane</strong></p>
</li>
<li>
<p><strong>Optical Center Coordinates on the Image Plane</strong> $(U_0, V_0)$</p>
</li>
</ul>
<h4 id="142-camera-intrinsic-matrix"><strong>1.4.2 Camera Intrinsic Matrix</strong><a hidden class="anchor" aria-hidden="true" href="#142-camera-intrinsic-matrix">#</a></h4>
<ul>
<li>Represent the above linear transformation in matrix form.</li>
</ul>
<div>$$K = \begin{pmatrix} f & 0 & U_0 \\ 0 & f & V_0 \\ 0 & 0 & 1 \end{pmatrix}$$</div>
<ul>
<li><strong>Matrix Mapping Relationship</strong>:</li>
</ul>
<div>$$\underline{P} = K \cdot \underline{m}$$</div>
<p>Where, $\underline{P}$ is the homogeneous coordinates of points on the image plane.</p>
<h4 id="143-inverse-process"><strong>1.4.3 Inverse Process</strong><a hidden class="anchor" aria-hidden="true" href="#143-inverse-process">#</a></h4>
<ul>
<li><strong>From Image Plane to Normalized Focal Plane</strong>:</li>
</ul>
<div>$$\underline{M} = K^{-1} \cdot \underline{P}$$</div>
<h4 id="144-viewing-frustum"><strong>1.4.4 Viewing Frustum</strong><a hidden class="anchor" aria-hidden="true" href="#144-viewing-frustum">#</a></h4>
<ul>
<li>Represents the spatial range that the camera can see. By converting the four corner points of the image to the normalized focal plane and then connecting them to the optical center, forming a viewing frustum.</li>
</ul>
<h3 id="15-distortion-modeling-and-correction"><strong>1.5 Distortion Modeling and Correction</strong><a hidden class="anchor" aria-hidden="true" href="#15-distortion-modeling-and-correction">#</a></h3>
<h4 id="151-sources-of-camera-distortion"><strong>1.5.1 Sources of Camera Distortion</strong><a hidden class="anchor" aria-hidden="true" href="#151-sources-of-camera-distortion">#</a></h4>
<ul>
<li>Optical defects in actual camera lenses, especially in wide-angle lenses, causing image distortion, straight lines becoming curved, image edges stretched or compressed.</li>
</ul>
<h4 id="152-distortion-model"><strong>1.5.2 Distortion Model</strong><a hidden class="anchor" aria-hidden="true" href="#152-distortion-model">#</a></h4>
<ul>
<li>
<p><strong>Distortion Function</strong>:</p>
</li>
<li>
<p>Apply a distortion function to ideal points on the normalized focal plane (from ideal image to distorted image) to obtain distorted points, which are actual $2D$ points on the distorted focal plane.</p>
</li>
</ul>
<div>$$\underline{m}_d = d(\underline{m}, k)$$</div>
<p>Where, $k$ are distortion parameters.</p>
<ul>
<li><strong>Example: Polynomial Radial Distortion Model</strong></li>
</ul>
<div>$$M_d = \left(1 + k_1 \|m\|_2^2 + k_2 \|m\|_2^4 + \dots \right) m$$</div>
<p>Where: $|m|_2^2 = m_x^2 + m_y^2$</p>
<h3 id="16-implementation-of-distortion-correction"><strong>1.6 Implementation of Distortion Correction</strong><a hidden class="anchor" aria-hidden="true" href="#16-implementation-of-distortion-correction">#</a></h3>
<h4 id="161-task-description"><strong>1.6.1 Task Description</strong><a hidden class="anchor" aria-hidden="true" href="#161-task-description">#</a></h4>
<ul>
<li><strong>Goal</strong>: Correct the distorted actual image to an ideal undistorted image.</li>
</ul>
<h4 id="162-implementation-steps"><strong>1.6.2 Implementation Steps</strong><a hidden class="anchor" aria-hidden="true" href="#162-implementation-steps">#</a></h4>
<p><strong>Define Parameters</strong>: Ideal camera intrinsic matrix $K_{\text{ideal}}$; distorted camera intrinsic matrix $K_{\text{real}}$; distortion parameters $k$.</p>
<p><strong>For each ideal image pixel coordinate, perform the following steps:</strong></p>
<p>​	1.	<strong>Convert pixel coordinates to normalized focal plane</strong>:</p>
<div>$$\underline{m} _{\text{ideal}} = K_{\text{ideal}}^{-1} \cdot \underline{P} _{\text{ideal}}$$</div>
<p>​	2.	<strong>Apply distortion function</strong>:</p>
<div>$$\underline{m}_d = d(\underline{m} _{\text{ideal}}, k)$$</div>
<p>​	3.	<strong>Map back to actual image coordinate system</strong>:</p>
<div>$$\underline{P} _{\text{real}} = K_{\text{real}} \cdot \underline{m}_d$$</div>
<p>​	4.	<strong>Interpolation</strong>:</p>
<p>Perform interpolation on $\underline{P} _{\text{real}}$ (since coordinates may be non-integer, possibly use bilinear interpolation)</p>
<p>​	5.	<strong>Generate corrected image</strong></p>
<h2 id="2--2d-rigid-transformation-and-homography"><strong>2.  2D Rigid Transformation and Homography</strong><a hidden class="anchor" aria-hidden="true" href="#2--2d-rigid-transformation-and-homography">#</a></h2>
<h3 id="21-2d-rigid-transformation"><strong>2.1 2D Rigid Transformation</strong><a hidden class="anchor" aria-hidden="true" href="#21-2d-rigid-transformation">#</a></h3>
<p>2D rigid transformations include <strong>translation</strong> and <strong>rotation</strong>.</p>
<h4 id="211-rotation"><strong>2.1.1 Rotation</strong><a hidden class="anchor" aria-hidden="true" href="#211-rotation">#</a></h4>
<div>$$\mathbf{U}^c = \overrightarrow{O _c U}^c \quad \mathbf{U}^w = \overrightarrow{O _w U}^w$$</div>
<div>$$\mathbf{R} _{wc} \underline{\mathbf{U}}^c = \mathbf{R} _{wc} \cdot \overrightarrow{O _c U}^c = \overrightarrow{O _w U}^w$$</div>
<p>Select a vector from one reference frame and then transform it to another coordinate frame.</p>
<p>$\mathbf{R} _{wc}$ is an orthogonal matrix.</p>
<h4 id="212-translation"><strong>2.1.2 Translation</strong><a hidden class="anchor" aria-hidden="true" href="#212-translation">#</a></h4>
<div>$$\mathbf{T} _{wc} = \overrightarrow{O _w O _c}^{w}$$</div>
<h4 id="213-rigid-transformation-formula"><strong>2.1.3 Rigid Transformation Formula</strong><a hidden class="anchor" aria-hidden="true" href="#213-rigid-transformation-formula">#</a></h4>
<div>$$\mathbf{U}^w = \mathbf{R} _{wc} \cdot \mathbf{U}^c + \mathbf{T} _{wc}$$</div>
<p><strong>Proof:</strong></p>
<div>$$\mathbf{R} _{wc} \cdot \mathbf{U}^c + \mathbf{T} _{wc} = \mathbf{R} _{wc} \cdot \overrightarrow{O _c U}^c + \overrightarrow{O _w O _c}^w = \overrightarrow{O _c U}^w + \overrightarrow{O _w O _c}^w = \overrightarrow{O _w U}^w = \mathbf{U}^w$$</div>
<h4 id="214-homogeneous-coordinates"><strong>2.1.4 Homogeneous Coordinates:</strong><a hidden class="anchor" aria-hidden="true" href="#214-homogeneous-coordinates">#</a></h4>
<div>$$\underline{\mathbf{U}}^w = \begin{bmatrix} \mathbf{U}^w \\ 1 \end{bmatrix}$$</div>
<div>$$\mathbf{M} _{wc} = \begin{bmatrix} \mathbf{R} _{wc} & \mathbf{T} _{wc} \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} r _{11} & r _{12} & r _{13} & t _{x} \\ r _{21} & r _{22} & r _{23} & t _{y} \\ r _{31} & r _{32} & r _{33} & t _{z} \\ 0 & 0 & 0 & 1 \end{bmatrix}$$</div>
<h4 id="215-inverse-transformation"><strong>2.1.5 Inverse Transformation:</strong><a hidden class="anchor" aria-hidden="true" href="#215-inverse-transformation">#</a></h4>
<div>$$\mathbf{M} _{cw} = \mathbf{M} _{wc}^{-1}$$</div>
<h4 id="216-composability-of-transformations"><strong>2.1.6 Composability of Transformations:</strong><a hidden class="anchor" aria-hidden="true" href="#216-composability-of-transformations">#</a></h4>
<div>$$\mathbf{M} _{ab} \cdot \mathbf{M} _{bc} = \mathbf{M} _{ac}$$</div>
<h3 id="22-homography"><strong>2.2 Homography</strong><a hidden class="anchor" aria-hidden="true" href="#22-homography">#</a></h3>
<h4 id="221-plane-scene-assumption"><strong>2.2.1 Plane Scene Assumption</strong><a hidden class="anchor" aria-hidden="true" href="#221-plane-scene-assumption">#</a></h4>
<div>$$\mathbf{U} _i^A = z _i^A \cdot \underline{\mathbf{m}} _{Ai}$$</div>
<p>This equation means that the point $\mathbf{U} _i ^ A$ can be represented by $\underline{\mathbf{m}} _{Ai}$ by multiplying its depth, since $\underline{\mathbf{m}} _{Ai}$ has unit depth.</p>
<h4 id="222-finding-the-correspondence-between-underlinemathbfm-_ai-and-underlinemathbfm-_bi"><strong>2.2.2 Finding the Correspondence Between $\underline{\mathbf{m}} _{Ai}$ and $\underline{\mathbf{m}} _{Bi}$</strong><a hidden class="anchor" aria-hidden="true" href="#222-finding-the-correspondence-between-underlinemathbfm-_ai-and-underlinemathbfm-_bi">#</a></h4>
<p>With only this equation, how do we find the correspondence between $\underline{\mathbf{m}} _{Ai}$ and $\underline{\mathbf{m}} _{Bi}$? In simple terms, how do we perform coordinate correspondence transformations?</p>
<p>​	1.	<strong>Key Formula for the Normal</strong></p>
<p>We need to recall a property to obtain a key formula involving the normal and the plane.</p>
<p>In reference frame $A$, the equation of plane $P$ is: $ax + by + cz + d = 0$, where $a, b, c$ are the components of the plane’s normal vector, and $d$ is a constant representing the relative distance between plane $P$ and the origin $O _A$.</p>
<p>In vector form, the plane equation can be simplified to:</p>
<div>$$\mathbf{n} _A^T \mathbf{U} _i^A + d = 0$$</div>
<p>$\mathbf{n} _A^T$ represents the normal vector of plane $P$ in reference frame $A$.</p>
<p>Through the vector form of the plane equation, we obtain a very important formula involving the normal vector.</p>
<p>​	2.	<strong>Obtaining the Depth Expression Using Variable Substitution</strong></p>
<p>Substitute $\mathbf{U} _i^A = z _i^A \cdot \underline{\mathbf{m}} _{A,i}$ into the above equation:</p>
<div>$$\mathbf{n} _A^T \cdot z _i^A \cdot \underline{\mathbf{m}} _{A,i} + d = 0 \quad \Rightarrow \quad z _i^A = -\dfrac{d}{\mathbf{n} _A^T \cdot \underline{\mathbf{m}} _{A,i}}$$</div>
<p>In this way, we have introduced $\underline{\mathbf{m}} _{Ai}$, where $z _i^A = -\dfrac{d}{\mathbf{n} _A^T \cdot \underline{\mathbf{m}} _{A,i}}$ represents the depth. In other words, by using the two equations of $\mathbf{U} _i^A$, we have replaced $\mathbf{U} _i^A$, thus obtaining the depth $z _i^A$. However, this still does not solve the problem $\Rightarrow$ that is to say, having only the equation related to $\underline{\mathbf{m}} _{Ai}$ is not enough; we also need to approach from $\underline{\mathbf{m}} _{Bi}$.</p>
<p>​	3.	<strong>Next, We Find the Point $\underline{\mathbf{m}} _{Bi}$ in Coordinate Frame $B$</strong></p>
<p>Starting from the rigid transformation formula $\mathbf{U}^w = \mathbf{R} _{wc} \cdot \mathbf{U}^c + \mathbf{T} _{wc}$, we can see that projecting from $c$ to $w$ only requires transforming $\mathbf{U}^c$. In other words, to obtain $\underline{\mathbf{m}} _{Bi}$, we only need to perform a rigid transformation on $\underline{\mathbf{m}} _{Ai}$.</p>
<div>$$\underline{\mathbf{m}} _{B,i} = \Pi \left( \mathbf{R} _{BA} \mathbf{U} _i^A + \mathbf{t} _{BA} \right)$$</div>
<p>where $\Pi(\cdot)$ is the projection function.</p>
<div>$$\underline{\mathbf{m}} _{B,i}= \Pi \left( \mathbf{R} _{BA} \left( -\dfrac{d}{\mathbf{n} _A^T \cdot \underline{\mathbf{m}} _{A,i}} \right) \cdot \underline{\mathbf{m}} _{A,i} + \mathbf{t} _{BA} \right)$$</div>
<p>Multiply both sides of the above equation by $-\dfrac{\mathbf{n} _A^T \cdot \underline{\mathbf{m}} _{A,i}}{d}$:</p>
<div>$$\underline{\mathbf{m}} _{B,i}= \Pi \left( \mathbf{R} _{BA} \cdot \underline{\mathbf{m}} _{A,i} - \dfrac{\mathbf{n} _A^T \cdot \underline{\mathbf{m}} _{A,i}}{d} \cdot \mathbf{t} _{BA} \right)$$</div>
<div>$$\underline{\mathbf{m}} _{B,i} = \Pi \left( \left( \mathbf{R} _{BA} - \dfrac{\mathbf{t} _{BA} \cdot \mathbf{n} _A^T}{d} \right) \cdot \underline{\mathbf{m}} _{A,i} \right)$$</div>
<p>This establishes the correspondence between point $A$ and point $B$ on their respective normalized planes.</p>
<p><strong>Question:</strong> In the above formula, why does multiplying both sides by $-\dfrac{\mathbf{n} _A^T \cdot \underline{\mathbf{m}} _{A,i}}{d}$ not change the equation?</p>
<p>The projection function $\Pi(\cdot)$ is a scale-invariant operation (it only considers direction and relative position, not absolute scale). Therefore, even if we multiply the right side by $-\dfrac{\mathbf{n} _A^T \cdot \underline{\mathbf{m}} _{A,i}}{d}$, it does not affect the condition for the equality to hold because the projection result remains the same.</p>
<h4 id="223-finding-the-correspondence-between-underlinemathbfp-_ai-and-underlinemathbfp-_bi"><strong>2.2.3 Finding the Correspondence Between $\underline{\mathbf{P}} _{A,i}$ and $\underline{\mathbf{P}} _{B,i}$</strong><a hidden class="anchor" aria-hidden="true" href="#223-finding-the-correspondence-between-underlinemathbfp-_ai-and-underlinemathbfp-_bi">#</a></h4>
<p>We know:</p>
<div>$$\left\{ \begin{aligned} \underline{\mathbf{m}} _{A,i} = K _A^{-1} \cdot \underline{\mathbf{P}} _{A,i}\\ \underline{\mathbf{m}} _{B,i} = K _B^{-1} \cdot \underline{\mathbf{P}} _{B,i} \end{aligned} \right.$$</div>
<p>$\underline{\mathbf{P}} _{B,i} = K _B \cdot \underline{\mathbf{m}} _{B,i} \Rightarrow$ Substitute the obtained $\underline{\mathbf{m}} _{B,i}$:</p>
<div>$$\underline{\mathbf{P}} _{B,i} = K _B \cdot \Pi \left( \left( \mathbf{R} _{BA} - \dfrac{\mathbf{t} _{BA} \cdot \mathbf{n} _A^T}{d} \right) \cdot \underline{\mathbf{m}} _{A,i} \right)$$</div>
<div>$$\underline{\mathbf{P}} _{B,i} = K _B \cdot \Pi \left( \left( \mathbf{R} _{BA} - \dfrac{\mathbf{t} _{BA} \cdot \mathbf{n} _A^T}{d} \right) \cdot K _A^{-1} \cdot \underline{\mathbf{P}} _{A,i} \right)$$</div>
<p>Recall the property:</p>
<div>$$K \cdot \Pi \left( \begin{bmatrix} a \\ b \\ c \end{bmatrix} \right) = \Pi \left( K \cdot \begin{bmatrix} a \\ b \\ c \end{bmatrix} \right)$$</div>
<p>Using this property, we get:</p>
<div>$$\underline{\mathbf{P}} _{B,i} = \Pi \left( K _B \cdot \left( \mathbf{R} _{BA} - \dfrac{\mathbf{t} _{BA} \cdot \mathbf{n} _A^T}{d} \right) \cdot K _A^{-1} \cdot \underline{\mathbf{P}} _{A,i}\right)$$</div>
<h4 id="224-obtaining-the-homography-matrix-mathbfh-_ab"><strong>2.2.4 Obtaining the Homography Matrix $\mathbf{H} _{AB}$</strong><a hidden class="anchor" aria-hidden="true" href="#224-obtaining-the-homography-matrix-mathbfh-_ab">#</a></h4>
<p>Assume:</p>
<p>$$\mathbf{H} _{AB} = K _B \cdot \left( \mathbf{R} _{BA} - \dfrac{\mathbf{t} _{BA} \cdot \mathbf{n} _A^T}{d} \right) \cdot K _A^{-1}$$</p>
<p>Therefore:</p>
<div>$$\left\{ \begin{aligned} &\underline{\mathbf{P}} _{B,i} = \Pi \left( \mathbf{H} _{BA} \cdot \underline{\mathbf{P}} _{A,i} \right) \quad \quad  A \Rightarrow B\\ &\underline{\mathbf{P}} _{A,i} = \Pi \left( \mathbf{H} _{BA}^{-1} \cdot \underline{\mathbf{P}} _{B,i} \right) = \Pi \left( \mathbf{H} _{AB} \cdot \underline{\mathbf{P}} _{B,i} \right) \quad \quad  B \Rightarrow A \\ \end{aligned} \right.$$</div>
<p>Through the homography matrix, we can transform a point from one camera’s image coordinate system to another camera’s image coordinate system, establishing a point mapping relationship.</p>
<h4 id="225-estimating-and-solving-the-homography-matrix"><strong>2.2.5 Estimating and Solving the Homography Matrix</strong><a hidden class="anchor" aria-hidden="true" href="#225-estimating-and-solving-the-homography-matrix">#</a></h4>
<div>$$\mathbf{H} _{AB} = \begin{bmatrix} h _1 & h _4 & h _7 \\ h _2 & h _5 & h _8 \\ h _3 & h _6 & h _9 \end{bmatrix}$$</div>
<p>This is a homogeneous matrix with 9 parameters $h _1$ to $h _9$. Homogeneous matrices have redundancy in scale, leading to a loss of degrees of freedom.</p>
<ul>
<li><strong>Simple Solution – Parameterization (Parameters to Estimate = Free Parameters)</strong></li>
</ul>
<div>$$\mathbf{H} _{AB} = \begin{bmatrix} h _1 & h _4 & h _7 \\ h _2 & h _5 & h _8 \\ h _3 & h _6 & 1 \end{bmatrix}$$</div>
<div>$$\mathbf{h} = \begin{bmatrix} h _1 \\ \vdots \\ h _8 \end{bmatrix}$$</div>
<p><strong>How to Estimate $\mathbf{h}$?</strong></p>
<ul>
<li>In this case, as long as we know one corresponding point, we can determine $h _1$ to $h _8$.</li>
</ul>
<div>$$\underline{\mathbf{P}} _{A,i} = \Pi \left( \begin{bmatrix} h _1 & h _4 & h _7 \\ h _2 & h _5 & h _8 \\ h _3 & h _6 & 1 \end{bmatrix} \cdot \underline{\mathbf{P}} _{B,i} \right)$$</div>
<p>Since $\underline{\mathbf{P}} _{A,i}$ and $\underline{\mathbf{P}} _{B,i}$ are homogeneous coordinates, we expand them:</p>
<div>$$\begin{bmatrix} P _{A,i,x} \\ P _{A,i,y} \\ 1 \end{bmatrix} = \Pi \left( \begin{bmatrix} h _1 & h _4 & h _7 \\ h _2 & h _5 & h _8 \\ h _3 & h _6 & 1 \end{bmatrix} \cdot \begin{bmatrix} P _{B,i,x} \\ P _{B,i,y} \\ 1 \end{bmatrix} \right)$$</div>
<div>$$\left\{ \begin{aligned} P _{A,i,x} = \dfrac{h _1 \cdot P _{B,i,x} + h _4 \cdot P _{B,i,y} + h _7}{h _3 \cdot P _{B,i,x} + h _6 \cdot P _{B,i,y} + 1} \\ P _{A,i,y} = \dfrac{h _2 \cdot P _{B,i,x} + h _5 \cdot P _{B,i,y} + h _8}{h _3 \cdot P _{B,i,x} + h _6 \cdot P _{B,i,y} + 1} \end{aligned} \right.$$</div>
<div>$$\left\{ \begin{aligned} P _{A,i,x} \cdot \left( h _3 \cdot P _{B,i,x} + h _6 \cdot P _{B,i,y} + 1 \right) = h _1 \cdot P _{B,i,x} + h _4 \cdot P _{B,i,y} + h _7 \\ P _{A,i,y} \cdot \left( h _3 \cdot P _{B,i,x} + h _6 \cdot P _{B,i,y} + 1 \right) = h _2 \cdot P _{B,i,x} + h _5 \cdot P _{B,i,y} + h _8 \end{aligned} \right.$$</div>
<div>$$\begin{bmatrix} P _{B,i,x} & 0 & -P _{A,i,x} \cdot P _{B,i,x} & P _{B,i,y} & 0 & -P _{A,i,x} \cdot P _{B,i,y} & 1 & 0 \\ 0 & P _{B,i,x} & -P _{A,i,y} \cdot P _{B,i,x} & 0 & P _{B,i,y} & -P _{A,i,y} \cdot P _{B,i,y} & 0 & 1 \end{bmatrix} \begin{bmatrix} h _1 \\ h _2 \\ h _3 \\ h _4 \\ h _5 \\ h _6 \\ h _7 \\ h _8 \end{bmatrix} = \begin{bmatrix} P _{A,i,x} \\ P _{A,i _y} \end{bmatrix}$$</div>
<p>Since there are 8 unknowns, we need eight independent linear equations. Each pair of corresponding points provides two corresponding equations (i.e., equations 59), so at least four pairs of corresponding points are required. That is, four matching pairs $\left( P _{A,i}, P _{B,i} \right) \quad i = 1, 2, 3, 4$ are needed.</p>
<p>$\Rightarrow \mathbf{h}^* = \arg\min _{\mathbf{h}} \sum _{i=1}^{4} \left\lVert M _i \mathbf{h} - P _{A,i} \right\rVert _2^2 \Rightarrow$ Linear Least Squares</p>
<h2 id="3-robust-homography-estimation-using-the-ransac-algorithm"><strong>3. Robust Homography Estimation Using the RANSAC Algorithm</strong><a hidden class="anchor" aria-hidden="true" href="#3-robust-homography-estimation-using-the-ransac-algorithm">#</a></h2>
<h3 id="31-objective"><strong>3.1 Objective</strong><a hidden class="anchor" aria-hidden="true" href="#31-objective">#</a></h3>
<ul>
<li><strong>Image Alignment and Stitching</strong>: Achieve automatic image stitching by estimating the homography between two images.</li>
</ul>
<h3 id="32-automatically-establishing-correspondences--sift-algorithm"><strong>3.2 Automatically Establishing Correspondences — SIFT Algorithm</strong><a hidden class="anchor" aria-hidden="true" href="#32-automatically-establishing-correspondences--sift-algorithm">#</a></h3>
<p><strong>Interest Point Detection</strong></p>
<ul>
<li>
<p>Use algorithms like SIFT to detect feature points in both images (this code is provided by the instructor), eliminating the need to manually label corresponding points and utilizing algorithms to automatically establish correspondences between images.</p>
</li>
<li>
<p>Therefore, we can find the most similar point pairs in both images. However, note that the point pairs may not correspond correctly.</p>
</li>
<li>
<p>That is, there may be incorrect matches (outliers). In such cases, we cannot directly use the correspondences. Instead, we will use another algorithm called RANSAC to automatically assess the correctness of corresponding points and obtain the most optimal $H$ matrix to output.</p>
</li>
</ul>
<h3 id="33-robust-estimation-using-the-ransac-algorithm"><strong>3.3 Robust Estimation Using the RANSAC Algorithm</strong><a hidden class="anchor" aria-hidden="true" href="#33-robust-estimation-using-the-ransac-algorithm">#</a></h3>
<p>​	1.	<strong>Algorithm Concept:</strong></p>
<ul>
<li>
<p><strong>Random Sample Consensus (RANSAC)</strong> is a robust algorithm for estimating model parameters ($H$) in the presence of outliers (incorrect points).</p>
</li>
<li>
<p>It repeatedly performs random sampling to find the best-fitting model.</p>
</li>
</ul>
<p>​	2.	<strong>RANSAC Procedure:</strong></p>
<ul>
<li>Repeat $N$ times</li>
</ul>
<p>(The number of iterations is determined based on experience or computation):</p>
<p>​	1.	<strong>Randomly Select 4 Pairs of Matching Points:</strong></p>
<ul>
<li>
<p>4 is the minimum number of matching points required to estimate the homography matrix.</p>
</li>
<li>
<p>Randomly select four from all matching points. Since it is uncertain which correspondences are correct, a subsequent estimation and evaluation criterion (Euclidean distance) is needed.</p>
</li>
</ul>
<p>​	2.	<strong>Estimate the Homography Matrix $H^k$:</strong></p>
<ul>
<li>Use the selected 4 pairs of matching points and the $DLT$ algorithm (done in the previous experiment, its purpose and function are to estimate the homography matrix given known corresponding points) to estimate the homography matrix.</li>
</ul>
<p>​	3.	<strong>Compute Errors and Evaluate the Model:</strong></p>
<ul>
<li>
<p>For all matching points (including those not selected), transform the second image’s point $P _{B _i}$ using the estimated $H^k$, i.e., $H^k P _{B _i}$.</p>
</li>
<li>
<p>Calculate the Euclidean distance between the transformed point (estimated point) and the actual point $P _{A _i}$ in the first image.</p>
</li>
<li>
<p><strong>Define the Cost Function</strong>: Use a binary kernel function $\phi _c(d)$ (either 0 or 1):</p>
</li>
<li>
<p>When the distance $d &lt; \tau$, consider the match correct, and the cost is 0.</p>
</li>
<li>
<p>When the distance $d \geq \tau$, consider the match incorrect, and the cost is 1.</p>
</li>
<li>
<p><strong>Total Cost</strong>: $L^k = \sum _{i} \phi _c(|P _{A _i} - H^k P _{B _i}|)$</p>
</li>
</ul>
<p>​	4.	<strong>Update the Best Model:</strong></p>
<ul>
<li>
<p>If the current cost $L^k$ is less than the previous minimum cost $L$, update $L$ and the corresponding $H$.</p>
</li>
<li>
<p><strong>Final Output:</strong></p>
</li>
<li>
<p>The homography matrix $H$ with the minimum cost.</p>
</li>
</ul>
<p>​	3.	<strong>Selection of Threshold $\tau$:</strong></p>
<ul>
<li>
<p>$\tau$ is the distance threshold to determine whether a match is an inlier. It is usually chosen based on image resolution and matching accuracy, generally between $0.5$ to $3$ pixels.</p>
</li>
<li>
<p>Choosing a too large $\tau$ increases incorrect matches, while a too small $\tau$ may ignore correct matches.</p>
</li>
</ul>
<h3 id="34-why-not-use-traditional-quadratic-cost-functions"><strong>3.4 Why Not Use Traditional Quadratic Cost Functions</strong><a hidden class="anchor" aria-hidden="true" href="#34-why-not-use-traditional-quadratic-cost-functions">#</a></h3>
<ul>
<li>
<p><strong>Sensitivity Issues:</strong></p>
</li>
<li>
<p>Quadratic cost functions (such as least squares) are very sensitive to outliers. If a point has a large error, it will cause the cost function value to be excessively large, making the errors of other points irrelevant.</p>
</li>
<li>
<p><strong>Robustness:</strong></p>
</li>
<li>
<p>Binary kernel functions are insensitive to those extremely large or outlying points (all equal to 1), effectively suppressing the influence of outliers and making the estimation result more robust.</p>
</li>
<li>
<p><strong>Other Kernel Functions:</strong></p>
</li>
<li>
<p>Besides binary kernel functions, there are other robust kernel functions like the $Huber$ kernel and $Lorentzian$ kernel, which can balance error magnitude and robustness to some extent.</p>
</li>
</ul>
<h3 id="35-limitations-of-the-ransac-algorithm"><strong>3.5 Limitations of the RANSAC Algorithm</strong><a hidden class="anchor" aria-hidden="true" href="#35-limitations-of-the-ransac-algorithm">#</a></h3>
<ul>
<li>
<p><strong>Influence of the Number of Parameters:</strong></p>
</li>
<li>
<p>As the number of model parameters increases, the required number of random samples grows exponentially, significantly increasing computational cost.</p>
</li>
<li>
<p><strong>Applicable Range:</strong></p>
</li>
<li>
<p>RANSAC is suitable for cases with a small number of parameters, such as line fitting, fundamental matrix estimation, and homography estimation.</p>
</li>
</ul>
<p><strong>4. Epipolar Geometry in Stereo Vision</strong></p>
<p>So far, we have studied the case of planar scenes and used homography to describe the relationship between two views. However, for general three-dimensional scenes, the planar assumption no longer holds. To address this, we introduce epipolar geometry.</p>
<p><strong>4.1 Epipolar Geometry</strong></p>
<p>Epipolar geometry can be well explained through a schematic diagram:</p>
<p>​	1.	<strong>Consider Two Cameras Located in Reference Frames 1 and 2 Respectively</strong></p>
<p>The optical center of Camera 1 is $O_1$, and the optical center of Camera 2 is $O_2$. A point $U$ in space is projected onto the image planes of both cameras, resulting in points $\underline{m}_1$ and $\underline{m}_2$.</p>
<p>​	2.	<strong>Problem Description</strong>:</p>
<ul>
<li>
<p>In general cases, we cannot make any assumptions about point $U$ (unlike the previous planar scene).</p>
</li>
<li>
<p>We need to find a method to establish a relationship between $\underline{m}_1$ and $\underline{m}_2$ without knowing $U$.</p>
</li>
</ul>
<p><strong>4.2 Epipolar Plane and Epipolar Lines</strong></p>
<p>​	1.	<strong>Epipolar Plane</strong></p>
<p>$U$ and the optical centers $O_1$, $O_2$ define a plane $\Rightarrow$ Points $m_1$, $m_2$, $O_1$, $O_2$ are coplanar $\Rightarrow$ This plane is called the epipolar plane.</p>
<p><strong>Epipolar Constraint</strong> = <strong>Coplanarity</strong>, meaning $\underline{m}_1$, $\underline{m}_2$, $O_1$, $O_2$ are coplanar.</p>
<p>In stereo vision, the fundamental matrix $F$ and the essential matrix $E$ both rely on the coplanarity condition for their computation.</p>
<p>​	2.	<strong>Epipolar Lines</strong></p>
<p>The epipolar plane intersects the image planes of the two cameras, resulting in epipolar lines $l_1$ and $l_2$ respectively.</p>
<p>$m_2$ is the projection of the three-dimensional point $U$ onto the image plane of the second camera. However, according to the constraints of epipolar geometry, $m_2$ must lie on the epipolar line $l_2$.</p>
<p>$\Rightarrow$ Given the position of point $m_1$, the corresponding epipolar line $l_2$ can be determined using the fundamental matrix $F$: $l_2 = F \cdot m_1$.</p>
<p>The fundamental matrix $F$ captures the relative pose and intrinsic parameters between the two cameras. This formula indicates that given point $m_1$, one can compute the epipolar line $l_2$ on which $m_2$ must lie.</p>
<p><strong>4.3 Epipolar Constraint</strong></p>
<p><strong>Objective</strong>: Utilize the above geometric relationships to formalize the epipolar constraint and establish a mathematical relationship between $m_1$ and $m_2$.</p>
<p><strong>Define Vectors:</strong></p>
<div>$$\left\{ \begin{aligned} 
<p>&amp;\mathbf{\underline{m}_1} \text{ is the vector from the optical center } O_1 \text{ to the image point } \underline{m}_1 \quad \overrightarrow{O_1 m_1}^{1} \</p>
<p>&amp;\mathbf{\underline{m}_2} \text{ is the vector from the optical center } O_2 \text{ to the image point } \underline{m}_2 \quad \overrightarrow{O_2 m_2}^{2} \</p>
<p>&amp;\mathbf{t_{12}} = \overrightarrow{O_1 O_2}^{1} \text{ is the translation vector between the two camera optical centers}</p>
<p>\end{aligned} \right.$$</div></p>
<p><strong>Define the Normal Vector of the Epipolar Plane:</strong></p>
<div>$$\left\{ \begin{aligned} 
<p>&amp;\text{In reference frame } 1, \quad \overrightarrow{\mathbf{n}_1}^{1} = \underline{\mathbf{m}}_1 \times \mathbf{t} _{12} \</p>
<p>&amp;\text{In reference frame } 2, \quad \overrightarrow{\mathbf{n}_2}^{2} = \mathbf{R} _{21} \overrightarrow{\mathbf{n}_1}^{1}, \text{ where } \mathbf{R} \text{ is the rotation matrix between the cameras}</p>
<p>\end{aligned} \right.$$</div></p>
<p><strong>Note:</strong></p>
<ul>
<li>
<p>Here, $\times$ denotes the cross product operation between two vectors. The result of the cross product is a vector <strong>perpendicular to both vectors involved in the operation</strong>, with its direction determined by the right-hand rule and magnitude equal to the area of the parallelogram formed by the two vectors.</p>
</li>
<li>
<p>Coordinate system transformations for the normal vector do not need to consider the translation component because unit normal vectors are not position coordinates. Direction vectors remain unchanged in magnitude during rotation and are unaffected by translation. In summary, normal vectors only consider the rotation matrix, while points need to consider both rotation and translation.</p>
</li>
</ul>
<div>$$\overrightarrow{\mathbf{n}_2}^{2} = \mathbf{R} _{21} \cdot \overrightarrow{\mathbf{n}_1}^{1} = \mathbf{R} _{21} \cdot \left( \underline{\mathbf{m}}_1 \times \mathbf{t} _{12} \right) = \mathbf{R} _{21} \cdot \underline{\mathbf{m}}_1 \times \mathbf{R} _{21} \cdot \mathbf{t} _{12}$$</div>
<p>Since we previously know that $\mathbf{t} _{21} = \mathbf{R}*{21} \cdot \mathbf{t} _{12}$, the above equation becomes</p>
<div>$$\overrightarrow{\mathbf{n}_2}^{2} = \mathbf{t} _{21} \times \left( \mathbf{R} _{21} \cdot \underline{\mathbf{m}}_1 \right)$$</div>
<p><strong>Recall Properties of Cross Product Operations:</strong></p>
<div>$$\mathbf{a} \times \mathbf{b} = \begin{bmatrix} a_x \\ a_y \\ a_z \end{bmatrix} \times \begin{bmatrix} b_x \\ b_y \\ b_z \end{bmatrix} = \begin{bmatrix} a_y b_z - a_z b_y \\ a_z b_x - a_x b_z \\ a_x b_y - a_y b_x \end{bmatrix} _{3 \times 1} \Rightarrow \left[\mathbf{a}\right]_{\times} = \begin{bmatrix} 0 & -a_z & a_y \\ a_z & 0 & -a_x \\ -a_y & a_x & 0 \end{bmatrix}$$</div>
<div>$$\mathbf{a} \times \mathbf{b} = \left[\mathbf{a}\right]_{\times} \mathbf{b} = \begin{bmatrix} 0 & -a_z & a_y \\ a_z & 0 & -a_x \\ -a_y & a_x & 0 \end{bmatrix} \begin{bmatrix} b_x \\ b_y \\ b_z \end{bmatrix}$$</div>
<p>Using the above property, we can see that the cross product operation can be transformed into matrix operations. Therefore, using the above property, we obtain:</p>
<div>$$\overrightarrow{\mathbf{n}_2}^{2} = \mathbf{t} _{21} \times \left( \mathbf{R} _{21} \cdot \underline{\mathbf{m}}_1 \right) = \left[ \mathbf{t} _{21} \right]_{\times} \cdot \mathbf{R} _{21} \cdot \underline{\mathbf{m}}_1$$</div>
<p>Because $\overrightarrow{\mathbf{n}_2}^{2}$ is the normal vector of $\mathbf{m}_2$, we have</p>
<p>$\Rightarrow \mathbf{m}_2^T \cdot \overrightarrow{\mathbf{n}_2}^{2} = 0$</p>
<div>$$\mathbf{m}_2^T \cdot \left[ \mathbf{t} _{21} \right]_{\times} \cdot \mathbf{R} _{21} \cdot \underline{\mathbf{m}}_1 = 0$$</div>
<div>$$\mathbf{m}_2^T \cdot \left( \left[ \mathbf{t} _{21} \right]_{\times} \cdot \mathbf{R} _{21} \right) \cdot \underline{\mathbf{m}}_1 = 0$$</div>
<p><strong>4.4 Essential Matrix (Matrice Essentielle)</strong></p>
<p>​	1.	<strong>Formula</strong></p>
<p>Assume</p>
<div>$$\mathbf{E} _{21} = \left[ \mathbf{t} _{21} \right]_{\times} \cdot \mathbf{R} _{21} \quad \Rightarrow \quad \text{essential matrix}$$</div>
<p>It contains information about the relative rotation $\mathbf{R}$ and translation $\mathbf{t}$ between the two cameras.</p>
<p>The original equation becomes $ \underline{\mathbf{m}}*2^T \cdot \mathbf{E} _{21} \cdot \underline{\mathbf{m}}_1 = 0 $</p>
<p>​	2.	<strong>Degrees of Freedom</strong></p>
<div>$$
<p>5 \text{ degrees of freedom} \ \downarrow\ 5 \text{ DoF} \left( \begin{array}{c} 3 , \mathbf{R} _{21} \quad \text{rotation} \ \quad 2 , \mathbf{t} _{21} \quad \text{translation} \end{array} \right)\ \downarrow\ \quad \quad | \mathbf{t} _{21} |_2 \quad \text{ unknown}</p>
<p>$$</div></p>
<p><strong>Degrees of Freedom:</strong></p>
<div>$$\left\{ \begin{aligned} 
<p>&amp;\text{The rotation matrix } \mathbf{R} \text{ has 3 degrees of freedom} \</p>
<p>&amp;\text{The translation vector } \mathbf{t} \text{ has 2 degrees of freedom (since the scale is unknown)} \</p>
<p>&amp;\text{Therefore, } \mathbf{E} \text{ has 5 degrees of freedom}</p>
<p>\end{aligned} \right.$$</div></p>
<ul>
<li>
<p><strong>Degrees of Freedom (DoF)</strong> refer to the number of independent parameters required to describe the essential matrix. In geometry and linear algebra, DoF reflects the number of independent directions or ways a system can vary without constraints.</p>
</li>
<li>
<p>The rotation matrix has 3 degrees of freedom, describing rotation in three-dimensional space.</p>
</li>
<li>
<p>The translation vector theoretically has 3 degrees of freedom in three-dimensional space. However, in the essential matrix, the translation vector typically only considers direction, ignoring magnitude (unknown scale), thus leaving the translation vector with only 2 effective degrees of freedom, describing the direction of translation.</p>
</li>
</ul>
<p><strong>4.5 Fundamental Matrix (Matrix Fundamental)</strong></p>
<p>Continuing the transformation of the above formula:</p>
<div>$$\underline{\mathbf{m}}_2^T \cdot \mathbf{E} _{21} \cdot \underline{\mathbf{m}}_1 = 0$$</div>
<p>Given:</p>
<div>$$\left\{ \begin{aligned} 
<p>\underline{\mathbf{m}}_2 = K^{-1} \cdot \underline{\mathbf{P}}_2 \</p>
<p>\underline{\mathbf{m}}_1 = K^{-1} \cdot \underline{\mathbf{P}}_1</p>
<p>\end{aligned} \right.$$</div></p>
<div>$$\underline{\mathbf{P}}_2^T \cdot (K^{-1})^T \cdot \mathbf{E} _{21} \cdot K^{-1} \cdot \underline{\mathbf{P}}_1 = 0$$</div>
<p>When the camera intrinsics are unknown or not considered, we introduce a fundamental matrix $\mathbf{F}$ to encapsulate $K$.</p>
<p>Assume $\mathbf{F} _{21} = (K^{-1})^T \cdot \mathbf{E} _{21} \cdot K^{-1}$</p>
<div>$$\mathbf{F} _{21} : \text{ fundamental matrix} \quad \Rightarrow \quad 7 \text{ DoF}\quad \left\{ \begin{aligned} 
<p>&amp; \text{- homogeneous matrix} \</p>
<p>&amp; \text{- rank}(\mathbf{F} _{21}) = 2 \quad \Rightarrow \quad \det(\mathbf{F} _{21}) = 0</p>
<p>\end{aligned} \right.$$</div></p>
<p><strong>Properties:</strong></p>
<div>$$\left\{ \begin{aligned} 
<p>&amp;\text{Homogeneous: The fundamental matrix } \mathbf{F} \text{ is a homogeneous matrix, and it can be scaled by any non-zero scalar without changing its properties} \</p>
<p>&amp;\text{Rank Constraint:} \mathbf{F} \text{ has rank } 2</p>
<p>\end{aligned} \right.$$</div></p>
<p>The original equation becomes $ \underline{\mathbf{P}}*2^T \cdot \mathbf{F} _{21} \cdot \underline{\mathbf{P}}_1 = 0 $</p>
<div>$$\text{Let:} \quad \mathbf{L}_2 = \mathbf{F} _{21} \cdot \underline{\mathbf{P}}_1 = \begin{bmatrix} a \\ b \\ c \end{bmatrix}$$</div>
<div>$$\underline{\mathbf{P}}_2^T \cdot \mathbf{L}_2 = 0 \quad \Leftrightarrow \quad a P_{2,x} + b P_{2,y} + c = 0$$</div>
<p>This is the equation of a line on Camera 2’s image plane $\Rightarrow$ Epipolar Line</p>
<p><strong>4.6 Estimation of Essential and Fundamental Matrices</strong></p>
<ul>
<li>
<p><strong>Camera Calibrated</strong> $\Rightarrow$ Estimate the essential matrix $\mathbf{E}$ (5 degrees of freedom) $\Rightarrow$ 5-point correspondence algorithm</p>
</li>
<li>
<p><strong>Camera Uncalibrated</strong> $\Rightarrow$ Estimate the fundamental matrix $\mathbf{F}$ (7 degrees of freedom) $\Rightarrow$ 7-point correspondence algorithm</p>
</li>
<li>
<p><strong>Solution</strong> $\Rightarrow$ 8-point correspondence algorithm $\Rightarrow$ Intentionally ignoring the constraint $\det(\mathbf{F}) = 0$</p>
</li>
</ul>
<p><strong>4.7 Algorithm</strong></p>
<p><strong>Steps of the 8-Point Correspondence Algorithm:</strong></p>
<p>​	1.	<strong>Collect Corresponding Point Pairs:</strong></p>
<p>Although $\mathbf{F}$ has 7 degrees of freedom, the algorithm ignores the rank-2 constraint. Therefore, at least 8 pairs of corresponding points are required to estimate $\mathbf{F}$.</p>
<p>​	2.	<strong>Construct a Linear System of Equations</strong></p>
<p>For each pair of corresponding points $(\mathbf{m}_1, \mathbf{m}_2)$, construct the equation:</p>
<div>$$\underline{\mathbf{m}}_2^T \cdot \mathbf{E} _{21} \cdot \underline{\mathbf{m}}_1 = 0$$</div>
<div>$$\underline{\mathbf{P}}_2^T \cdot \mathbf{F} _{21} \cdot \underline{\mathbf{P}}_1 = 0$$</div>
<p>​	3.	<strong>Solve:</strong></p>
<p>Represent the system of equations as: $\underline{\mathbf{P}}_2^T \cdot \mathbf{L}_2 = 0$</p>
<p>​	4.	<strong>RANSAC Algorithm Steps</strong></p>
<p>Handle outliers (incorrect matches) in the set of corresponding points and robustly estimate $\mathbf{F}$.</p>
<ul>
<li>
<p><strong>Random Sampling</strong>: Use the <strong>8-point correspondence algorithm</strong> to estimate $\mathbf{F}$.</p>
</li>
<li>
<p><strong>Model Evaluation</strong>: Use the estimated $\mathbf{F}$ to calculate the epipolar constraint error for all corresponding point pairs, i.e., the distance from each point to its corresponding epipolar line.</p>
</li>
<li>
<p><strong>Determine Inliers</strong>: Based on a set distance threshold, determine which corresponding points are inliers.</p>
</li>
<li>
<p><strong>Iteration</strong>: Repeat the above process until the model with the highest number of inliers is found.</p>
</li>
</ul>
<p><strong>5. Bundle Adjustment</strong></p>
<ul>
<li>
<p>Bundle adjustment is a technique that simultaneously optimizes camera parameters (including position, orientation, and intrinsic parameters) and the positions of three-dimensional points in the scene.</p>
</li>
<li>
<p>Its core idea is to make the optimized model more consistent with actual observations by minimizing the re-projection error of three-dimensional points onto the images.</p>
</li>
<li>
<p>Remember these five words: <strong>Minimize Re-projection Error</strong></p>
</li>
</ul>
<p><strong>5.1 Case of Two Cameras</strong></p>
<p><strong>5.1.1 Data</strong></p>
<p>$$\left( P_{A,i}, P_{B,i} \right) _{i=1,\dots,N} \implies N \text{ correspondences}$$</p>
<p><strong>5.1.2 Parameters to Estimate</strong></p>
<p>Camera poses and the three-dimensional point cloud data set.</p>
<div>$$\mathbf{R} _{W1} \quad \mathbf{t} _{W1} \quad \mathbf{R} _{W2} \quad \mathbf{t} _{W2} \quad \left\{\mathbf{U}^w_i \right\} _{i=1,\dots,N}$$</div>
<p><strong>5.1.3 Loss Function</strong></p>
<div>$$\mathcal{L} \left( \mathbf{R} _{w1}, \mathbf{t} _{w1}, \mathbf{R} _{w2}, \mathbf{t} _{w2}, \left\{ \mathbf{U}^w_i \right\} _{i=1,\dots,N} \right) = \sum_{i=1}^{N} \left( \left\lVert P_{1,i} - K_1 \Pi \left( \mathbf{R} _{w1}^T \mathbf{U}_i^{w} - \mathbf{R} _{w1}^T \mathbf{t} _{w1} \right) \right\rVert_2^2 + \left\lVert P_{2,i} - K_2 \Pi \left( \mathbf{R} _{w2}^T \mathbf{U}_i^{w} - \mathbf{R} _{w2}^T \mathbf{t} _{w2} \right) \right\rVert_2^2 \right)$$</div>
<p>Where:</p>
<ul>
<li>
<p>$K_A$ and $K_B$ are the intrinsic matrices of cameras $A$ and $B$, respectively.</p>
</li>
<li>
<p>$\Pi(\cdot)$ is the projection function that projects three-dimensional points onto a two-dimensional plane.</p>
</li>
<li>
<p>$\mathbf{R} _{w1}^T$ and $\mathbf{R} _{w2}^T$ are equivalent to $\mathbf{R} _{1w}$ and $\mathbf{R} _{2w}$, which transform points from the world coordinate system to the camera coordinate system.</p>
</li>
<li>
<p>$\mathbf{R} _{w1}^T \mathbf{t} _{w1}$ is equivalent to $\mathbf{t} _{1w}$, representing the translation vector.</p>
</li>
<li>
<p>$\mathbf{U}_i^{1} = \mathbf{R} _{w1}^T \cdot \mathbf{U}_i^{w} - \mathbf{R} _{w1}^T \cdot \mathbf{t} _{w1}$, which transforms $\mathbf{U}_i^{w}$ to $\mathbf{U}_i^{1}$, i.e., from the world coordinate system to the camera coordinate system.</p>
</li>
<li>
<p><strong>Difference</strong>: The image coordinates in camera $A$ or $B$ (actual) minus the estimated image coordinates obtained through three-dimensional space rotation and transformation equals the re-projection error.</p>
</li>
</ul>
<p><strong>5.2 Case of Multiple Cameras</strong></p>
<p><strong>5.2.1 Data</strong></p>
<p>Detected points in each image are:</p>
<div>$$\left\{ \left\{ P_{m,i} \right\} _{i=1,\dots,N_m} \right\} _{m=1,\dots,M}$$</div>
<ul>
<li>
<p>These points form tracks across different viewpoints.</p>
</li>
<li>
<p>Points detected by the $m$-th camera, where $N_m$ is the number of points detected by the $m$-th camera.</p>
</li>
</ul>
<div>$$\left\{ \text{p2d-id}_m, \ \text{p3d-id}_m \right\} _{m=1,\dots,M}$$</div>
<p>Where:</p>
<ul>
<li>
<p>$\text{p2d-id}_m$ is the index of the two-dimensional point in the image.</p>
</li>
<li>
<p>$\text{p3d-id}_m$ is the index of the corresponding three-dimensional point in the point cloud.</p>
</li>
<li>
<p>Both have dimensions of $C_m \times 1$.</p>
</li>
</ul>
<p><strong>5.2.2 Parameters to Estimate</strong></p>
<ul>
<li><strong>Camera Extrinsics</strong>:</li>
</ul>
<div>$$ \left\{ \left( \mathbf{R} _{wm}, \mathbf{t} _{wm} \right) \right\} _{m=1,\dots,M} $$</div>
<ul>
<li><strong>Positions of Three-Dimensional Points</strong>:</li>
</ul>
<div>$$ \left\{ \mathbf{U}_i^{w} \right\} _{i=1,\dots,N} $$</div>
<p><strong>5.2.3 Loss Function</strong></p>
<p>The cost function extends to calculate errors across all cameras and all detected points, minimizing the distance between projected points and actual observed points:</p>
<div>$$\mathcal{L}(x) = \sum_{m=1}^{M} \sum_{c=1}^{C_m} \left\| P_{m,\ \text{p2d-id}_m(c)} - K_m \Pi \left( \mathbf{R} _{wm}^\top \mathbf{U} _{\text{p3d-id}_m(c)}^{w} - \mathbf{R} _{wm}^\top \mathbf{t} _{wm} \right) \right\|_2^2$$</div>
<ul>
<li>
<p>$C_m$ is the number of observations for the $m$-th camera.</p>
</li>
<li>
<p>$\mathbf{U} _{\text{p3d-id}_m(c)}^{w}$ is the three-dimensional point corresponding to the observation.</p>
</li>
</ul>
<p>We can simply simplify the above cost function to:</p>
<div>$$\mathcal{L}(x) = \sum_{i=1}^{N} \left\| f_i(x) \right\|_2^2 \quad \left\{ \begin{array}{l} x \in \mathbb{R}^D \\ f_i : \mathbb{R}^D \rightarrow \mathbb{R}^B \end{array} \right.$$</div>
<ul>
<li>
<p>$x$ represents all parameters to be optimized (camera parameters and three-dimensional point coordinates).</p>
</li>
<li>
<p>$f_i(x)$ is the $i$-th residual function, representing the re-projection error of the $i$-th observation.</p>
</li>
<li>
<p>Our goal is to find $x$ that minimizes $\mathcal{L}(x)$. This is a non-linear least squares problem, typically solved using iterative methods.</p>
</li>
</ul>
<p><strong>5.3 Gauss-Newton Algorithm</strong></p>
<ul>
<li>An iterative optimization algorithm used for non-linear least squares problems. $\Rightarrow$ <strong>Iterative</strong> $\quad \delta_{k+1} = \delta_k + d_k$</li>
</ul>
<p>​	1.	<strong>Linearization of</strong> $f_i$:</p>
<div>$$f_i(x_k + d_k) \approx f_i(x_k) + \mathbf{J}_i(x_k) \cdot d_k$$</div>
<ul>
<li>
<p>$\delta x$ is the increment of the parameters to be solved.</p>
</li>
<li>
<p>For each iteration, we perform a Taylor expansion of $f_i(x)$ around the current estimate $x_k$ and ignore higher-order terms.</p>
</li>
<li>
<p>$f_i(x_k + d_k) \in \mathbb{R}^B$</p>
</li>
<li>
<p>$f_i(x_k) \in \mathbb{R}^B$</p>
</li>
<li>
<p>$\mathbf{J}_i(x_k) \in \mathbb{R}^{B \times D}$</p>
</li>
<li>
<p>$d_k \in \mathbb{R}^D$</p>
</li>
</ul>
<p>​	2.	<strong>Jacobian Matrix:</strong></p>
<div>$$\mathbf{J}_i(x_k) = \frac{\partial f_i(x_k + d_k)}{\partial d_k} \bigg|_{d_k=0}$$</div>
<ul>
<li>
<p>Represents the partial derivatives of the function $f_i$ with respect to $d_k$ at the point $x_k$.</p>
</li>
<li>
<p>Describes the linear rate of change of the function $f_i$ at the point $x_k$.</p>
</li>
</ul>
<p>​	3.	<strong>Linear Least Squares:</strong></p>
<p>$$
L_k(d_k) = \sum_{i=1}^{N} \left| f_i(x_k) + \mathbf{J}_i(x_k) \cdot d_k \right|_2^2
$$</p>
<div>$$\quad \mathbf{J}_k = 
<p>\begin{bmatrix}</p>
<p>\quad J_1(x_k) \</p>
<p>\quad J_2(x_k) \</p>
<p>\quad J_3(x_k) \</p>
<p>\vdots \</p>
<p>\quad J_N(x_k)</p>
<p>\end{bmatrix} \quad \mathbf{b}_k =</p>
<p>\begin{bmatrix}</p>
<p>\quad f_1(x_k) \</p>
<p>\quad f_2(x_k) \</p>
<p>\quad \vdots \</p>
<p>\quad f_N(x_k)</p>
<p>\end{bmatrix}$$</div></p>
<ul>
<li>
<p>$\mathbf{b}_k$ is the combination of all residuals.</p>
</li>
<li>
<p>The linear least squares problem becomes:</p>
</li>
</ul>
<div>$$
<p>L_k(d_k) = \lVert \mathbf{b}_k + \mathbf{J}_k \cdot d_k \rVert_2^2</p>
<p>$$</div></p>
<p>By minimizing $L_k(d_k)$, we obtain the linear system of equations:</p>
<div>$$\mathbf{J}_k^\top \cdot \mathbf{J}_k \cdot d_k = -\mathbf{J}_k^\top \cdot \mathbf{b}_k \quad $$</div>
<ul>
<li>
<p>Here, the left matrix $\mathbf{J}_k^T \mathbf{J}_k$ is an approximation of the Hessian matrix, and the right vector $-\mathbf{J}_k^T \mathbf{b}_k$ is the negative of the gradient.</p>
</li>
<li>
<p>Solving this linear system yields the parameter update $d_k$.</p>
</li>
</ul>
<p>​	4.	<strong>Levenberg-Marquardt Algorithm</strong></p>
<p>Introduces a damping factor $\lambda$ to the Gauss-Newton algorithm, allowing the optimization process to exhibit the fast convergence of Gauss-Newton when close to the solution and the stability of gradient descent when far from the solution.</p>
<ul>
<li>
<p>Commonly used for iterative optimization in non-linear least squares problems.</p>
</li>
<li>
<p><strong>Objective Function:</strong></p>
</li>
</ul>
<div>$$L_k(d_k) = \lVert \mathbf{b}_k + \mathbf{J}_k d_k \rVert_2^2 + \lambda \lVert d_k \rVert_2^2 \quad \quad \Rightarrow (J_k^T J_k + \lambda I_k) d_k = -J_k^T b_k$$</div>
<ul>
<li>$\lambda$ is the damping factor.</li>
</ul>
<div>$$\begin{cases} 
<p>\text{Si } \lambda = 0 &amp; \Rightarrow \text{Gauss-Newton} \</p>
<p>\text{Si } \lambda \rightarrow +\infty &amp; \Rightarrow \lambda d_k \rightarrow -J_k^T b_k \quad \text{descente de gradient}</p>
<p>\end{cases}$$</div></p>
<ul>
<li>
<p><strong>If the new cost function value decreases</strong> (indicating an effective update), reduce $\lambda$ to make the algorithm closer to Gauss-Newton, accelerating convergence.</p>
</li>
<li>
<p><strong>If the cost function value does not decrease</strong>, increase $\lambda$ to make the algorithm closer to gradient descent, ensuring stability.</p>
</li>
</ul>
<p><strong>5.4 Summary of Algorithm Steps</strong></p>
<p>In practical applications, the steps of the Levenberg-Marquardt algorithm are as follows:</p>
<p>​	1.	<strong>Initialization</strong>:</p>
<ul>
<li>
<p>Set initial parameters $x$ and damping factor $\lambda$.</p>
</li>
<li>
<p>Compute the initial cost function $L_{\min}$.</p>
</li>
</ul>
<p>​	2.	<strong>Iteration</strong>:</p>
<ul>
<li>
<p><strong>Compute Jacobian Matrix</strong> $\mathbf{J}$ and residuals $\mathbf{b}$.</p>
</li>
<li>
<p><strong>Solve Linear System</strong>:</p>
</li>
</ul>
<div>$$ (J^T J + \lambda I_d) d = -J^T b$$</div>
<ul>
<li><strong>Update Parameters</strong>:</li>
</ul>
<div>$$x' = x + d$$</div>
<ul>
<li><strong>Compute New Cost Function</strong> $L’$</li>
</ul>
<p>​	3.	<strong>Evaluate Update Effectiveness</strong>:</p>
<ul>
<li>
<p><strong>If $L’ &lt; L_{\min}$</strong> (cost function decreases):</p>
</li>
<li>
<p>Accept the update: $x = x’$, $L_{\min} = L’$.</p>
</li>
<li>
<p>Decrease $\lambda$: $\lambda = \lambda / 2$.</p>
</li>
<li>
<p>Continue iteration.</p>
</li>
<li>
<p><strong>Otherwise</strong> (cost function does not decrease):</p>
</li>
<li>
<p>Reject the update, do not change $x$.</p>
</li>
<li>
<p>Increase $\lambda$: $\lambda = 2\lambda$.</p>
</li>
<li>
<p>Check if $\lambda$ exceeds the maximum value; if so, stop iteration.</p>
</li>
</ul>
<p>​	4.	<strong>Termination Conditions</strong>:</p>
<ul>
<li>When $\lambda$ exceeds a preset maximum value or the parameter updates become smaller than a threshold, stop iterating.</li>
</ul>
<p>All content in this document is theoretical and part of the ‘Vidéo 3D - <strong>Computer Vision</strong>’ course, taught by instructor <strong>Guillaume Bourmaud</strong>. All rights are reserved by the instructor.</p>
<p>For specific experimental content and complete code, please refer to <strong>Guillaume Bourmaud</strong>’s official website: <a href="https://gbourmaud.github.io/teaching/">https://gbourmaud.github.io/teaching/</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/image-processing/">Image Processing</a></li>
      <li><a href="http://localhost:1313/tags/computer-vision/">Computer Vision</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/signal/test/">
    <span class="title">« Prev</span>
    <br>
    <span>tst title</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/signal_cn/%E9%9A%8F%E6%9C%BA%E4%BF%A1%E5%8F%B7tp3/">
    <span class="title">Next »</span>
    <br>
    <span></span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span><a href="https://github.com/adityatelange/hugo-PaperMod/graphs/contributors">PaperMod</a></span> · 


    <span>
        
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a>  
        
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
