[{"content":"第一章: 介绍 1.1 无监督学习概述 1.1.1 有监督学习 有监督学习，就是数据集有样本有标签，其目标是学习 $x$ 到 $y$ 的映射，相当于告诉正确答案，如果训练中做错了就根据答案调整一下。\n1.1.2 无监督学习 无监督学习，数据没有类标，那就不是映射了，其目标就是找到隐含在数据里的模式和结构，常见任务有：\n聚类任务 降维: 三维到二维，且同时保留有用信息。最常用的降维就是主成分分析，是一种线性降维方式 特征学习: 也是一种降维，使用了自编码器结构，将一个输入图像通过神经网络映射到一个低维空间，同时注意，保留的主要信息还得能够重构自己，区别在于这是非线性方法 密度估计：这就是我们下面要重点讲的方面——概率密度函数估计 1.2 生成模型与概率密度建模 1.2.1 生成模型 生成模型的定义\n我们给机器一个训练样本集，希望他能产生与训练样本集同分布的新样本。\n比如说有一组训练数据，服从其概率密度分布为 $p_{\\text{data}}(x)$，我们希望找到一个模型，其概率密度分布为 $p_{\\text{model}}(x)$ ，并且和训练样本的概率密度分布相近（即概率密度函数一样），因此可以从这个 (新构造的) 模型中采样数据从而产生新的样本，并且新样本服从 $p_{\\text{model}}(x)$这个概率密度分布。\n换句话说，我们用了一个模型去逼近这个原始真实输入数据分布 $p_{\\text{data}}(x)$ ，但是注意，真实分布 $p_{\\text{data}}(x)$ 是没有办法知道的，因为它太高维了，只能用一个模型去逼近。有了这个模型我们就可以从其中采样了。\n总而言之，生成模型通过学习输入数据的的联合概率密度分布，学习数据的生成过程，来产生新的且合理的数据样本\n生成模型分类\n根据概率密度估计方法的不同分为两类：\n显式的：估计的分布方程是可以写出来的。换句话说，模型可见。 RNN 的概率密度函数可定义、也可解。 VAE 密度函数能定义但是不可解。 隐式的：不要问模型长什么样子，也不要问概率分布是什么样的，反正我能生成样本，你要什么我就给你什么样的新样本。模型不可见。 GAN 完全不需要密度函数。 1.2.2 概率密度估计建模 机器学习中的不确定性建模\n机器学习，特别是生成模型中的核心理念之一：世界上的一切都是不确定的，任何事实发生的背后都有一个对应的概率分布。机器学习模型（如深度神经网络、生成模型）试图去建模真实世界中的不确定性。换句话说，我们认为数据（无论是图像、文本、音频等）都是由某种潜在的概率分布生成的。在生成模型中，我们希望通过学习这个分布来进行新的数据生成。\n任务目标\n因此这也引出了无监督模型里的一个核心问题，概率密度估计问题。\n初始的输入数据是有限的样本集，我们的目标是利用这些样本来构建一个生成模型，该模型能够生成与原始数据分布相似的无限新数据。这相当于用一个模型来近似原始数据分布。\n因此，我们面临的核心问题是：如何通过有限的样本去近似这个未知的、且可能是高维的原始数据分布。这也是密度估计问题的本质：在已知样本的前提下，估计出样本所属的概率密度函数（PDF）。即 密度估计的任务就是近似原始数据的概率密度分布。\n概率建模定义\n对于给定的输入数据，我们通过构建模型来推断出数据背后的潜在生成机制，并推断其最可能的输出数据。\n基本组成部分\n随机变量： 是一个不确定的变量，离散或连续都可以，它们的值可以通过概率分布来描述。 概率分布： 概率分布定义了随机变量取值的可能性。 对于随机变量，其概率分布是未知的，但是我们可以先入为主 假设其概率分布，例如正态分布、离散分布或指数分布。这种假定的分布就是我们构建的假设空间。 联合分布和条件分布： **联合分布 **表示多个随机变量同时出现的概率。例如，如果有两个变量 $X$ 和 $Y$，那么联合分布 $P(X, Y)$ 描述了 $X$ 和 $Y$ 同时发生的可能性。 条件分布 描述在已知某些变量取值的情况下，其他变量的概率分布。比如，$P(X|Y)$ 表示在已知 $Y$ 的情况下，$X$ 的分布。 监督学习中的概率建模\n在监督学习中，我们希望通过已知的数据样本 $D = {(x_i, y_i)}$ 来学习 $x$ 和 $y$ 之间的条件概率分布 $P(y|x)$。因此在给定新数据 $x$ 时，可以进行预测 $y$ (比如说给定一个猫的图片$x$ ，然后判断是 +1 的概率是多少 0 的概率是多少)。典型的基于概率密度建模的监督学习方法有逻辑回归、朴素贝叶斯和贝叶斯网络。\n非监督学习中的概率建模\n没有标签 y 与数据 x 的对应关系。因此我们需要从 未标注的数据样本 $D = {x_i}$ 中挖掘数据的潜在结构、分布或者规律。因此概率建模的目标是估计 $P(x)$ ，而不再是条件概率。典型的基于概率密度建模的非监督学习方法有 高斯混合模型 (GMM)、 核密度估计 (KDE) 和 变分自动编码器 (VAE)\n概率建模的步骤\n定义问题：找到随机变量，并且确定随机变量之间的相关性 (独立与否)，同时找到可观测的随机变量及不可观测的随机变量。 选择模型：选择一个合适描述这些随机变量分布的概率模型。比如，线性回归、逻辑回归、高斯分布、混合高斯模型等。 推断与学习：根据观测数据 $X$ 来估计模型参数$\\theta$ ，可以通过贝叶斯推断或最大似然估计等方法。 预测与决策：用训练好的模型对新数据进行预测、分类，或生成与数据分布相似的新样本。 贝叶斯推断\n贝叶斯推断是基于概率建模的重要推断方法。贝叶斯推断方法通过先验分布 $P(\\theta)$ 和似然函数 $P(X | \\theta)$ 来更新对参数的相信度： $$ P(\\theta | X) = \\frac{P(X|\\theta)P(\\theta)}{P(X)} $$ 其中，$\\theta$ 是模型的参数，$X$ 是观测数据。贝叶斯推断是一种动态的过程：从“先验假设”出发，结合“观测证据”，更新“后验认知”。\n朴素贝叶斯分类器 (监督学习中的分类问题)\n在朴素贝叶斯模型中，$Y$ 是目标变量，表示分类标签 (对于二分类是 0 或 1；或者离散值集合$ {C_1, C_2, \\dots, C_k} $ 的多分类)。 $X = (x_1, x_2, \\dots, x_n)$ ：输入特征，假设互相自独立。我们希望通过学习条件概率 $P(Y|X)$ 来对样本 X 进行分类。根\n据贝叶斯定理 即条件概率： $$ P(Y | X) = \\frac{P(X | Y) P(Y)}{P(X)} $$ 由于假设特征独立，我们可以将 $P(X|Y)$ 拆解为： $$ P(X | Y) = P(x_1, x_2, \\dots, x_n | Y) = \\prod_{i=1}^{n} P(x_i | Y) $$ 将多维特征联合分布的计算转化为单个特征的条件概率的乘积。现在只需要分别计算每个特征 $x_i$ 的条件概率 $P(x_i|Y)$，简化计算。\n最后选择能够最大后验概率的分类标签 Y $$ \\hat{Y} = \\arg\\max_Y P(Y) \\prod_{i=1}^n P(x_i | Y) $$ 独立性假设 是朴素贝叶斯的核心假设，现实中不可能完全独立。\n变分自编码器（VAE）\nVAE 是生成模型的一个典型例子，它通过概率建模来学习数据的隐含表示和生成过程。VAE 使用变分推断近似后验分布 Q(Z|X) 来计算对数似然的下界（Evidence Lower Bound，ELBO）：\n$$ \\log P(X) \\geq \\mathbb{E}_{Q(Z|X)} \\left[\\log P(X|Z)\\right] - D_{\\text{KL}}(Q(Z|X) \\| P(Z)) $$ ​\t•\t第一项：重构损失（$ \\mathbb{E}_{Q(Z|X)}[\\log P(X|Z)]$）\n表示使用潜在变量 Z 重构原始数据 X 的好坏，衡量生成数据与真实数据的接近程度。\n​\t•\t第二项：KL 散度项 （$ D_{\\text{KL}}(Q(Z|X) | P(Z)$ )\n表示近似后验分布 Q(Z|X) 与先验分布 P(Z) 的差异。通过最小化它，使潜在变量的分布接近设定的先验（通常是标准正态分布）。\n先不做展开，后续会极大篇幅讲解VAE原理及数学背景以及必要的证明。\n概率密度建模\n由于后续会大规模讲解潜在变量，因此概率建模会细致到概率密度建模\nVAE 的目标之一是学习在给定潜在变量 Z 的条件下观测数据 X 的概率密度，即 $P(X|Z$ ，因此如何从潜在变量生成观测数据，从而实现对数据分布的近似建模。这就是概率密度建模 问题\nVAE使用了变分推断，由于后验分布 $P(Z|X)$ 复杂，我们要使用优化方法来近似它，其核心思想是 在潜在变量 Z 的概率空间中寻找一个最优的近似分布$ Q(Z|X)$，使其尽可能接近真实的后验分布 $P(Z|X)$ 。这同样属于概率密度建模的范畴 (复杂概率分布的近似和优化)\n概率密度建模的任务是找到数据的概率密度函数 $P_{\\theta}(x)$，但是它必须满足以下两个基本约束条件。\n$P_{\\theta}(x) \\geq 0, \\forall x \\in \\mathcal{X}$：概率密度函数必须为非负的。 $\\int P_{\\theta}(x) dx = 1, \\forall \\theta \\in \\Theta$：概率密度函数需要归一化，使得其积分为1（确保总概率为1）。 概率密度建模的困难\n在概率密度建模中，往往需要对概率密度函数的对数求梯度，即$\\nabla_{\\theta} \\log P_{\\theta}(x)$ 不容易计算，其具体解释如下\n高维积分的不可解析性 $$ P(x) = \\int P(x, z) dz=\\int P(x|z) P(Z) , dz $$ 边缘概率表示通过积分消去潜在变量 $z$ 的影响来得到 $P(x)$。在实际应用中，求解这个积分往往没有解析解 (数据维度高、模型复杂)，直接计算 $P(x)$ 是不可行的\n梯度的复杂性\n真实的后验分布 P(Z|X) 是难以计算甚至无法表示的，这使得直接基于后验分布进行梯度计算是难实现的\n$$ \\nabla_\\theta \\log P_\\theta(x) = \\frac{\\nabla_\\theta P_\\theta(x)}{P_\\theta(x)} = \\frac{\\int \\nabla_\\theta P_\\theta(x|z) P(Z) , dz}{\\int P_\\theta(x|z) P(Z) , dz} $$\n高维数据采样生成的复杂性\n在图像生成任务中，像素空间可能具有数十万维度（ 例如 256×256×3 ），而真实数据分布可能集中在一个复杂的低维流形上。因此从高维分布直接采样既困难又不准确。\n1.3 概率分布假设 1.3.1 高斯分布 我们通常对概率分布的形式做出某种假设（因为我们本身不知道它的概率分布）。最常用的是 假设其概率分布是高斯分布：\n假设概率分布是多维高斯分布 $\\mathcal{N}(\\mu_{\\theta}(x), \\Sigma_{\\theta}(x))$，其中均值 $\\mu_{\\theta}(x)$ 和协方差 $\\Sigma_{\\theta}(x)$ 是数据 $x$ 的函数，并且由参数 $\\theta$ 控制。这个假设简化了模型的计算，因为对于高斯分布，许多操作都有解析解，比如极大似然估计。\n多维正态分布的概率密度函数（PDF）为：\n$f(z) = \\frac{1}{\\sqrt{(2\\pi)^d |\\Sigma_\\theta(x)|}} \\exp\\left( -\\frac{(z - \\mu_\\theta(x))^T (z - \\mu_\\theta(x))}{2\\Sigma_\\theta(x)} \\right)$\n对比一维正态分布的概率密度函数(PDF):\n$f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right)$\n1.3.2 极大似然估计（MLE） 详细内容见极大似然估计文章\n经常使用极大似然估计（MLE）来推断高斯分布的参数 $\\mu_{\\theta}, \\Sigma_{\\theta}$ 。其目标是：给定一个数据 X ，找到能够最大化数据发生概率的模型参数$\\theta$ 。\n假设我们有一个概率模型 $P(X|\\theta)$，其中 $X$ 是观测到的数据，$\\theta$ 是我们希望估计的参数。MLE 的目标是找到一个参数估计值 $\\hat{\\theta}$，使得在参数 $\\hat{\\theta}$ 下，数据 $X$ 的似然函数最大化\n1.3.3 VAE 中的概率分布假设 VAE中有两个重要的概率分布：$Q_{\\phi}(Z|X)$ 和 $P_{\\theta}(Z|X)$，它们分别表示从数据 $X$ 到潜在变量 $Z$ 的近似后验分布，以及真实后验分布\n在后续变分推断的步骤中，VAE将有大量的假设概率分布的步骤\n1.4 序列建模与自回归模型 序列建模，特别是 自回归模型（Autoregressive Models） 是通过条件概率展开来建模高维数据的分布。\n1.4.1 条件概率链式法则 对于一个序列化的数据 $X = (x_1, x_2, \\dots, x_n)$，我们可以通过 条件概率链式法则 来表达整个序列的联合概率分布：\n$$ P(x_1, x_2, \\dots, x_n) = P(x_1) P(x_2 | x_1) \\cdots P(x_n | x_1, \\dots, x_{n-1}) $$\n序列中所有变量的联合概率分布等于每个变量在给定前面所有变量条件下的条件概率的连乘。\n1.4.2 自回归模型的原理和实现 自回归模型核心思想是通过条件概率分解来生成序列中的每一个元素。基于条件概率链式法则，每次生成当前元素时都会依赖之前已经生成的元素 ( 每一步的输出用于指导下一步的生成)。\n1.4.3 图像生成中自回归模型的挑战 图像二维空间数据，而自回归模型本质上是为一维序列设计的。因此，在应用自回归模型于图像生成时，必须将二维像素展平成一维序列。例如生成一张分辨率为 $1920 \\times 1080$ 的图片，其像素总数为 $1920 \\times 1080 = 2,073,600 $。这意味着模型需要依次生成长度为 207 万的序列。\n1.5 基于隐变量的生成模型类型 1.5.1 隐变量 隐变量（Latent Variable）是模型中引入的一类未被直接观测到的变量。可以捕获数据的潜在结构并且其模型能够实现降维操作。\n1.5.2 核心思想 基于隐变量的模型会假设观测数据 X 是由某些隐变量 Z 通过某种概率分布生成的，其中结构包括\n先验分布 $P(Z) $：\n通常假设隐变量 Z 服从某个先验分布，例如标准正态分布 P(Z) \\sim \\mathcal{N}(0, I) 。即隐变量的初始状态分布。\n生成过程$ P(X|Z) $\n在给定隐变量 Z 的条件下，观测数据 X 的生成过程被建模为条件分布 P(X|Z) 。这个分布可以由神经网络、混合高斯模型等方法建模。\n后验分布$ P(Z|X) $\n给定观测数据 X，隐变量 Z 的分布可以通过贝叶斯公式推断为 P(Z|X) 。这个后验分布基本上解不出来，变分自编码器 VAE 通过引入近似方法来估计。\n1.5.3 常见的基于隐变量的模型 混合高斯模型（GMM） 隐马尔可夫模型（HMM） 变分自编码器（VAE) 生成对抗网络（GAN） 1.5.4 变分自编码器（VAE）基于隐变量的思想 VAE通过学习数据的潜在表示（隐变量）来生成新的样本，使用了变分推断的思想，将复杂的后验分布近似为简单的高斯分布。其过程简单概述为:\n编码器将输入数据 $X$ 压缩到隐变量 $Z$ 的概率分布。\n解码器从隐变量 $Z$ 生成数据 $X$。\n通过优化损失函数（包括重构误差和KL散度），模型学习到如何从潜在空间生成真实数据。\n1.5.5 扩散模型 (Diffusion Model) 基于隐变量的思想 扩散模型（Diffusion Model）基于概率和隐变量的思想，利用逐步添加噪声和去噪的过程实现数据生成。这种生成模型通过建模从数据分布到噪声分布的正向过程，然后学习其反向去噪过程来生成数据。\n在这个过程中，隐变量是扩散过程中的中间状态 $x_1, x_2, \\dots, x_T $，这些状态表示逐步添加噪声后的数据分布，换句话说，正向扩散过程通过向数据逐步添加噪声，将观测变量 $x_0$ 转化为纯噪声 $x_T$ 。这一过程可以视为构造了一组隐变量 $x_1, x_2, \\dots, x_T $ 。\n详细内容看 扩散模型\n第二章：生成模型的数学基础 详细内容见 生成模型的数学基础\n第三章：基于隐变量的生成模型 3.1 潜在变量的概念 3.1.1 定义 潜在变量（Latent Variables），在统计学和机器学习中，是指那些未被直接观测到的变量，它们隐含在数据背后，控制着可观测变量的行为或模式。好比韩国的财阀，美国的背后集团，看不见老板，但是控制着一切\n在很多生成模型中，观测到的数据 X 通常是通过某些潜在的生成过程产生的，而生成这些数据的因素可以通过潜在变量 Z 来表示。\n观测变量 X：我们可以直接测量或观测到的数据。 潜在变量 Z：隐含在观测数据背后的未知变量，这些变量控制着数据的生成过程，但我们无法直接观测到它们。 举个简单的例子：在天气预测模型中，我们只能观测到温度、湿度、风速等变量，但得到了这些观测变量 X 的用处不大，也就多穿件衣服，我们真正想知道的是导致它这样的 背后规律，即我们无法直接测量的潜在变量 Z（比如说 大气层的复杂状态、地球表面气流的动态等），它们才是关键因素，找到这些决定气候的潜在因素，就可以控制气候\n3.1.2 高维空间与低维子空间的投影 我们观察到的世界可能是一个高维空间到低维子空间的投影。我们观测到的信息 X 并不总是完整的，而只是一些维度的投影。因此，观测到的 X 是潜在变量 Z 的某种反映或表现。\n3.1.3 图像中的潜在变量 尽管一张图片包含了所有的视觉信息，但很多信息是人脑在处理时通过反馈分析得出的。例如，给定一张图片，计算机看到的全是一堆数字，但是人类会自然而然地分析出图片中的颜色、特征等信息，因为大脑通过识别图片中的像素值从而推测出 潜在特征，这些特征就是潜在变量。\n在人脸识别任务中：给定一张人脸图片，潜在变量 Z 可能是皮肤颜色、脸型、发型、眼睛、鼻子的形状等。这些特征不能直接从像素中得知，但通过分析，我们可以推测这些特征控制了图片的生成过程。 潜在空间 $Z$ 两个关键属性\n连续平滑性：潜在空间 $Z$ 应该是连续的，比如说，假如潜在空间中的某个点 $z_1$ 解码为一张“闭着眼的猫”的图像，而另一个点 $z_2$ 解码为一张“睁开眼的猫”的图像，连续性保证我们可以通过从 $z_1$ 到 $z_2$ 的过程中生成一系列图像，代表猫从闭眼到睁眼的平滑变化。 完备性：潜在空间中的任意一个采样点 $Z$ 都应该能够解码成一个有意义的图像，即使是之前从未见过的 $Z$ 也能生成合理的图像。这确保潜在空间 $Z$ 覆盖了所有可能的图像。比如说 $z_1$ 是黑猫， $z_2$ 是白猫，应该合理的生成类似灰猫这样的图片，而不是给我一个狗。 3.1.4 引入潜在空间的必要性 换句话说，为什么要有潜在空间？因为直接在高维空间上对图像进行处理 (建模和采样) 很困难。这因为图像是高维的，而我们又是在像素级别试图对所有的图像进行建模，比如 128×128 的图像有 16,384 维，这个维度上有效的样本是很稀疏的，99%的采样都采到空白处。打个比方，一把豆子在一张纸上很容易找到(采样)，但是在房间里就很难了，更别提在一座大楼及更高维度中，这个难度是急剧增加的。同时，我们无法控制生成过程，比如说我现在就想要一个绿豆子(特定属性的图像)，我们是找不到的，本来找豆子都费劲，就算找到了，五颜六色什么都有随机采样导致是随机色的，根本做不到。\n因此，直接对高维图像 X 建模根本做不了。\n插曲\n为了图像生成，可以让每个像素都服从一个高斯分布，所以我们把每个像素都重新在对应的高斯分布中采样(均值 $\\mu_i$ 就是原始图像当前像素值，方差 $\\sigma_i^2$ 是人为设定的)，这不就能生成新的且非常相似略有不同的图像了吗。\n这个方法有两个缺点，第一，每个像素的分布是独立的，相邻像素没有相关性，比如说这个人胸前画了一个米老鼠，这是有强关联的，特征提取可以直接提取出这是个米老鼠，但是独立像素只会生成唐老鸭形状的麻子。第二，我们只是照猫画猫，照虎画虎，但是给你一道题举一反三照猫画虎，是绝对做不了的，因为它不知道猫的特征，也不知道老虎的特征，它做不到给猫的头上画个王字这么明显的特征。\n引入低维编码空间 Z\n为了解决上述问题，关键在于引入低维潜在空间 Z，将高维的图像的特征映射到一个低维空间上，即捕捉了图像的关键特征，可以被看作是对图像的压缩表示\n隐变量生成模型的优化与采样\n最大似然估计的目标是最大化数据的概率的对数似然函数：\n$$ \\sum_{i} \\log P_\\theta(X_i) $$\n这里的目标是通过对参数 $\\theta$ 进行优化，使得模型生成的样本与真实数据 $X$ 的分布最为接近。但是在VAE这样的生成模型中，建模上面这个东西不容易，因此通过引入隐变量 $Z$ 可以通过 $Z$ 来生成 $X$。原式变为: $$ P_\\theta(X) = \\int P_\\theta(X|Z) P_\\theta(Z) dZ $$\n3.2 变分自编码器（VAE） 在 VAE 中，潜在变量是核心思想。VAE 通过潜在变量空间 Z 的分布 来生成与原数据分布相似的新样本观测数据 X ，即 $Z \\rightarrow X$，这个过程是通过对潜在变量进行建模来捕获数据的生成机制而完成的。\n潜在变量Z\nZ 控制了数据 X 的生成过程。在训练过程中，VAE 会学习如何从一个简单的分布（如标准正态分布）中生成潜在变量 Z，然后通过解码器将 Z 映射到观测数据空间 X 中。\n样本 $(X, Z)$：\n这里样本的概念变了，不再是生成的数据 X，而是由 $X$ (可观测) 和潜在变量 $Z$ (不可观测)构成一个完整的样本。每一个观测样本 $X$ 都对应一个特定的潜在变量 $Z$ (影子)。\n3.2.1 VAE的目标 VAE目标是通过最大化数据 $X$ 的对数似然 $\\log P(X)$ 来学习模型的参数 $\\theta$。因为最开始的时候直接优化 $P(X)$ 很困难，因为它是高维空间的积分，所以给他变形，最终就可以得到对数似然。\n$$ P(X) = \\int_Z P_\\theta(X, Z) dZ $$\n通过对 $Z$ 进行采样并基于条件概率 $P(X|Z)$ 来生成对应的观测数据 $X$ : $$ P(X) = \\int_Z P_\\theta(X|Z) P_\\theta(Z) dZ $$ 求对数似然: $$ \\log P_\\theta(X) = \\log \\int P_\\theta(X|Z) P_\\theta(Z) dZ $$ 变到这里，直接计算这个积分仍然是非常困难的，因此 VAE 引入了变分推断来近似优化这个目标函数，使用 ELBO（Evidence Lower Bound）来近似最大化 $\\log P_\\theta(X)$。\n下面我们来详细推导后续的过程，首先要优化的目标为: $$ \\log P_\\theta(X) = \\log \\int P_\\theta(X|Z) P_\\theta(Z) dZ $$ 首先使用蒙特卡洛（Monte Carlo, MC）方法。其基本思想是通过对 $Z$ 进行采样来近似 $P(X)$，进而计算对数似然。\n上面概率积分的形式可以写成期望：\n$$ P(X) = \\mathbb{E}_{P(Z)} \\left[ P_\\theta(X|Z) \\right] $$ 蒙特卡洛采样过程：\n从 $P(Z)$ 中采样 $Z_1, Z_2, \\dots, Z_n$。\n通过这些采样的 $Z$ 来计算 $P(X)$ 的近似：\n$$ P(X) \\approx \\frac{1}{n} \\sum_{i=1}^{n} P_\\theta(X|Z_i) $$\n$$ \\log P_\\theta(X) \\approx \\log \\left( \\frac{1}{n} \\sum_{i=1}^{n} P_\\theta(X|Z_i) \\right). $$\n然后，我们可以基于这个近似来对参数 $\\theta$ 进行优化。\n但是这个方法也有一个问题，就是维度灾难，高维度情况下，$P(X|Z_i)$ 的值会非常小（接近 0），导致采样效率非常低。但是没关系，我们有办法。\n解决方案：\n为了提高采样效率，我们需要更有效地采样 $Z$，最理想的情况是从 $P(Z|X)$ 中采样 $Z$，因为从 $X$ 中得到的 $Z$ 可以更好地解释数据 $X$\n但是 $P(Z|X)$ 是未知的，我们可以通过一个近似分布 $Q(Z|X)$ 来逼近 $P(Z|X)$。这样我们就能通过 $Q(Z|X)$ 进行采样，并计算：\n$$ \\mathbb{E}_{Q(Z|X)} \\left[ P_\\theta(X|Z) \\right] $$ 这就是变分推断的核心思想，通过引入近似分布 $Q(Z|X)$，我们可以从 Q(Z|X) 采样得到一个高效很漂亮的 $Z$，再用之前的MC方法，可以有效地优化隐变量模型。\n正式推导过程: $$ \\log P(X) = \\log \\int P(X, Z) , dZ $$ 引入近似分布 $Q(Z|X)$ $$ \\log P(X) = \\log \\int P(X, Z) \\frac{Q(Z)}{Q(Z)} , dZ $$\n$$ \\log P(X) = \\log \\mathbb{E}_{Q(Z)} \\left[ \\frac{P(X, Z)}{Q(Z)} \\right] $$\n利用 Jensen 不等式，将 $\\log $穿过积分，即穿过期望。\nJensen 不等式的形式如下：\n​\t•\t定义：对于一个convexe函数 f(x)，任意随机变量 X，有： $$ f(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X)] $$ ​\t•\tconvexe函数 f(x) 的值在平均值点上$ f(\\mathbb{E}[X])$，总是小于或等于对所有 X 的 f(X) 的加权平均值$ \\mathbb{E}[f(X)]$。\n但是$\\log$ 是一个concave函数（对 $(0, \\infty) $范围来说），因此，Jensen 不等式应用到 $\\log$ 上需要改一下方向，因此有 $$ \\log \\mathbb{E}[f(Z)] \\geq \\mathbb{E}[\\log f(Z)] $$\n原式变成:\n$$ \\log P(X) = \\log \\mathbb{E}_{Q(Z)}\\left[\\frac{P(X, Z)}{Q(Z)}\\right] \\geq \\mathbb{E}_{Q(Z)}\\left[\\log \\frac{P(X, Z)}{Q(Z)}\\right] $$ 右边这个式子就是ELBO，即:\n$$ \\mathcal{L}(Q) = \\mathbb{E}_{Q(Z)} \\left[ \\log \\frac{P(X, Z)}{Q(Z)} \\right] $$ 但是这里我们忽略了一个问题，那就是 $\\log P(X)\\geq \\mathbb{E}_{Q(Z)}\\left[\\log \\frac{P(X, Z)}{Q(Z)}\\right]$ 是大于等于，并不是完全等于，为什么？我们需要从新变换数学公式来得到一个等号形式\n3.2.2 变分推断（VI） $$ \\begin{align*} \\log P(X) \u0026= \\mathbb{E}_{Q(Z)} \\log P(X) \\\\ \u0026= \\mathbb{E}_{Q(Z)} \\log \\frac{P(X, Z)}{P(Z \\mid X)} \\\\ \u0026= \\mathbb{E}_{Q(Z)} \\log \\frac{P(X, Z) Q(Z)}{P(Z \\mid X) Q(Z)} \\\\ \u0026= \\mathbb{E}_{Q(Z)} \\log \\frac{P(X, Z)}{Q(Z)} + \\mathbb{E}_{Q(Z)} \\log \\frac{Q(Z)}{P(Z \\mid X)} \\\\ \u0026= \\mathbb{E}_{Q(Z)} \\log \\frac{P(X, Z)}{Q(Z)} + D\\big[Q(Z) \\| P(Z \\mid X)\\big] \\\\ \u0026= \\mathcal{L}(Q) + D_{KL}(Q(Z) \\| P(Z|X)) \\end{align*} $$ $D_{KL}(Q(Z) | P(Z|X)) $ 是 $KL$ 散度，用来衡量近似分布 $Q(Z)$ 和真实后验 $P(Z|X)$ 的差异。当 $Q(Z) = P(Z|X)$ 时，KL 散度为零，优化就完美达成。对于高斯分布， KL 散度有解析解： $$ D_{KL}(Q_\\phi(Z|X) | P(Z)) = \\frac{1}{2} \\left( \\text{tr}(\\Sigma_\\phi(X)) + \\mu_\\phi(X)^\\top \\mu_\\phi(X) - k - \\log \\det(\\Sigma_\\phi(X)) \\right) $$\n因此，最大化 ELBO 等价于同时最小化 KL 散度和提升重建能力。\n由于 $D_{KL} \\geq 0$，因此： $$ \\log P_\\theta(X) \\geq \\mathcal{L}(\\phi, \\theta) $$ 变分的思想就是，通过最大化 $\\mathcal{L}(Q_\\phi) $来逼近$ \\log P(X)$，并找到能最大化$\\mathcal{L}(Q) $的那个$\\phi$ 即，$\\max_\\phi \\mathcal{L}(Q)$ 。\n$$ \\mathcal{L}(Q) = \\mathbb{E}_{Q(Z)} \\left[ \\log \\frac{P(X, Z)}{Q(Z)} \\right] $$\n第一步 拆 $P(X, Z)$ : $$ P(X,Z) = P(X|Z)*P(Z) $$\n第二步 $$ \\mathcal{L}(Q) = \\mathbb{E}_{Q(Z)} \\left[ \\log P(X|Z) + \\log P(Z) - \\log Q(Z) \\right] $$\n第三步 $$ \\mathcal{L}(Q) = \\underbrace{\\mathbb{E}_{Q(Z)} \\left[ \\log P(X|Z) \\right]}_{\\text{Erreur de reconstruction}} - \\underbrace{D_{KL}(Q(Z) \\| P(Z))}_{\\text{Terme de régularisation}} $$ 重建误差项衡量生成图像与原始图像的差异，控制图像质量。正则项（KL 散度），确保潜在空间每个点都能生成有意义的图像。在优化过程中，需要在重建误差和 KL 散度之间找到平衡。\n3.2.3 参数化 ELBO 近似后验分布：$Q(Z|X) = \\mathcal{N}(Z | \\mu_\\phi(X), \\Sigma_\\phi(X))$\n$\\mu_\\phi(X)$ 和 $\\Sigma_\\phi(X)$ 由编码器网络计算。 生成模型：$P(X|Z) = \\mathcal{N}(X | f_\\theta(Z), I)$\n$f_\\theta(Z)$ 由解码器网络计算。 先验分布： $P(Z)$\n通常设为标准正态分布：$P(Z) = \\mathcal{N}(0, I)$ 因此原式变成:\n$$ \\mathcal{L}(\\phi, \\theta) = \\mathbb{E}_{Q_\\phi(Z|X)} \\left[ \\log P_\\theta(X|Z) \\right] - D_{KL}(Q_\\phi(Z|X) \\| P(Z)) $$ 3.2.4 最终的优化目标 优化参数：\n$\\phi$：编码器参数，控制 $Q(Z|X)$ $\\theta$：解码器参数，控制 $P(X|Z)$ 因此，我们重写上面提到的变分思想，最大化初始下界 $\\mathcal{L}(\\phi, \\theta)$，从而逼近真实对数似然 $\\log P_\\theta(X)$，同时优化编码器参数 $\\phi$ 和解码器参数 $\\theta$，得到优化后的下界，再重复。\n3.2.5 重参数化技巧 我们再次完整的回顾VAE的优化目标\n$$ \\log P_\\theta(X) \\geq \\mathcal{L}(\\phi, \\theta) = \\mathbb{E}_{Q_\\phi(Z|X)} \\left[ \\log P_\\theta(X|Z) \\right] - D_{KL}(Q_\\phi(Z|X) \\| P(Z)) $$ 对 $\\theta$ 的求导是没有问题的，因为 $\\theta$ 只出现在 $P_\\theta(X|Z)$ 中，这个是可以直接计算的。 对 $\\phi$ 的求导是很复杂， $\\phi$ 出现在 $Q_\\phi(Z|X)$ 中， $Z$ 是从分布 $Q_\\phi(Z|X)$ 中随机采样的，即随机性被直接嵌入到了 Z 的采样过程，而这个采样过程又依赖 $\\phi$，因此 $\\phi$ 也被嵌入了 Z 的采样过程。这种情况相当于对随机变量求导。 通俗来讲，为了生成新的样本，我们希望能够从潜在变量空间 z 中采样，并使用解码器生成近似于输入数据 x 的样本。但是随机变量 $Z$ 没有导数，因此用梯度下降法进行优化，梯度下降法都用不了还玩什么神经网络。这就是 重新参数化技巧（reparameterization trick） 的核心所在，它通过辅助变量 $\\epsilon$ 将随机采样与模型参数分离 为了解决对 $\\phi$ 的求导问题，提出了重参数化技巧，即 引入辅助变量 $\\epsilon \\sim \\mathcal{N}(0, I)$ $$ Z = \\mu_\\phi(X) + \\Sigma_\\phi^{1/2}(X) \\cdot \\epsilon $$ 或者简单表示 $$ Z = \\mu_x + \\epsilon \\sigma_x $$ 这样$Z$ 的随机性完全来自 $\\epsilon$，而不是像以前一样由 $Q_\\phi(Z|X)$ 决定，使 $Z$ 关于 $\\phi$ 可导 (均值 $\\mu_\\phi(X)$ 和方差 $\\Sigma_\\phi(X)$ 是 $\\phi$ 的显式函数)。换句话说，可控部分由均值 $\\mu_\\phi(X)$ 和方差 $\\Sigma_\\phi(X)$ 组成，模型的优化过程完全依赖于 $\\mu_\\phi(X)$ 和 $\\Sigma_\\phi(X)$ ，不可控的随机部分由 $\\epsilon$决定，与$\\phi$ 无关，这样可以将随机性隔离在 $\\epsilon$ 中，可以用常规的梯度下降法优化。\n举个例子，一个球可以被推到某个位置。原始情况是球的位置 Z 直接由 $Q_\\phi(Z|X)$ 决定，而这个过程依赖于一个复杂的随机性，导致很难控制。 我们使用重参数化方法后，改用一个简单的标准随机输入 $\\epsilon$（比如“风力”），再通过某种“可控装置”（由 $\\mu_\\phi(X)$ 和 $\\Sigma_\\phi(X)$ 表示）决定最终位置 Z。这样，“装置”（即 $\\phi$）是可控可导的。\n一句话简要概括就是，潜在变量 z 的生成不再是直接从某个分布中随机抽样，而是给他构建一个公式，这个公式中保留了一丝随机性。\n因此根据重参数化，重写目标函数：\n$$ \\mathcal{L}(\\phi, \\theta) = \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0, I)} \\left[ \\log P_\\theta(X | \\mu_\\phi(X) + \\Sigma_\\phi^{1/2}(X) \\cdot \\epsilon) \\right] - D_{KL}(Q_\\phi(Z|X) \\| P(Z)) $$ 然后用蒙特卡洛方法MC通过采样近似复杂积分期望部分，即 $\\mathbb{E} _{\\epsilon \\sim \\mathcal{N}(0, I)} \\left[ \\log P _\\theta(X | Z) \\right]$ 。\n​\t(a) 从标准正态分布 $ \\mathcal{N}(0, I) $ 中采样 $\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_n $\n​\t(b) 对每个采样点 $\\epsilon_i$ ，计算对应的 $Z_i = \\mu_\\phi(X) + \\Sigma_\\phi^{1/2}(X) \\cdot \\epsilon_i$\n​\t(c) 用这些采样点的函数值的均值来近似期望值：\n$$ \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0, I)} \\left[ \\log P_\\theta(X | Z) \\right] \\approx \\frac{1}{n} \\sum_{i=1}^n \\log P_\\theta(X | Z_i) $$ 3.2.6 生成过程流程 编码阶段 ​\t(a) 输入 $X$ 是观测到的样本（如一张图像），接下来我们着手提取潜在变量 Z 的分布。\n​\t(b) 编码器通过神经网络计算潜在变量的均值 $\\mu_\\phi(X)$ 和协方差矩阵 $\\Sigma_\\phi(X)$ 来估计潜在变量分布 $Q_\\phi(Z|X)$\n​\t(c) 有了潜在变量分布 $Q_\\phi(Z|X)$ ，就可以得到潜在变量 Z 的后验分布$Z \\sim Q_\\phi(Z|X) = \\mathcal{N}(Z | \\mu_\\phi(X), \\Sigma_\\phi(X))$\n​\t(d) 使用重参数化从标准正态分布 $\\epsilon \\sim \\mathcal{N}(0, I)$ 中采样辅助变量 $\\epsilon $ ，并根据公式 $Z = \\mu_\\phi(X) + \\Sigma_\\phi^{1/2}(X) \\cdot \\epsilon$ 来得到 （采样）潜在变量 Z\n解码阶段 ​\t(e) 将编码器生成的潜在变量 Z 输入解码器\n​\t(f) 解码器通过神经网络建模条件分布 $P_\\theta(X|Z)$ ，并输出重构数据 $ \\hat{X} $ 的均值 $\\mu_\\theta(Z)$ 和协方差矩阵（或方差）$ \\ \\Sigma_\\theta(Z)$\n​\t(g) 新的观测数据 $X$ 的就可以生成了 $\\hat{X} \\sim \\mathcal{N}(\\mu_\\theta(Z), \\Sigma_\\theta(Z))$\n​\t(h) 其整个过程对应的数学公式为边缘分布 $P(X)$ $$ P(X) = \\int_Z P_\\theta(X, Z) dZ = \\int_ZP(Z) P(X|Z) $$ ​\t它代表了观测到某个数据 X 的概率，可以通过对所有潜在变量 Z 的可能性进行加权求和（积分）得到。\n这种从潜在变量到观测数据的生成过程称为Ancestral Sampling，即先生成 $Z$，再生成 $X$。\n补充内容，还有另一种采样方式，叫做 VDM（扩散模型）采样，也是基于隐变量的生成模型，它通过逐步从噪声 $Z_n$ 还原到最终的图像 $X$。采样过程是逐步递归的，逐层还原潜在表示 $Z_{n-1}, Z_{n-2}, \\dots, Z_1, X$。 具体内容看 diffusion\n3.3 高斯混合模型 (GMM) 3.3.1 定义 看图比较直观\n高斯混合模型也是一个生成模型，多个高斯分布的线性组合成一个混合高斯分布，并以此生成观测数据。模型中每个数据点 $X$ 都可能来自 $K$ 个不同的高斯分布中的某一个，但是每个高斯分布的权重不一样。GMM 的概率密度函数 $P(X)$ 为：\n$$ P(X) = \\sum_{Z}P(Z)P(X|Z) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x|\\mu_k, \\Sigma_k) $$\n其中：\n$\\pi_k$ 是每个高斯成分的混合系数，满足 $\\sum_{k=1}^{K} \\pi_k = 1$。\n$\\mathcal{N}(x|\\mu_k, \\Sigma_k)$ 是第 $k$ 个高斯成分的概率密度函数，$\\mu_k$ 为均值，$\\Sigma_k$ 为协方差矩阵，更具体公式如下：\n$$ \\mathcal{N}(x|\\mu_k, \\Sigma_k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}} \\exp\\left( -\\frac{1}{2}(x-\\mu_k)^\\top \\Sigma_k^{-1} (x-\\mu_k) \\right) $$ 生成过程：\n​\t(a) 选择一个高斯分量：根据先验 $P(Z)$ 决定 $X$ 来自哪个高斯成分 $k$，这个过程根据混合系数 $\\pi_k$ 进行采样。比如说有一个多色球的盒子，每种颜色代表一个高斯分量。每种颜色球的比例就是混合系数 $\\pi_k$ ，因此我们随机抽一个球出来，如果抽到了蓝色球，蓝色对应着第 $k$ 个高斯分量，就意味着选择第 $k$ 个高斯分量来生成数据。\n​\t(b) 从该分量采样：在选择了某个高斯成分 $k$ 之后，接下来基于 $P(X|Z)$ 从该分量的高斯分布 $\\mathcal{N}(x|\\mu_k, \\Sigma_k)$ 中生成 $X$。相当于我们有了一套生成规则，比如蓝色球对应一个“蓝色规则”，也就是正态分布 $\\mathcal{N}(x|\\mu_k, \\Sigma_k)$，我们就按照这个分布来生成随机数据点 $X$\n3.3.2 引入潜在变量 Z来优化 我们之前可见，GMM中直接优化 $P(X)$ 涉及对多个高斯成分的求和，我们想要简化计算。\n$$ P(X,Z) = \\prod_{k=1}^{K} \\pi_k^{Z_k} \\mathcal{N}(X|\\mu_k, \\Sigma_k)^{Z_k} $$\n其中\n$Z = (Z_1, Z_2, \\dots, Z_K)$ 是一个 one-hot 形式的向量(只有一个元素为 1，其他所有元素都为 0) $\\pi_k^{Z_k}$：表示混合系数 $\\pi_k$ ，只有当$Z_k = 1$ (第$k$个高斯分布被选中)时，才会保留对应的混合系数 $\\pi_k$。对于其他没被选中的都等于1，在连乘中相当于被忽略。 $\\mathcal{N}(X|\\mu_k, \\Sigma_k)^{Z_k}$：注意右上角有个次方，只有选中了的高斯分布才有意义，没有选中的统统等于1被忽略。 在这种情况下，联合概率的对数形式为：\n$$ \\log P(X, Z) = \\sum_{k=1}^{K} Z_k \\left[\\log \\pi_k + \\log \\mathcal{N}(X|\\mu_k, \\Sigma_k)\\right] $$\n这个对数似然函数的形式就更容易优化了。\n3.3.3 变分自编码器 (VAE) 与 GMM 的类比 VAE 可以被视为一种特殊形式的高斯混合模型，但有一些关键的不同。\nGMM 是引入隐变量 Z 来表示数据点属于哪个高斯分量，而 VAE 引入隐变量 z 是为了建模潜在空间的。 GMM 先由 Z 确定选择一个高斯分量，然后从该分量的高斯分布中生成观测数据 X 。而 VAE 是先从 Z 采样（通常是标准正态分布），然后通过解码器生成观测数据 X，但是有一点，VAE最后把多个从标准正态分布采样得到的 $P(Z)$做积分时，用的还是GMM的无穷混合思想，换句话说，$P_\\theta(X)$ 可以看作是无穷多个高斯模型的混合。 GMM 通过 EM 算法优化， VAE 通过变分推断优化。 GMM 完全基于概率分布， VAE 编码器和解码器是神经网络来完成的。 3.4 扩展问题 何用一个标准正态分布（高斯分布）的随机变量，生成符合任意分布（比如均匀分布、指数分布等）的随机变量？\n1. 高斯分布转均匀分布\n有一个标准正态分布$N \\sim \\mathcal{N}(0, 1)$，通过正态分布的累积分布函数（$CDF$，记作 $ \\Psi $），可以把它变成一个均匀分布 $Y \\sim \\text{Uniform}(0, 1)$。\n高斯分布的 CDF 是一个函数，输入高斯随机变量的值 $n$，会输出 $n$ 在高斯分布中的累积概率 $y = \\Psi (n)$。这个 $y$ 是一个在 $[0, 1]$ 区间内的均匀分布随机数。 证明\nY 的分布是均匀分布，即 $P(Y \\leq y) = y $ $$ P(Y \\leq y) = P(\\Psi(N) \\leq y) = P(N \\leq \\Psi^{-1}(y)) = \\Psi(\\Psi^{-1}(y)) = y $$\n2. 均匀分布转目标分布\n有了均匀分布的随机变量 $Y \\sim \\text{Uniform}(0, 1)$，接下来用目标分布的逆累积分布函数（Inverse CDF ，记作 $F^{-1}$），把均匀分布的随机变量转换为任意目标分布的随机变量 $X$。 $$ X = F^{-1}(Y) $$ 证明\n我们希望证明 X 的分布函数 $P(X \\leq x)$ 等于目标分布的 CDF，即 $F(x)$ 。 $$ P(X \\leq x) = P(F^{-1}(Y) \\leq x) = P(Y \\leq F(x)) = F(x) $$\n为什么要用高斯分布？\n高斯分布在整个实数域上都有定义，特别是在高维空间中，能够均匀覆盖潜在空间 标准正态分布便于计算 通过GMM我们可以看出，通过加权混合多个高斯分布，可以近似任意复杂的分布。 ","permalink":"http://localhost:1313/zh/posts/machinelearning_cn/vae/","summary":"VAE是一种生成式模型，其核心思想就是概率密度估计问题，属于无监督学习。","title":"变分自编码器VAE"},{"content":"Wiener-Hunt 方法：无监督方面 在上一个实践内容中，我们介绍了去卷积问题的困难，即在应用卷积或者低通滤波器后所导致的观测数据缺失高频相关信息的情况。我们使用了 $Wiener$-$Hunt$ 方法：将量化解的误差的二次项和数据相结合，并在损失函数中使用带有正则化系数的二次惩罚标准以量化解的粗糙度。我们得到了相对来说不错的结果。但是这个方法的缺点是它需要调节一个参数，即正则化参数 $\\mu$。我们最开始由经验选择到观察选择，一直到最后循环找到 $\\mu$ 的最优值，使得去卷积后的图像既不过于不规则也不过于平滑。下面的工作重点在于介绍一种自动调节超参数的方法。\n1. 超参数与全后验分布 这个方法基于 $Wiener$-$Hunt$ 解的贝叶斯解释。该解释本身基于关于误差 $e$ 和关于图像 $x$ 的两个高斯概率模型。\n1.1 误差分布 误差被建模为白色、零均值同质高斯向量。白色指的是像白噪音一样，在其频谱特性中，所有频率分量有相同的功率密度，即信号在不同频率上的能量分布是均匀的，在数学层面，它具有零相关性，即不同时间点的误差值是统计独立的（不相关的）。\n对于高斯分布，选择了一个涉及所谓精度参数 $\\gamma_e$（方差的倒数）的替代参数化。根据这种参数化，其表达式可写为：\n$$f(e \\mid \\gamma_e) = (2\\pi)^{-N/2} \\gamma_e^{N/2} \\exp\\left( -\\frac{\\gamma_e \\|e\\|^2}{2} \\right)$$ 根据 $y = Hx + e$，数据 $y$ 和感兴趣信号 $x$ 的似然函数表达式：\n$$f(y \\mid x, \\gamma_e) = (2\\pi)^{-N/2} \\gamma_e^{N/2} \\exp\\left( -\\frac{\\gamma_e \\|y - Hx\\|^2}{2} \\right)$$ 根据所给的似然表达式 $f(y \\mid x, \\gamma_e)$，量化重建物体 $x$ 相对于观测数据 $y$ 的充分性可以通过协对数（log-likelihood）的形式表示。这种表达经常用于概率模型中，特别是最大似然估计（MLE）或贝叶斯推断中，用于评估模型参数的适配程度。\n再补充一下协对数相关内容，就是似然函数取对数，对于上述似然函数表达式，取其对数后为：\n$$\\log f(y \\mid x, \\gamma_e) = \\log\\left( (2\\pi)^{-N/2} \\gamma_e^{N/2} \\exp\\left( -\\frac{\\gamma_e \\|y - Hx\\|^2}{2} \\right) \\right)$$ 利用对数的性质：$\\ln(a \\cdot b) = \\ln a + \\ln b$，将三部分拆分开：\n$$\\log f(y \\mid x, \\gamma_e) = \\log\\left( (2\\pi)^{-N/2} \\right) + \\log\\left( \\gamma_e^{N/2} \\right) + \\log\\left( \\exp\\left( -\\frac{\\gamma_e \\|y - Hx\\|^2}{2} \\right) \\right)$$ 逐项计算：\n$$\\log\\left( (2\\pi)^{-N/2} \\right) = -\\frac{N}{2} \\log(2\\pi)$$ $$\\log\\left( \\gamma_e^{N/2} \\right) = \\frac{N}{2} \\log(\\gamma_e)$$ $$\\log\\left( \\exp\\left( -\\frac{\\gamma_e \\|y - Hx\\|^2}{2} \\right) \\right) = -\\frac{\\gamma_e}{2} \\|y - Hx\\|^2$$ 合并得到：\n$$\\log f(y \\mid x, \\gamma_e) = -\\frac{N}{2} \\log(2\\pi) + \\frac{N}{2} \\log(\\gamma_e) - \\frac{\\gamma_e}{2} \\|y - Hx\\|^2$$ 回到之前的内容，我们使用了协对数来表达数据能否充分重建原信号，我们给出这种拟合程度的量化指标：\n$$\\mathcal{J}_{LS}(x) = \\|y - Hx\\|^2 = -k_y \\log f(y \\mid x, \\gamma_e) + C_y$$ ​\t•\t$|y - Hx|^2$ 是重建信号（模型参数 $x$）与观测数据 $y$ 的误差平方和，称为残差平方和（Residual Sum of Squares, RSS）。\n​\t•\t$-k_y \\log f(y \\mid x, \\gamma_e)$ 是协对数的负加权形式，其中 $k_y \u0026gt; 0$，是一个常数。\n​\t•\t$C_y$ 也是一个常数。\n我们现在给出两个常数 $k_y$ 和 $C_y$ 的对应表达式：\n前面得到：\n$$\\log f(y \\mid x, \\gamma_e) = -\\frac{N}{2} \\log(2\\pi) + \\frac{N}{2} \\log(\\gamma_e) - \\frac{\\gamma_e \\|y - Hx\\|^2}{2}$$ 将上述结果带入 $\\mathcal{J}_{LS}(x) = -k_y \\log f(y \\mid x, \\gamma_e) + C_y$ 得：\n$$\\mathcal{J}_{LS}(x) = -k_y \\left( -\\frac{N}{2} \\log(2\\pi) + \\frac{N}{2} \\log(\\gamma_e) - \\frac{\\gamma_e \\|y - Hx\\|^2}{2} \\right) + C_y$$ $$\\mathcal{J}_{LS}(x) = k_y \\left( \\frac{N}{2} \\log(2\\pi) - \\frac{N}{2} \\log(\\gamma_e) + \\frac{\\gamma_e \\|y - Hx\\|^2}{2} \\right) + C_y$$ 将上述结果和原公式 $\\mathcal{J}_{LS}(x) = |y - Hx|^2$ 做对比得到结果：\n$$k_y = \\frac{2}{\\gamma_e} \\quad\\quad\\quad C_y = -\\frac{N}{\\gamma_e} \\left( \\log(2\\pi) - \\log(\\gamma_e) \\right)$$ 1.2 感兴趣信号的分布 贝叶斯解释要求为感兴趣的信号 $x$ 提供一个概率分布。其模型也是高斯分布，只是这里它不是白色的，也就是说，它的各组成部分之间存在相关性。在接下来的内容中，相关性实际上是通过协方差矩阵 $R$ 来建模的。\n贝叶斯解释的核心思想是为感兴趣的信号 $x$ 提供一个概率分布，而不是一个确定值。这种概率分布反映了我们对 $x$ 的不确定性以及其任何可能的取值。因此我们假设在模型中，$x$ 服从一个高斯分布。\n但是它是一个非白色的高斯分布，也就是它的各组成部分之间存在相关性，协方差矩阵 $R$ 是一个非对角矩阵，其非零非对角元素表示信号的不同分量之间的相关性。后续我们就使用这个协方差矩阵 $R$ 在贝叶斯框架中建模信号的相关性。\n补充一下关于协方差矩阵的相关内容\n协方差矩阵 $R$ 提供了对信号相关性的精确描述。元素 $R_{ij}$ 表示信号第 $i$ 和第 $j$ 个分量之间的协方差：\n$$R_{ij} = \\mathbb{E}[(x_i - \\mu_i)(x_j - \\mu_j)]$$ 根据相关性，$R$ 可能是一个稀疏矩阵或者满矩阵。\n在贝叶斯建模中：\n​\t1.\t信号 $x$ 的 先验 分布 $p(x)$ 使用协方差矩阵 $R$ 的描述公式为：\n$$p(x) = \\frac{1}{(2\\pi)^{N/2} |R|^{1/2}} \\exp\\left( -\\frac{1}{2} x^T R^{-1} x \\right)$$ 其中，$R^{-1}$ 是协方差矩阵的逆，也称为精度矩阵，定义了 $x$ 的相关性强度。\n​\t2.\t最大后验估计（MAP）：\n利用先验分布 $p(x)$ 和观测数据 $y$ 的似然函数 $p(y \\mid x)$，可以通过贝叶斯法则计算 $x$ 的后验概率分布 $p(x \\mid y)$，并基于该分布选择最优解。\n回到之前，我们通过逆矩阵 $R^{-1} = \\gamma_x \\Pi$ 来表示建模信号的相关性，\n其中：\n​\t•\t精度参数 $\\gamma_x$ 控制相关性的强度\n​\t•\t矩阵 $\\Pi$ 决定了相关性的结构\n我们将 $R^{-1} = \\gamma_x \\Pi$ 带入之前的 $f(x \\mid \\gamma_x)$ 公式可得：\n$$f(x \\mid \\gamma_x) = (2\\pi)^{-N/2} \\det[\\Pi]^{1/2} \\gamma_x^{N/2} \\exp\\left( -\\frac{\\gamma_x}{2} x^T \\Pi x \\right)$$ 也就是说：\n$$f(x \\mid \\gamma_x) \\propto \\exp\\left( -\\frac{\\gamma_x}{2} x^T \\Pi x \\right)$$ 量化物体相对于先验信息充分性的项表现为密度的协对数：\n$$\\mathcal{J}_0(x) = -k_x \\log f(x \\mid \\gamma_x) + C_x = x^T \\Pi x$$ 其中：\n​\t•\t$\\mathcal{J}_0(x)$ 是对信号 $x$ 的量化，用来描述 $x$ 相对于先验信息（即对 $x$ 的已知假设或统计特性）是否充分匹配。其形式以密度的协对数（具体是取对数的负数）表示，结合了贝叶斯模型中的先验分布。\n​\t•\t$f(x \\mid \\gamma_x)$ 是 $x$ 的先验概率密度函数，反映了 $x$ 如何符合假设的先验模型，我们之前在假设 $x$ 服从高斯分布的前提下，得到了其表达式（见上面）。\n​\t•\t正则化项 $x^T \\Pi x$ 描述了信号 $x$ 的“复杂度”或“平滑性”，由 $\\Pi$ 决定其结构，精度参数 $\\gamma_x$ 控制正则化的强度，当 $\\gamma_x$ 较大时，正则化约束更强。\n同样，在上述公式中，我们添加了加法常数 $C_x$ 和乘法常数 $k_x$。为了与之前已经计算过的 Wiener-Hunt 方法联系起来，只需选择 $\\Pi = D^T D$。\n现在我们要给出两个常数的对应表达式。\n已知原公式：\n$$f(x \\mid \\gamma_x) = (2\\pi)^{-N/2} \\det(\\Pi)^{1/2} \\gamma_x^{N/2} \\exp\\left( -\\frac{\\gamma_x x^T \\Pi x}{2} \\right)$$ 两边取对数：\n$$\\log f(x \\mid \\gamma_x) = \\log\\left( (2\\pi)^{-N/2} \\det(\\Pi)^{1/2} \\gamma_x^{N/2} \\exp\\left( -\\frac{\\gamma_x x^T \\Pi x}{2} \\right) \\right)$$ $$\\log f(x \\mid \\gamma_x) = -\\frac{N}{2} \\log(2\\pi) + \\frac{1}{2} \\log\\left( \\det(\\Pi) \\right) + \\frac{N}{2} \\log(\\gamma_x) - \\frac{\\gamma_x x^T \\Pi x}{2}$$ 根据公式：\n$$\\mathcal{J}_0(x) = -k_x \\log f(x \\mid \\gamma_x) + C_x$$ 将上述结果带入其中得到：\n$$\\mathcal{J}_0(x) = -k_x \\left( -\\frac{N}{2} \\log(2\\pi) + \\frac{1}{2} \\log\\left( \\det(\\Pi) \\right) + \\frac{N}{2} \\log(\\gamma_x) - \\frac{\\gamma_x x^T \\Pi x}{2} \\right) + C_x$$ $$\\mathcal{J}_0(x) = k_x \\left( \\frac{N}{2} \\log(2\\pi) - \\frac{1}{2} \\log\\left( \\det(\\Pi) \\right) - \\frac{N}{2} \\log(\\gamma_x) + \\frac{\\gamma_x x^T \\Pi x}{2} \\right) + C_x$$ 对比：\n$$\\mathcal{J}_0(x) = x^T \\Pi x$$ 得到：\n$$k_x = \\frac{2}{\\gamma_x} \\quad\\quad\\quad C_x = \\frac{N}{\\gamma_x} \\log(2\\pi) - \\frac{1}{\\gamma_x} \\log\\left( \\det(\\Pi) \\right) - \\frac{N}{\\gamma_x} \\log(\\gamma_x)$$ 但是严格来说，上述解释并不完全正确，因为 $D^T D$。常量图像只不过是对应于特征值为零的特征向量（这对应于零频率）。严格的发展要求引入一个用于零频率下能量的惩罚项（通过一个可以设定为任意小值的参数）。不懂，后面补充\n上面提到的这个 $f(x \\mid \\gamma_x) = (2\\pi)^{-N/2} \\det(\\Pi)^{1/2} \\gamma_x^{N/2} \\exp\\left( -\\frac{\\gamma_x x^T \\Pi x}{2} \\right)$ 先验分布（a priori），因为它使人们能够处理先验信息，从而更倾向于具有更高规则性的图像。对于给定图像的概率越高，则图像越规则。\n其中的 $\\gamma_x$ 精度参数我们非常关注，因为它控制着图像的平滑度，进而影响着概率分布的整体趋势。\n​\t•\t当 $\\gamma_x$ 较大时，指数项中的 $x^T \\Pi x$ 会被放大。\n​\t•\t当 $\\gamma_x$ 较小时，指数项中的 $x^T \\Pi x$ 对总的概率密度的影响较小。待补充\n1.3 后验分布 借助前面定义的两个成分，并使用概率的乘法规则，现在可以构造重构信号 $x$ 和数据 $y$ 的联合密度：\n$$f(x, y \\mid \\gamma_e, \\gamma_x) = f(y \\mid x, \\gamma_e) f(x \\mid \\gamma_x)$$ 将之前得到的结果代入：\n$$f(x, y \\mid \\gamma_e, \\gamma_x) = (2\\pi)^{-N} \\det[\\Pi]^{1/2} \\gamma_x^{N/2} \\gamma_e^{N/2} \\exp\\left( -\\left[ \\gamma_e \\|y - Hx\\|^2 + \\gamma_x x^T \\Pi x \\right] / 2 \\right)$$ 这个表达式由两个精度参数 $\\gamma_e$ 和 $\\gamma_x$ 参数化。可以注意到在指数项内部，我们得到了加权最小二乘准则的表达式：\n$$\\mathcal{J}_{PLS}(x) = \\mathcal{J}_{LS}(x) + \\mu \\mathcal{J}_0(x)$$ $$\\mathcal{J}_{PLS}(x) = \\|y - Hx\\|^2 + \\mu x^T \\Pi x$$ 其中，正则化参数 $\\mu$ 表示为信噪比的倒数 $\\mu = \\gamma_x / \\gamma_e$。正则化参数 $\\mu$ 在 $\\gamma_e$ 和 $\\gamma_x$ 中的作用是？\n待补充\n通过贝叶斯定理可以确定感兴趣信号的后验分布（后验概率分布）：\n$$f(x \\mid y, \\gamma_e, \\gamma_x) = \\frac{f(x, y \\mid \\gamma_e, \\gamma_x)}{f(y \\mid \\gamma_e, \\gamma_x)} \\propto \\exp\\left( -\\gamma_e \\mathcal{J}_{PLS}(x) / 2 \\right)$$ 这就是给定数据（已观测到的）和参数下的感兴趣信号的分布。\n我们希望为感兴趣信号构造的任何估计器都基于上述分布。最常见的估计器是后验分布的均值、中位数或众数（即后验的最大化者）。在当前情况下，当后验分布是高斯分布时，这三者是相等的。众数或后验最大化者（MAP），记为 $\\hat{x} _{MAP}$ 。也就是最小化准则 $\\mathcal{J} _{PLS}(x)$ 的解\n$$\\hat{x}_{MAP} = \\arg \\max_{x} f(x \\mid y, \\gamma_e, \\gamma_x) = \\arg \\min_{x} \\mathcal{J}_{PLS}(x) = \\hat{x}_{PLS}$$ 结论是，最小二乘准则的解 $\\hat{x} _{MAP}$ 就是之前的工作中推导出来的后验分布的众数 $ \\hat{x} _{MAP}$。\n1.4 扩展的后验分布 到目前为止，贝叶斯方法只允许我们对已经存在的超参数值的估计给出概率解释。将之前的框架扩展到包含超参数的估计，需要为两个精度参数 $\\gamma_e$ 和 $\\gamma_x$ 引入一个先验分布。在多种可选方案中，接下来我们将重点关注伽马分布：\n$$f(\\gamma) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\gamma^{\\alpha - 1} \\exp[-\\beta \\gamma] \\cdot 1_{\\mathbb{R}^+}(\\gamma)$$ 它由两个正实数参数 $(\\alpha, \\beta)$ 驱动，具有均值 $\\alpha / \\beta$ 和方差 $\\alpha / \\beta^2$。这种选择的理由如下：\n​\t•\t选择伽马分布作为先验分布确保了条件后验分布也是伽马分布（我们正在讨论共轭先验）。在算法上，这意味着只需要更新分布参数的值（具体见下面）。\n​\t•\t这种选择允许在参数值的信息较少（也称为“非信息先验”）或精确（如名义值或某种不确定性）的情况下进行处理。该工作中特别感兴趣的是“非信息先验”的极限情况，即 $(\\alpha, \\beta) = (0, 0)$。\n此外，关于变量 $\\gamma_e$ 和 $\\gamma_x$ 的组合，它们被建模为独立的。\n从伽马分布：\n$$f(\\gamma) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\gamma^{\\alpha - 1} \\exp[-\\beta \\gamma] \\cdot 1_{\\mathbb{R}^+}(\\gamma)$$ 和部分联合分布：\n$$f(x, y \\mid \\gamma_e, \\gamma_x) = f(y \\mid x, \\gamma_e) f(x \\mid \\gamma_x) = (2\\pi)^{-N} \\det[\\Pi]^{1/2} \\gamma_x^{N/2} \\gamma_e^{N/2} \\exp\\left( -\\left[ \\gamma_e \\|y - Hx\\|^2 + \\gamma_x x^T \\Pi x \\right] / 2 \\right)$$ 的表达式出发，我们推导出对于 $y, x, \\gamma_e$ 和 $\\gamma_x$ 的完整联合分布的表达式为：\n$$f(y, x, \\gamma_e, \\gamma_x) = f(y, x \\mid \\gamma_e, \\gamma_x) f(\\gamma_e) f(\\gamma_x)$$ 其明确表达为：\n$$f(x, y, \\gamma_e, \\gamma_x) = (2\\pi)^{-N} \\det[\\Pi]^{1/2} \\frac{\\beta_e^{\\alpha_e} \\beta_x^{\\alpha_x}}{\\Gamma(\\alpha_e) \\Gamma(\\alpha_x)} \\gamma_e^{\\alpha_e + N/2 - 1} \\gamma_x^{\\alpha_x + N/2 - 1} \\exp\\left( -\\left[ \\gamma_e \\left( \\beta_e + \\|y - Hx\\|^2 / 2 \\right) + \\gamma_x \\left( \\beta_x + x^T \\Pi x / 2 \\right) \\right] \\right)$$ 注意: 这个密度非常重要，因为它允许推导出所有相关的联合、边缘和条件密度。\n现在我们可以通过贝叶斯规则推导出完整的后验分布，即给定观测数据$y$时，感兴趣信号$x$和超参数$\\gamma_e$, $\\gamma_x$的分布：\n$$f(x, \\gamma_e, \\gamma_x | y) = \\frac{f(x, y, \\gamma_e, \\gamma_x)}{f(y)}$$ $$f(x, \\gamma_e, \\gamma_x | y) \\quad \\propto \\quad \\gamma_e^{\\alpha_e + N/2 - 1} \\gamma_x^{\\alpha_x + N/2 - 1} \\exp \\left( - \\left[ \\gamma_e \\left( \\beta_e + \\|y - Hx\\|^2 / 2 \\right) + \\gamma_x \\left( \\beta_x + \\|x\\|_\\Pi^2 / 2 \\right) \\right] \\right)$$ 这汇总了所有关于感兴趣信号和超参数在数据视角下的可用信息：对于三重项 $x$, $\\gamma_e$, $\\gamma_x$，它量化了后验密度，即在给定观测数据下三重项的概率。感兴趣信号和超参数的估计器是从这个分布中构造出来的。我们可以查看后验分布的均值、中位数或众数。每种方法都有其优缺点。在接下来的内容中，我们将重点讨论均值。\n1.5 后验均值 考虑到后验分布（上面这个）的复杂性，获得均值的解析公式是不可行的。为了计算后验均值，有几种方法可用，在这里我们将重点关注随机采样技术。最终，它归结为对后验分布进行随机采样，然后取样本的经验均值，从而近似后验均值。\n后验分布的采样可以通过马尔可夫链蒙特卡洛（MCMC）方法来实现。它要求构建一个迭代过程，以生成随机样本，经过一定的时间（称为 burn-in），这些样本将根据目标分布进行分布。构建这样一个过程并不容易，但在当前情况下，存在一个标准算法可以轻松使用：Gibbs 采样算法。它将对三重项 $x$, $\\gamma_e$, $\\gamma_x$ 的后验分布进行采样的问题，转换为它们三个各自的更简单分布的采样问题。每个分布实际上是条件分布，给定其余参数的条件下对其中一个参数进行采样。该算法的工作原理在下表中给出，接下来的部分我们将详细说明这些步骤。\n$$\\begin{aligned} \u0026\\bullet \\, \\text{Initialize } x^{[0]} = y \\\\ \u0026\\bullet \\, \\text{For } k = 1, 2, \\dots \\, \\text{repeat} \\\\ \u0026\\quad \\text{(a) \\ sample } \\gamma_e^{[k]} \\text{ under } f(\\gamma_e \\mid \\gamma_x^{[k-1]}, x^{[k-1]}, y) \\\\ \u0026\\quad \\text{(b) \\ sample } \\gamma_x^{[k]} \\text{ under } f(\\gamma_x \\mid \\gamma_e^{[k]}, x^{[k-1]}, y) \\\\ \u0026\\quad \\text{(c) \\ sample } x^{[k]} \\text{ under } f(x \\mid \\gamma_e^{[k]}, \\gamma_x^{[k]}, y) \\end{aligned}$$ 1.5.1 采样逆误差功率 采样对应于步骤 (a) 的超参数 $\\gamma_e$ 需要从条件后验分布 $f(\\gamma_e | x, \\gamma_x, y)$ 中采样。该分布由完整的联合分布 $f(x, y, \\gamma_e, \\gamma_x)$ 获得，如下所示：\n$$\\text{posterior distribution }: \\quad f(\\gamma_e | x, \\gamma_x, y) = \\frac{f(x, y, \\gamma_e, \\gamma_x)}{f(x, \\gamma_x, y)}$$ 仅保留包含 $\\gamma_e$ 的项（与 $\\gamma_e$ 相关的部分），并且由于分母不依赖于 $\\gamma_e$，我们得到\n$$f(\\gamma_e | x, \\gamma_x, y) \\propto f(x, y, \\gamma_e, \\gamma_x)$$ $$f(\\gamma_e | x, \\gamma_x, y) \\quad \\propto \\quad \\gamma_e^{\\alpha_e + N/2 - 1} \\exp \\left( - \\gamma_e \\left( \\beta_e + \\| y - Hx \\|^2 / 2 \\right) \\right)$$ 由此获得的条件分布实际上是具有参数 $(\\alpha, \\beta)$ 的伽马分布：\n$$\\alpha = \\alpha_e + N/2 \\quad \\text{and} \\quad \\beta = \\beta_e + \\| y - Hx \\|^2 / 2$$ 在先验参数 $(\\alpha_e, \\beta_e)$ 等于 $(0, 0)$ 的极限情况下，后验的参数为：\n$$\\alpha = N/2 \\quad \\text{and} \\quad \\beta = \\| y - Hx \\|^2 / 2$$ 因此我们关注于这个 $f(\\gamma_e | x, \\gamma_x, y)$ 分布的均值和方差表达式，并将它与输出误差 $y - Hx$ 的功率相关联。\n已知伽马分布的概率密度函数：\n$$f(\\gamma) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\gamma^{\\alpha - 1} \\exp[-\\beta \\gamma] \\cdot 1_{\\mathbb{R}+}(\\gamma)$$ 其中：$\\alpha$ 是形状参数，$\\beta$ 是尺度参数。\n对于伽马分布 $\\text{Gamma}(\\alpha, \\beta)$，其均值和方差的标准表达式分别为：\n均值：\n$$\\mathbb{E}[\\gamma_e] = \\frac{\\alpha}{\\beta}$$ 方差：\n$$\\text{Var}[\\gamma_e] = \\frac{\\alpha}{\\beta^2}$$ 已知：\n$$\\alpha = N/2 \\quad \\text{and} \\quad \\beta = \\| y - Hx \\|^2 / 2$$ 误差精度参数 $\\gamma_e$ 的均值：\n$$\\mathbb{E}[\\gamma_e] = \\frac{\\alpha_e + N/2}{\\beta_e + \\frac{\\| y - Hx \\|^2}{2}}$$ 误差精度参数 $\\gamma_e$ 的方差：\n$$\\text{Var}[\\gamma_e] = \\frac{\\alpha_e + N/2}{\\left( \\beta_e + \\frac{\\| y - Hx \\|^2}{2} \\right)^2}$$ 伽马分布中的 $\\beta$ 参数直接依赖于 $| y - Hx |^2$。\n当误差 $| y - Hx |^2$ 较大时，$\\beta$ 参数也会增大，表示模型拟合较差，这意味着伽马分布的均值和方差会减小，反映了误差增加时对数据的信任度降低。 反之，误差 $| y - Hx |^2$ 较小时，精度参数 $\\gamma_e$ 的均值增大，表示对数据的信任度较高。 因此为了实现步骤 (a)：$\\gamma_e^{[k]}$ 的样本从具有上述两个参数 $\\alpha$ 和 $\\beta$ 的伽马分布中抽取，我们可以使用 Matlab 中的 RNDGamma(Alpha, Beta); 函数，具体代码为：\n1 2 3 4 function SamplePrecision = RNDGamma(Alpha,Beta)\t% The Precision variable is a sample of the gamma distribution with parameters Alpha and Beta % Tirage d\u0026#39;un échantillon Gamma approché par du Gauss (JFG+TBC) SamplePrecision = Alpha/Beta + sqrt( Alpha/(Beta*Beta) ) * randn; 注意：计算参数 $\\beta$ 涉及计算建模误差 $| y - Hx |^2$ 的范数。计算空间域中的范数通常成本较高，但是可以在傅里叶域中进行计算以降低成本。\n1.5.2 采样感兴趣信号的逆功率 现在我们将重点放在采样超参数 $\\gamma_x$ 上，对应于表 1 中算法的步骤 (b)。\n这需要采样条件后验分布 $f(\\gamma_x | x, \\gamma_e, y)$。使用与上一节类似的方法，我们得到：\n$$f(\\gamma_x | x, \\gamma_e, y) \\propto \\gamma_x^{\\alpha_x + N/2 - 1} \\exp \\left( - \\gamma_x \\left( \\beta_x + \\| x \\|_\\Pi^2 / 2 \\right) \\right)$$ 可以看出，这个条件后验分布也是伽马分布。在先验参数 $(\\alpha_x, \\beta_x)$ 等于 $(0, 0)$ 的极限情况下，后验参数为：\n$$\\alpha = N/2 \\quad \\text{and} \\quad \\beta = \\| x \\|_\\Pi^2 / 2$$ 因此我们关注于 $f(\\gamma_x | x, \\gamma_e, y)$ 这个分布的均值和方差表达式，并将它与输出误差 $y - Hx$ 的功率相关联。\n均值：\n$$\\mathbb{E}[\\gamma_x] = \\frac{\\alpha_x}{\\beta_x}$$ 方差：\n$$\\text{Var}[\\gamma_x] = \\frac{\\alpha_x}{\\beta_x^2}$$ $$\\mathbb{E}[\\gamma_x] = \\frac{\\alpha_x + N/2}{\\beta_x + \\frac{\\| x \\|_{\\Pi}^2}{2}}$$ $$\\text{Var}[\\gamma_x] = \\frac{\\alpha_x + N/2}{\\left( \\beta_x + \\frac{\\| x \\|_{\\Pi}^2}{2} \\right)^2}$$ 当图像 $x$ 的二次形式较小时（即图像较为平滑），均值会相应增大。这表明我们更信任较为平滑的图像。\n因此为了实现步骤 (b) $\\gamma_x^{[k]}$ 的样本从具有上述两个参数的伽马分布中抽取，同样使用 RNDGamma 函数。\n注意：计算参数 $\\beta$ 涉及计算建模误差 $| y - Hx |^2$ 的范数。计算空间域中的范数通常成本较高，但是可以在傅里叶域中进行计算以降低成本。\n1.6 采样感兴趣的物体 最后但同样重要的是，我们将处理对应于表 1 中算法步骤 (c) 的感兴趣物体 $x$ 的采样。这意味着采样条件后验分布 $f(x | \\gamma_x, \\gamma_e, y)$，其表达式已经推导出\n$$f(x | \\gamma_e, \\gamma_x, y) = \\frac{f(x, y | \\gamma_e, \\gamma_x)}{f(y | \\gamma_e, \\gamma_x)} \\propto \\exp \\left( - \\gamma_e \\mathcal{J}_{PLS}(x) / 2 \\right)$$ 并且它便捷地重新写为：\n$$f(x | \\gamma_e, \\gamma_x, y) \\propto \\exp \\left( - \\left[ \\gamma_e \\| y - Hx \\|^2 + \\gamma_x \\| x \\|_\\Pi^2 \\right] / 2 \\right) = \\exp \\left( - \\gamma_e \\mathcal{J}_{PLS}(x) / 2 \\right)$$ 该密度本身是高斯分布，因为指数项内的变量 $x$ 是正定的二次项。它由均值和协方差矩阵完全表征。在这种情况下：\n均值 $\\mu_{x|*}$ 也是众数，作为最小化 $\\mathcal{J}_{PLS}(x)$ 的解，即之前实践工作中讨论的 Wiener-Hunt 解。 协方差矩阵 $\\Sigma_{x|*}$ 通过计算 $\\mathcal{J}_{PLS}(x)$ 的 Hessian（即二阶导数矩阵）获得。 我们得到以下表达式：\n$$\\mu_{x|*} = \\gamma_e \\Sigma_{x|*} H^T y$$ $$\\Sigma_{x|*} = \\left( \\gamma_e H^T H + \\gamma_x \\Pi \\right)^{-1}$$ 当前面临的数值问题是对可能具有高维的高斯分布进行采样。这个高维度性阻止了协方差矩阵 $\\Sigma_{x|*}$ 的求逆或分解，这意味着没有简单的采样方案可用。为了解决这个问题，我们采用循环矩阵的近似方法，从而能够在傅里叶域中进行快速的矩阵运算。给出了以下表达式：\n$$\\overset{\\circ}{\\mu}_{x|*} = \\gamma_e \\Lambda_{x|*} \\Lambda_H^{\\dagger} \\overset{\\circ}{y}$$ $$\\Lambda_{x|*} = \\left( \\gamma_e \\Lambda_H^\\dagger \\Lambda_H + \\gamma_x \\Lambda_D^\\dagger \\Lambda_D \\right)^{-1}$$ 在傅里叶域中，协方差矩阵是对角线形式的，这意味着其各个分量是解相关的。因此，每个分量是独立的，这使得可以并行采样。\n接下来我们给出上面两个表达式的推导过程，基于循环矩阵对角化的思想。\n由于矩阵 $H$ 是实数矩阵，因此它也是它的复共轭矩阵，且 $H^t = H^\\dagger$。$D$ 同理。\n因此我们将利用循环矩阵的对角化性质，由于 $H$ 和 $D$ 是循环矩阵，且循环矩阵是具有特殊结构的方阵，矩阵的每一行元素是前一行元素循环右移一位。基于这一性质，循环矩阵的一个重要性质是，它可以通过傅里叶变换对角化。具体来说，任何 $N \\times N$ 的循环矩阵 $C$ 都可以写成：\n$$C = F \\Lambda F^\\dagger$$ $F$ 是离散傅里叶变换矩阵，$F^\\dagger$ 是其共轭转置（逆傅里叶变换）。 $\\Lambda$ 是一个对角矩阵，其元素为矩阵 $C$ 的特征值（傅里叶系数）。 之前我们得到的方程为：\n$$\\mu_{x|*} = \\gamma_e \\Sigma_{x|*} H^T y$$ $$\\Sigma_{x|*} = \\left( \\gamma_e H^T H + \\gamma_x \\Pi \\right)^{-1}$$ 我们的目标是利用循环矩阵的性质将其转换到频域，从而得到新的表达式。先将 $H$ 和 $D$ 对角化：\n$$H = F \\Lambda_H F^\\dagger$$ $$\\Pi = F \\Lambda_\\Pi F^\\dagger$$ 带入原方程：\n$$\\Sigma_{x|*} = \\left( \\gamma_e H^T H + \\gamma_x \\Pi \\right)^{-1}$$ 将 $F$ 和 $F^\\dagger$ 提到外面：\n$$\\Sigma_{x|*} = F \\left( \\gamma_e \\Lambda_H^\\dagger \\Lambda_H + \\gamma_x \\Lambda_\\Pi \\right)^{-1} F^\\dagger$$ 第一个公式证明完毕：\n$$\\Lambda_{x|*} = \\left( \\gamma_e \\Lambda_H^\\dagger \\Lambda_H + \\gamma_x \\Lambda_D^\\dagger \\Lambda_D \\right)^{-1}$$ 回到开始，我们有：\n$$H = F \\Lambda_H F^\\dagger$$ $$\\Pi = F \\Lambda_\\Pi F^\\dagger$$ 把上述公式带入原方程：\n$$\\mu_{x|*} = \\gamma_e \\Sigma_{x|*} H^T y$$ 计算得：\n$$\\mu_{x|*} = \\gamma_e F \\left( \\gamma_e \\Lambda_H^\\dagger \\Lambda_H + \\gamma_x \\Lambda_D^\\dagger \\Lambda_D \\right)^{-1} F^\\dagger F \\Lambda_H^\\dagger \\hat{y}$$ 由于 $F^\\dagger F = I$，可以简化为：\n$$\\mu_{x|*} = \\gamma_e \\Lambda_{x|*} \\Lambda_H^\\dagger \\hat{y}$$ 第二个公式证毕。\n在进行步骤 (c) 时，对图像 $x^{[k]}$ 的样本应从具有在傅里叶域中给定的第一和第二矩中的高斯分布中抽取。可以使用自定义的 Matlab 函数 RNDGauss(Moy, Cov)，Moy 和 Cov 必须在傅里叶域中给出，函数具体内容为：\n1 2 3 4 5 6 7 8 9 10 11 12 13 function SampleImage = RNDGauss(MoyGauss,Covariance)\t% Generate an image sample under a Gaussian distribution, with the mean given by Moy and the covariance given by Cov. % The parameters Moy and Cov, and Image, are all in the Fourier domain, not in the spatial domain. % Paramètre de Taille Taille = length(MoyGauss); % Tirage d\u0026#39;un bruit blanc avec la bonne symétrie BoutGauss = randn(Taille,Taille) + sqrt(-1) * randn(Taille,Taille); BoutGauss = MyFFT2( real( MyIFFT2(BoutGauss) ) ); % Filtrage du bruit blanc SampleImage = MoyGauss + BoutGauss .* sqrt(Covariance); 2 实现 在 Matlab 实践中，我们使用和上次内容相同的数据集，最后结果理论上应该相似。因为我们做出的改进只是自动调整正则化参数。我们之前介绍了算法步骤，并且详细解释了涉及两个超参数 $\\gamma_e$ 和 $\\gamma_x$ 的条件分布采样，以及图像 $x$ 的条件分布采样，所以这里直接给代码。\n","permalink":"http://localhost:1313/zh/posts/signal_cn/%E5%8F%8D%E9%97%AE%E9%A2%98-tp2/","summary":"继续扩展反问题中的Wiener-Hunt方法，主要关注于超参数的自动调节。通过贝叶斯解释，介绍了误差分布与信号分布的建模，详细阐述了马尔可夫链蒙特卡洛（MCMC）方法中的Gibbs采样算法","title":"反问题 TP2"},{"content":"1. 图像投影模型与3D重建理论 1.1. SLAM的概念 SLAM（Simultaneous Localization and Mapping，即同步定位与地图构建）。 通过估计每个相机的位置和场景的三维点，实现对场景的重建。 1.2. 逆向二维图像 从二维图像中提取三维场景的信息，即进行$3D$重建\n逆向投影\n图像是三维场景经过投影后的二维表示，要恢复三维信息，需要逆转这个投影过程。 建立一个数学模型，描述三维场景如何投影到二维图像中，然后尝试逆向求解。 1.3 针孔相机模型 1.3.1 模型概述 定义：针孔相机模型假设所有的光线都通过一个公共点，即光心（光学中心）。 优点：模型简单，易于逆向计算，在三维重建中广泛使用。 1.3.2 坐标系和符号约定 摄像机坐标系： 原点 $O _C$：光学中心，坐标为 $(0, 0, 0)$。 轴方向：建立右手坐标系，$X _C$ 向右，$Y _C$ 向下，$Z _C$ 指向后方（场景深度方向）。 优势：$Z$ 轴指向后方，物体深度为正，符合直觉。 1.3.3 三维点的投影到归一化焦平面 三维点表示： 点 $U$：坐标为 $(U _X, U _Y, U _Z)$，表示空间中的一个三维点。 归一化焦平面： 一个与光心 $O _C$ 距离为 $1$ 的平面（$Z _C = 1$），称为归一化聚焦平面。 将远处的三维点投影到此平面上。 1.3.4 齐次坐标与非齐次坐标 齐次坐标（Homogeneous Coordinates）：\n定义：在原有坐标后增加一个维度（通常为 $1$），方便表示投影和变换。 表示：对于二维点 $m = (m _X, m _Y)^\\top$，其齐次坐标为 $\\bar{m} = (m _X, m _Y, 1)^\\top$ 作用：齐次坐标方便矩阵运算，尤其是在投影和变换过程中。\n非齐次坐标（Inhomogeneous Coordinates）： 标准的笛卡尔坐标表示法，不包含额外的维度。 1.4 摄像机的线性校准 1.4.1 从归一化焦平面到图像平面 图像平面：\n坐标系：像素坐标系，通常以图像左上角为原点，向右为 $X$ 轴（列索引），向下为 $Y$ 轴。 目的：将归一化焦平面上的点映射到实际的图像像素坐标上。 线性变换：\n变换公式：\n$$\\begin{cases} P _U = f \\cdot m _X + U _0 \\\\ P _V = f \\cdot m _Y + V _0 \\end{cases}$$ 焦距 $f$ $m _X$, $m _Y$ 是归一化焦平面上的点 光学中心在图像平面中的坐标 $(U _0, V _0)$\n1.4.2 摄像机内参矩阵 将上述线性变换表示为矩阵形式 $$K = \\begin{pmatrix} f \u0026 0 \u0026 U _0 \\\\ 0 \u0026 f \u0026 V _0 \\\\ 0 \u0026 0 \u0026 1 \\end{pmatrix}$$ 矩阵映射关系： $$\\underline{P} = K \\cdot \\underline{m}$$ 其中，$\\underline{P}$ 是图像平面中的点的齐次坐标\n1.4.3 逆向过程 从图像平面到归一化焦平面： $$\\underline{M} = K^{-1} \\cdot \\underline{P}$$ 1.4.4 可视锥 表示相机能够看到的空间范围。通过将图像的四个角点转换到归一化焦平面，然后连接光学中心，形成视锥。 1.5 畸变建模与校正 1.5.1 相机畸变的来源 实际相机镜头的光学缺陷，尤其在广角镜头中，导致图像出现畸变，直线变曲，图像边缘出现拉伸或压缩。 1.5.2 畸变模型 畸变函数：\n将归一化焦平面上的理想点经过畸变函数（从理想图像到畸变图像），得到畸变后的点，此点为 $2D$ 实际畸变聚焦平面\n$$\\underline{m} _d = d(\\underline{m}, k)$$ 其中，$k$ 是畸变参数\n举个例子: 多项式径向畸变模型\n$$M _d = \\left(1 + k _1 \\|m\\| _2^2 + k _2 \\|m\\| _2^4 + \\dots \\right) m$$ 其中: $|m| _2^2 = m _x^2 + m _y^2$\n1.6 畸变校正的实现 1.6.1 任务描述 目标：将畸变的实际图像校正为理想的无畸变图像 1.6.2 实现步骤 定义参数: 理想的摄像机内参矩阵 $K _{\\text{ideal}}$ ; 畸变的摄像机内参矩阵 $K _{\\text{real}}$; 失真参数 $k$ 。\n对于每个理想图像的像素坐标，执行以下步骤\n将像素坐标转换到归一化焦平面：\n$$\\underline{m} _{\\text{ideal}} = K _{\\text{ideal}}^{-1} \\cdot \\underline{P} _{\\text{ideal}}$$ 应用畸变函数：\n$$\\underline{m} _d = d(\\underline{m} _{\\text{ideal}}, k)$$ 映射回实际图像坐标系：\n$$\\underline{P} _{\\text{real}} = K _{\\text{real}} \\cdot \\underline{m} _d$$ 插值：\n对 $\\underline{P} _{\\text{real}}$ 进行插值（由于坐标可能为非整数，可能要用双线性插值）\n生成校正后的图像\n2. 二维刚性变换和单应性 2.1 二维刚性变换 二维刚性变换包括平移和旋转\n2.1.1 旋转 $$\\mathbf{U}^c = \\overrightarrow{O _c U}^c \\quad \\mathbf{U}^w = \\overrightarrow{O _w U}^w$$ $$\\mathbf{R} _{wc} \\underline{\\mathbf{U}}^c = \\mathbf{R} _{wc} \\cdot \\overrightarrow{O _c U}^c = \\overrightarrow{O _w U}^w$$ 从一个参考系中选取一个向量然后转换到另一个坐标系中\n$\\mathbf{R} _{wc}$ 是一个正交矩阵\n2.1.2 平移 $$\\mathbf{T} _{wc} = \\overrightarrow{O _w O _c}^{w}$$ 2.1.3 刚性变换公式 $$\\mathbf{U}^w = \\mathbf{R} _{wc} \\cdot \\mathbf{U}^c + \\mathbf{T} _{wc}$$ 证明：\n$$\\mathbf{R} _{wc} \\cdot \\mathbf{U}^c + \\mathbf{T} _{wc} = \\mathbf{R} _{wc} \\cdot \\overrightarrow{O _c U}^c + \\overrightarrow{O _w O _c}^w = \\overrightarrow{O _c U}^w + \\overrightarrow{O _w O _c}^w = \\overrightarrow{O _w U}^w = \\mathbf{U}^w$$ 2.1.4 齐次坐标： $$\\underline{\\mathbf{U}}^w = \\begin{bmatrix} \\mathbf{U}^w \\\\ 1 \\end{bmatrix}$$ $$\\mathbf{M} _{wc} = \\begin{bmatrix} \\mathbf{R} _{wc} \u0026 \\mathbf{T} _{wc} \\\\ 0 \u0026 1 \\end{bmatrix} = \\begin{bmatrix} r _{11} \u0026 r _{12} \u0026 r _{13} \u0026 t _{x} \\\\ r _{21} \u0026 r _{22} \u0026 r _{23} \u0026 t _{y} \\\\ r _{31} \u0026 r _{32} \u0026 r _{33} \u0026 t _{z} \\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\end{bmatrix}$$ 2.1.5 反变换： $$\\mathbf{M} _{cw} = \\mathbf{M} _{wc}^{-1}$$ 2.1.6 变换的组合性： $$\\mathbf{M} _{ab} \\cdot \\mathbf{M} _{bc} = \\mathbf{M} _{ac}$$ 2.2 单应性 2.2.1 平面场景假设 $$\\mathbf{U} _i^A = z _i^A \\cdot \\underline{\\mathbf{m}} _{Ai}$$ 这个方程的意思就是，$ \\mathbf{U} _i ^ A $ 这个点可以由 $ \\underline{\\mathbf{m}} _{Ai}$ 来表示，怎么表示呢？$\\Rightarrow$ 乘它的深度即可（因为 $\\underline{\\mathbf{m}} _{Ai}$ 是单位深度）。\n2.2.2 寻找 $\\underline{\\mathbf{m}} _{Ai}$ 和 $\\underline{\\mathbf{m}} _{Bi}$ 之间的对应关系 光有这个方程，我们怎么找到 $\\underline{\\mathbf{m}} _{Ai}$ 和 $\\underline{\\mathbf{m}} _{Bi}$ 之间的对应关系呢，通俗来讲，怎么进行坐标对应变换呢？\n法线关键公式\n我们需要先回顾一个性质，来得到一个法线和平面间的关键公式\n在参考系 $A$ 中，平面 $P$ 的方程为：$ax + by + cz + d = 0$ 其中 $a, b, c$ 是平面法向量分量，$d$ 是常数项，代表平面 $P$ 和原点 $O _A$ 相对距离\n在向量形式中，平面方程可以化简为：\n$$\\mathbf{n} _A^\\top \\mathbf{U} _i^A + d = 0$$ $\\mathbf{n} _A^\\top$ 代表：向量 $P$ 在参考系 $A$ 中的法向量\n通过这个平面方程的向量形式，我们得到了一个带有法向量的一个很重要的公式。\n利用变量代换得到深度表达式\n将 $\\mathbf{U} _i^A = z _i^A \\cdot \\underline{\\mathbf{m}} _{A,i}$ 带入上式中\n$$\\mathbf{n} _A^\\top \\cdot z _i^A \\cdot \\underline{\\mathbf{m}} _{A,i} + d = 0 \\quad \\Rightarrow \\quad z _i^A = -\\dfrac{d}{\\mathbf{n} _A^\\top \\cdot \\underline{\\mathbf{m}} _{A,i}}$$ 这样，我们就把 $\\underline{\\mathbf{m}} _{Ai}$ 给引进来了，其中 $z _i^A = -\\dfrac{d}{\\mathbf{n} _A^\\top \\cdot \\underline{\\mathbf{m}} _{A,i}}$ 代表了深度。换句话说，我们利用 $\\mathbf{U} _i^A$ 的两个方程，将 $\\mathbf{U} _i^A$ 替换掉了，这样就得到 $z _i^A$ 深度，可是仍然解决不了问题 $\\Rightarrow$ 也就是说光有关于 $\\underline{\\mathbf{m}} _{Ai}$ 的方程是不够的，还需要从 $\\underline{\\mathbf{m}} _{Bi}$ 入手\n接下来我们找 $B$ 坐标系下的点 $\\underline{\\mathbf{m}} _{Bi}$\n我们从刚性变换公式入手 $\\mathbf{U}^w = \\mathbf{R} _{wc} \\cdot \\mathbf{U}^c + \\mathbf{T} _{wc}$ 可见从 $c$ 投影到 $w$ 只需要对 $\\mathbf{U}^c$ 进行变换即可，也就是说，为了得到 $\\underline{\\mathbf{m}} _{Bi}$ 只需要对 $\\underline{\\mathbf{m}} _{Ai}$ 进行刚性变换即可\n$$\\underline{\\mathbf{m}} _{B,i} = \\Pi \\left( \\mathbf{R} _{BA} \\mathbf{U} _i^A + \\mathbf{t} _{BA} \\right)$$ 其中 $\\Pi(\\cdot)$ 是投影函数\n$$\\underline{\\mathbf{m}} _{B,i}= \\Pi \\left( \\mathbf{R} _{BA} \\left( -\\dfrac{d}{\\mathbf{n} _A^\\top \\cdot \\underline{\\mathbf{m}} _{A,i}} \\right) \\cdot \\underline{\\mathbf{m}} _{A,i} + \\mathbf{t} _{BA} \\right)$$ 将上公式左右两边都乘 $-\\dfrac{\\mathbf{n} _A^\\top \\cdot \\underline{\\mathbf{m}} _{A,i}}{d}$\n$$\\underline{\\mathbf{m}} _{B,i}= \\Pi \\left( \\mathbf{R} _{BA} \\cdot \\underline{\\mathbf{m}} _{A,i} - \\dfrac{\\mathbf{n} _A^\\top \\cdot \\underline{\\mathbf{m}} _{A,i}}{d} \\cdot \\mathbf{t} _{BA} \\right)$$ $$\\underline{\\mathbf{m}} _{B,i} = \\Pi \\left( \\left( \\mathbf{R} _{BA} - \\dfrac{\\mathbf{t} _{BA} \\cdot \\mathbf{n} _A^\\top}{d} \\right) \\cdot \\underline{\\mathbf{m}} _{A,i} \\right)$$ 也就得到了各自归一化平面上 $A$ 点到 $B$ 点的对应关系\n问题：上述公式中左右两边都乘了 $-\\dfrac{\\mathbf{n} _A^\\top \\cdot \\underline{\\mathbf{m}} _{A,i}}{d}$，为什么保持不变？\n投影函数 $\\Pi(\\cdot)$ 的特点是它是一个比例不变的操作（即只看方向和相对位置，不看绝对尺度）。因此，即使我们在右边乘上 $-\\dfrac{\\mathbf{n} _A^\\top \\cdot \\underline{\\mathbf{m}} _{A,i}}{d}$，也不会影响等式成立的条件，因为投影结果相同\n2.2.3 寻找 $\\underline{\\mathbf{P}} _{A,i}$ 和 $\\underline{\\mathbf{P}} _{B,i}$ 之间的对应关系 我们已知：\n$$\\left\\{ \\begin{aligned} \\underline{\\mathbf{m}} _{A,i} = K _A^{-1} \\cdot \\underline{\\mathbf{P}} _{A,i}\\\\ \\underline{\\mathbf{m}} _{B,i} = K _B^{-1} \\cdot \\underline{\\mathbf{P}} _{B,i} \\end{aligned} \\right.$$ $\\underline{\\mathbf{P}} _{B,i} = K _B \\cdot \\underline{\\mathbf{m}} _{B,i} \\Rightarrow$ 将上面得到的 $\\underline{\\mathbf{m}} _{B,i}$ 带入\n$$\\underline{\\mathbf{P}} _{B,i} = K _B \\cdot \\Pi \\left( \\left( \\mathbf{R} _{BA} - \\dfrac{\\mathbf{t} _{BA} \\cdot \\mathbf{n} _A^\\top}{d} \\right) \\cdot \\underline{\\mathbf{m}} _{A,i} \\right)$$ $$\\underline{\\mathbf{P}} _{B,i} = K _B \\cdot \\Pi \\left( \\left( \\mathbf{R} _{BA} - \\dfrac{\\mathbf{t} _{BA} \\cdot \\mathbf{n} _A^\\top}{d} \\right) \\cdot K _A^{-1} \\cdot \\underline{\\mathbf{P}} _{A,i} \\right)$$ 回顾性质：\n$$K \\cdot \\Pi \\left( \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} \\right) = \\Pi \\left( K \\cdot \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} \\right)$$ 利用此性质，可得：\n$$\\underline{\\mathbf{P}} _{B,i} = \\Pi \\left( K _B \\cdot \\left( \\mathbf{R} _{BA} - \\dfrac{\\mathbf{t} _{BA} \\cdot \\mathbf{n} _A^\\top}{d} \\right) \\cdot K _A^{-1} \\cdot \\underline{\\mathbf{P}} _{A,i}\\right)$$ 2.2.4 得到单应性矩阵 $\\mathbf{H} _{AB}$ 假设\n$$\\mathbf{H} _{AB} = K _B \\cdot \\left( \\mathbf{R} _{BA} - \\dfrac{\\mathbf{t} _{BA} \\cdot \\mathbf{n} _A^\\top}{d} \\right) \\cdot K _A^{-1}$$\n因此：\n$$\\left\\{ \\begin{aligned} \u0026\\underline{\\mathbf{P}} _{B,i} = \\Pi \\left( \\mathbf{H} _{BA} \\cdot \\underline{\\mathbf{P}} _{A,i} \\right) \\quad \\quad A \\Rightarrow B\\\\ \u0026\\underline{\\mathbf{P}} _{A,i} = \\Pi \\left( \\mathbf{H} _{BA}^{-1} \\cdot \\underline{\\mathbf{P}} _{B,i} \\right) = \\Pi \\left( \\mathbf{H} _{AB} \\cdot \\underline{\\mathbf{P}} _{B,i} \\right) \\quad \\quad B \\Rightarrow A \\\\ \\end{aligned} \\right.$$ 通过单应性矩阵我们可以将某点从一个相机图片坐标系变换到另一个相机图片坐标系，也就是点映射关系\n2.2.5 单应性矩阵估计求解 $$\\mathbf{H} _{AB} = \\begin{bmatrix} h _1 \u0026 h _4 \u0026 h _7 \\\\ h _2 \u0026 h _5 \u0026 h _8 \\\\ h _3 \u0026 h _6 \u0026 h _9 \\end{bmatrix}$$ 这是一个齐次矩阵，它有 $9$ 个参数 $h _1$ 到 $h _9$，齐次矩阵在尺度上具有冗余性，所以会导致自由度的丢失\n简单的解法\u0026ndash;参数化（要估计的参数 = 自由参数） $$\\mathbf{H} _{AB} = \\begin{bmatrix} h _1 \u0026 h _4 \u0026 h _7 \\\\ h _2 \u0026 h _5 \u0026 h _8 \\\\ h _3 \u0026 h _6 \u0026 1 \\end{bmatrix}$$ $$\\mathbf{h} = \\begin{bmatrix} h _1 \\\\ \\vdots \\\\ h _8 \\end{bmatrix}$$ 如何估计 $\\mathbf{h}$？\n在这种情况下只要我们了解一个对应点就可以求得 $h _1$ 到 $h _8$ $$\\underline{\\mathbf{P}} _{A,i} = \\Pi \\left( \\begin{bmatrix} h _1 \u0026 h _4 \u0026 h _7 \\\\ h _2 \u0026 h _5 \u0026 h _8 \\\\ h _3 \u0026 h _6 \u0026 1 \\end{bmatrix} \\cdot \\underline{\\mathbf{P}} _{B,i} \\right)$$ 由于 $\\underline{\\mathbf{P}} _{A,i}$，$\\underline{\\mathbf{P}} _{B,i}$ 是齐次坐标，我们将其展开：\n$$\\begin{bmatrix} P _{A,i,x} \\\\ P _{A,i,y} \\\\ 1 \\end{bmatrix} = \\Pi \\left( \\begin{bmatrix} h _1 \u0026 h _4 \u0026 h _7 \\\\ h _2 \u0026 h _5 \u0026 h _8 \\\\ h _3 \u0026 h _6 \u0026 1 \\end{bmatrix} \\cdot \\begin{bmatrix} P _{B,i,x} \\\\ P _{B,i,y} \\\\ 1 \\end{bmatrix} \\right)$$ $$\\left\\{ \\begin{aligned} P _{A,i,x} = \\dfrac{h _1 \\cdot P _{B,i,x} + h _4 \\cdot P _{B,i,y} + h _7}{h _3 \\cdot P _{B,i,x} + h _6 \\cdot P _{B,i,y} + 1} \\\\ P _{A,i,y} = \\dfrac{h _2 \\cdot P _{B,i,x} + h _5 \\cdot P _{B,i,y} + h _8}{h _3 \\cdot P _{B,i,x} + h _6 \\cdot P _{B,i,y} + 1} \\end{aligned} \\right.$$ $$\\left\\{ \\begin{aligned} P _{A,i,x} \\cdot \\left( h _3 \\cdot P _{B,i,x} + h _6 \\cdot P _{B,i,y} + 1 \\right) = h _1 \\cdot P _{B,i,x} + h _4 \\cdot P _{B,i,y} + h _7 \\\\ P _{A,i,y} \\cdot \\left( h _3 \\cdot P _{B,i,x} + h _6 \\cdot P _{B,i,y} + 1 \\right) = h _2 \\cdot P _{B,i,x} + h _5 \\cdot P _{B,i,y} + h _8 \\end{aligned} \\right.$$ $$\\begin{bmatrix} P _{B,i,x} \u0026 0 \u0026 -P _{A,i,x} \\cdot P _{B,i,x} \u0026 P _{B,i,y} \u0026 0 \u0026 -P _{A,i,x} \\cdot P _{B,i,y} \u0026 1 \u0026 0 \\\\ 0 \u0026 P _{B,i,x} \u0026 -P _{A,i,y} \\cdot P _{B,i,x} \u0026 0 \u0026 P _{B,i,y} \u0026 -P _{A,i,y} \\cdot P _{B,i,y} \u0026 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} h _1 \\\\ h _2 \\\\ h _3 \\\\ h _4 \\\\ h _5 \\\\ h _6 \\\\ h _7 \\\\ h _8 \\end{bmatrix} = \\begin{bmatrix} P _{A,i,x} \\\\ P _{A,i _y} \\end{bmatrix}$$ 因为有 $8$ 个未知数，需要八个独立的线性方程，而每对对应点可以提供两个对应方程（即方程 59），因此需要至少四对对应点。即需要四个匹配 $\\left( P _{A,i}, P _{B,i} \\right) \\quad i = 1, 2, 3, 4$。\n$\\Rightarrow \\mathbf{h}^* = \\arg\\min _{\\mathbf{h}} \\sum _{i=1}^{4} \\left\\lVert M _i \\mathbf{h} - P _{A,i} \\right\\rVert _2^2 \\Rightarrow$ 线性最小二乘法\n3. 使用 RANSAC 算法进行稳健的单应性估计 3.1 目标 图像对齐与拼接：通过估计两幅图像之间的单应性（Homography），实现图像的自动拼接。 3.2 自动建立对应关系\u0026mdash;SIFT 算法 兴趣点检测 使用 SIFT 等算法在两幅图像中检测特征点（这段代码由老师提供），无需手动标记对应点，利用算法自动建立图像间的对应关系。 因此我们可以找到两幅图像中最相似的点对，但注意，点对并不一定正确对应。 也就是可能会出现错误匹配（离群点），这种情况下不可以直接用对应关系，我们将使用另一种算法叫做 RANSAC 来自动评估对应点之间的正确性，并得到最理想的 $H$ 矩阵并输出。 3.3 RANSAC 算法进行稳健估计 算法思想：\n随机抽样一致性（Random Sample Consensus） 是一种在存在离群点（错误点）的情况下估计模型参数（$H$）的稳健算法。 通过反复随机抽样，寻找最符合的模型。 $RANSAC$ 流程：\n重复 $N$ 次\n（迭代次数根据经验或计算确定）：\n随机选取 4 对匹配点：\n4 是估计单应性矩阵所需的最小匹配点数。 从所有的匹配点中随机选四个，不确定哪个对应关系正确，所以后续中有一个估计评判标准（欧几里得距离）。 估计单应性矩阵 $H^k$：\n使用选取的 4 对匹配点，通过 $DLT$ 算法（上个实验做过，其目的与作用是，在已知对应点的情况下，将一个相机视角转换到另一个相机视角）估计单应性矩阵。 计算误差并评估模型：\n对于所有匹配点（包括未选取的），将第二幅图像的点 $P _{B _i}$ 通过估计的 $H^k$ 转换，即 $H^k P _{B _i}$。\n计算变换后的点（估计点）与第一幅图像实际点 $P _{A _i}$ 之间的欧氏距离。\n定义代价函数\n：使用二值核（要么为 0 要么为 1）函数 $\\phi _c(d)$：\n当距离 $d \u0026lt; \\tau$ 时，认为匹配正确，代价为 0。 当距离 $d \\geq \\tau$ 时，认为匹配错误，代价为 1。 总代价 $L^k = \\sum _{i} \\phi _c(|P _{A _i} - H^k P _{B _i}|)$\n更新最佳模型：\n如果当前代价 $L^k$ 小于之前的最小代价 $L$，则更新 $L$ 和对应的 $H$。 最终输出：\n具有最小代价的单应性矩阵 $H$。 阈值 $\\tau$ 的选择：\n$\\tau$ 是判断匹配是否为内点的距离阈值，通常根据图像分辨率和匹配精度选择，一般在 $0.5$ 到 $3$ 个像素之间。 选择过大会增加错误匹配，过小会忽略正确匹配。 3.4 为什么不用传统的二次代价函数 敏感性问题： 二次代价函数（如最小二乘法）对离群点非常敏感，如果某个点的误差很大，会导致代价函数值过大，这时即使其他点的误差很小也没有用。 稳健性： 二值核函数对那些特别大、离谱的点不敏感（都等于 1），能够有效抑制离群点的影响，使得估计结果更稳健。 其他核函数： 除了二值核函数，还存在其他稳健核函数，如 $Huber$ 核、$Lorentzian$ 核等，可以在一定程度上兼顾误差大小和稳健性。 3.5 RANSAC 算法的局限性 参数数量影响： 当模型参数数量增加时，所需的随机采样次数会指数增长，计算成本显著提高。 适用范围： RANSAC 适用于参数数量较少的情况，如直线拟合、基础矩阵和单应性估计等。 4. 立体视觉中的对极几何 到目前为止，我们已经研究了平面场景的情况，使用了单应性（Homography）来描述两个视图之间的关系。然而，对于一般的三维场景，平面假设不再成立。为此，我们引入了对极几何（Epipolar Geometry）。\n4.1 对极几何 对极几何可以通过一个示意图很好地解释：\n考虑两个相机，分别位于参考系 1 和参考系 2\n相机 1 的光心为 $O _1$，相机 2 的光心为 $O _2$，空间中的一点 $U$ 投影到两个相机的图像平面上，得到点 $\\underline{m} _1$ 和 $\\underline{m} _2$。\n问题描述：\n在一般情况下，我们无法对点 $U$ 做出任何假设（与之前的平面场景不同）。 我们需要找到一种方法，在不知道 $U$ 的情况下，建立 $\\underline{m} _1$ 和 $\\underline{m} _2$ 之间的关系。 4.2 对极平面和对极线 对极平面\n$U$ 和光心 $O _1$、$O _2$ 定义了一个平面 $\\Rightarrow$ 点 $m _1$、$m _2$、$O _1$、$O _2$ 共面 $\\Rightarrow$ 称为对极平面。\n$\\text{Contrainte épipolaire} = \\text{coplanarité}$，即 $\\underline{m} _1$、$\\underline{m} _2$、$O _1$、$O _2$ 共面。\n在立体视觉中，基础矩阵 $F$ 和本质矩阵 $E$ 都依赖于共面性条件来计算。\n对极线\n对极平面与两个相机的图像平面相交，分别得到对极线 $l _1$ 和 $l _2$。\n$m _2$ 是三维点 $U$ 在第二个图像平面的投影，但根据对极几何的约束，$m _2$ 必须位于对极线 $l _2$ 上。\n$\\Rightarrow$ 给定点 $m _1$ 的位置，可以通过基础矩阵 $F$ 确定对应的对极线 $l _2$：$l _2 = F \\cdot m _1$\n基础矩阵 $F$ 捕捉了两个相机之间的相对姿态和内在参数信息。这个公式表明，给定点 $m _1$，可以计算出 $m _2$ 必须位于的对极线 $l _2$。\n4.3 对极约束 目标：利用上述几何关系，形式化对极约束，建立 $m_1$ 和 $m_2$ 之间的数学关系。\n定义向量：\n$$\\left\\{ \\begin{aligned} \u0026\\mathbf{\\underline{m}_1} \\text{ is the vector from the optical center } O_1 \\text{ to the image point } \\underline{m}_1 \\quad \\overrightarrow{O_1 m_1}^{1} \\\\ \u0026\\mathbf{\\underline{m}_2} \\text{ is the vector from the optical center } O_2 \\text{ to the image point } \\underline{m}_2 \\quad \\overrightarrow{O_2 m_2}^{2} \\\\ \u0026\\mathbf{t_{12}} = \\overrightarrow{O_1 O_2}^{1} \\text{ is the translation vector between the two camera optical centers} \\end{aligned} \\right.$$ 定义对极平面的法向量：\n$$\\left\\{ \\begin{aligned} \u0026\\text{In reference frame } 1, \\quad \\overrightarrow{\\mathbf{n}_1}^{1} = \\underline{\\mathbf{m}}_1 \\times \\mathbf{t}_{12} \\\\ \u0026\\text{In reference frame } 2, \\quad \\overrightarrow{\\mathbf{n}_2}^{2} = \\mathbf{R}_{21} \\overrightarrow{\\mathbf{n}_1}^{1}, \\text{ where } \\mathbf{R} \\text{ is the rotation matrix between the cameras} \\end{aligned} \\right.$$ 注意：\n其中 $\\times$ 表示两个向量之间的叉积运算。叉积的结果是一个向量，它垂直于运算的两个向量，方向由右手定则决定，大小为这两个向量构成的平行四边形的面积。 法向量的坐标系变换不用考虑平移部分，因为单位法向量并不是坐标位置，方向向量在旋转过程中大小不变，不受平移的影响。总而言之，法向量只考虑旋转矩阵，而点则需要考虑旋转加平移。 $$\\overrightarrow{\\mathbf{n}_2}^{2} = \\mathbf{R}_{21} \\cdot \\overrightarrow{\\mathbf{n}_1}^{1} = \\mathbf{R}_{21} \\cdot \\left( \\underline{\\mathbf{m}}_1 \\times \\mathbf{t}_{12} \\right) = \\mathbf{R}_{21} \\cdot \\underline{\\mathbf{m}}_1 \\times \\mathbf{R}_{21} \\cdot \\mathbf{t}_{12}$$ 由于之前我们已知 $\\mathbf{t} _{21} = \\mathbf{R} _{21} \\cdot \\mathbf{t} _{12}$，所以上式变为\n$$\\overrightarrow{\\mathbf{n}_2}^{2} = \\mathbf{t}_{21} \\times \\left( \\mathbf{R}_{21} \\cdot \\underline{\\mathbf{m}}_1 \\right)$$ 回顾叉积运算性质：\n$$\\mathbf{a} \\times \\mathbf{b} = \\begin{bmatrix} a_x \\\\ a_y \\\\ a_z \\end{bmatrix} \\times \\begin{bmatrix} b_x \\\\ b_y \\\\ b_z \\end{bmatrix} = \\begin{bmatrix} a_y b_z - a_z b_y \\\\ a_z b_x - a_x b_z \\\\ a_x b_y - a_y b_x \\end{bmatrix}_{3 \\times 1} \\Rightarrow \\left[\\mathbf{a}\\right]_{\\times} = \\begin{bmatrix} 0 \u0026 -a_z \u0026 a_y \\\\ a_z \u0026 0 \u0026 -a_x \\\\ -a_y \u0026 a_x \u0026 0 \\end{bmatrix}$$ $$\\mathbf{a} \\times \\mathbf{b} = \\left[\\mathbf{a}\\right]_{\\times} \\mathbf{b} = \\begin{bmatrix} 0 \u0026 -a_z \u0026 a_y \\\\ a_z \u0026 0 \u0026 -a_x \\\\ -a_y \u0026 a_x \u0026 0 \\end{bmatrix} \\begin{bmatrix} b_x \\\\ b_y \\\\ b_z \\end{bmatrix}$$ 利用上述性质，我们可以看出，叉积运算可以变成矩阵运算，因此我们利用以上性质得到：\n$$\\overrightarrow{\\mathbf{n}_2}^{2} = \\mathbf{t}_{21} \\times \\left( \\mathbf{R}_{21} \\cdot \\underline{\\mathbf{m}}_1 \\right) = \\left[ \\mathbf{t}_{21} \\right]_{\\times} \\cdot \\mathbf{R}_{21} \\cdot \\underline{\\mathbf{m}}_1$$ 因为 $\\overrightarrow{\\mathbf{n}_2}^{2}$ 是 $\\mathbf{m}_2$ 的法线 $\\Rightarrow \\mathbf{m}_2^\\top \\cdot \\overrightarrow{\\mathbf{n}_2}^{2} = 0$\n$$\\mathbf{m}_2^\\top \\cdot \\left[ \\mathbf{t}_{21} \\right]_{\\times} \\cdot \\mathbf{R}_{21} \\cdot \\underline{\\mathbf{m}}_1 = 0$$ $$\\mathbf{m}_2^\\top \\cdot \\left( \\left[ \\mathbf{t}_{21} \\right]_{\\times} \\cdot \\mathbf{R}_{21} \\right) \\cdot \\underline{\\mathbf{m}}_1 = 0$$ 4.4 本质矩阵（matrice essentielle） 公式\n假设 $\\mathbf{E} _{21} = \\left[ \\mathbf{t} _{21} \\right] _{\\times} \\cdot \\mathbf{R} _{21} \\quad \\Rightarrow \\quad \\text{matrice essentielle}$\n它包含了两个相机之间的相对旋转 $\\mathbf{R}$ 和平移 $\\mathbf{t}$ 的信息。\n原式 $ = \\underline{\\mathbf{m}} _2^\\top \\cdot \\mathbf{E} _{21} \\cdot \\underline{\\mathbf{m}} _1 = 0$\n自由度\n$$ 5 \\text{ degre de liberte} \\\\ \\downarrow\\\\ 5 \\text{ DDL} \\left( \\begin{array}{c} 3 , \\mathbf{R} _{21} \\quad \\text{rotation} \\\\ \\quad 2 , \\mathbf{t} _{21} \\quad \\text{translation} \\end{array} \\right)\\\\ \\downarrow\\\\ \\quad \\quad | \\mathbf{t} _{21} |_2 \\quad \\text{ inconnue} $$ 自由度：\n$$\\left\\{ \\begin{aligned} \u0026\\text{The rotation matrix } \\mathbf{R} \\text{ has 3 degrees of freedom} \\\\ \u0026\\text{The translation vector } \\mathbf{t} \\text{ has 2 degrees of freedom (since the scale is unknown)} \\\\ \u0026\\text{Therefore, } \\mathbf{E} \\text{ has 5 degrees of freedom} \\end{aligned} \\right.$$ 自由度（degree of freedom, DoF）是指描述本质矩阵所需的独立参数数量。在几何和线性代数中，自由度反映了系统在不受限制的情况下可以独立变化的方向或方式。 旋转矩阵具有 3 个自由度，描述了三维空间中的旋转。 平移向量理论上在三维空间中有 3 个自由度。但是本质矩阵中的平移向量一般只关注方向，对长度忽略（未知），所以平移向量只剩下 2 个有效的自由度，描述了平移的方向。 4.5 基础矩阵（Matrix Fundamental） 对上述公式继续变换：\n$$\\underline{\\mathbf{m}}_2^\\top \\cdot \\mathbf{E}_{21} \\cdot \\underline{\\mathbf{m}}_1 = 0$$ 已知：\n$$\\left\\{ \\begin{aligned} \\underline{\\mathbf{m}}_2 = K^{-1} \\cdot \\underline{\\mathbf{P}}_2 \\\\ \\underline{\\mathbf{m}}_1 = K^{-1} \\cdot \\underline{\\mathbf{P}}_1 \\end{aligned} \\right.$$ $$\\underline{\\mathbf{P}}_2^\\top \\cdot (K^{-1})^\\top \\cdot \\mathbf{E}_{21} \\cdot K^{-1} \\cdot \\underline{\\mathbf{P}}_1 = 0$$ 当相机内参未知或未被考虑时，我们引入一个基础矩阵 $\\mathbf{F}$ 来覆盖 $K$。\n假设 $\\mathbf{F} _{21}= (K^{-1})^\\top \\cdot \\mathbf{E} _{21} \\cdot K^{-1}$\n$$\\mathbf{F}_{21} : \\text{ matrice fondamentale} \\quad \\Rightarrow \\quad 7 \\text{ DDL}\\quad \\left\\{ \\begin{aligned} \u0026 \\text{- matrice homogène} \\\\ \u0026 \\text{- rang}(\\mathbf{F}_{21}) = 2 \\quad \\Rightarrow \\quad \\det(\\mathbf{F}_{21}) = 0 \\end{aligned} \\right.$$ 性质：\n$$\\left\\{ \\begin{aligned} \u0026\\text{齐次：基础矩阵 } \\mathbf{F} \\text{ 是齐次矩阵，可以乘以任意非零标量而不改变其性质} \\\\ \u0026\\text{秩约束：} \\mathbf{F} \\text{ 的秩为 } 2 \\end{aligned} \\right.$$ 原式 $= \\underline{\\mathbf{P}} _2^\\top \\cdot \\mathbf{F} _{21} \\cdot \\underline{\\mathbf{P}} _1 = 0$\n$$\\text{设：} \\quad \\mathbf{L}_2 = \\mathbf{F}_{21} \\cdot \\underline{\\mathbf{P}}_1 = \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix}$$ $$\\underline{\\mathbf{P}}_2^\\top \\cdot \\mathbf{L}_2 = 0 \\quad \\Leftrightarrow \\quad a P_{2,x} + b P_{2,y} + c = 0$$ 这就是相机 2 的图像平面中的直线方程 $\\Rightarrow$ 对极线\n4.6 本质和基础矩阵的估计 相机已经校准 $\\Rightarrow$ 本质矩阵 $\\mathbf{E}$ 的估计（5 个自由度）$\\Rightarrow$ 5 点对应算法\n相机未校准 $\\Rightarrow$ 基础矩阵 $\\mathbf{F}$ 的估计（7 个自由度）$\\Rightarrow$ 7 点对应算法\n求解 $\\Rightarrow$ 8 点对应算法 $\\Rightarrow$ 故意忽略约束条件 $\\det(\\mathbf{F}) = 0$\n4.7 算法 8 点对应算法步骤：\n收集匹配点对：\n$\\mathbf{F}$ 有 7 个自由度，但在算法中忽略了秩为 2 的约束，因此需要至少 8 对匹配点来估计 $\\mathbf{F}$。\n构建线性方程组\n对于每一对匹配点 $(\\mathbf{m} _1, \\mathbf{m} _2)$，构建方程：\n$$\\underline{\\mathbf{m}}_2^\\top \\cdot \\mathbf{E}_{21} \\cdot \\underline{\\mathbf{m}}_1 = 0$$ $$\\underline{\\mathbf{P}}_2^\\top \\cdot \\mathbf{F}_{21} \\cdot \\underline{\\mathbf{P}}_1 = 0$$ 求解：\n将方程组表示为：$\\underline{\\mathbf{P}} _2^\\top \\cdot \\mathbf{L} _2 = 0$\n$RANSAC$ 算法步骤\n处理匹配点对中的离群点（错误匹配），稳健地估计 $\\mathbf{F}$\n随机采样：利用 8 点对应算法 来估计 $\\mathbf{F}$ 评估模型：利用估计得到的 $\\mathbf{F}$ 来计算所有匹配点对的对极约束误差，即点到对应对极线的距离 判断内点：根据设定的距离阈值，判断哪些匹配点是内点 迭代：重复上述过程，直到找到内点数量最多的模型 5. Ajustement de faisceaux (束调整) 束调整是一种同时优化摄像机参数（包括位置、姿态和内参）和场景中三维点位置的技术 其核心思想是通过最小化三维点在图像上的重投影误差，使得优化后的模型与实际观测更加吻合 记住五个字：最小化投影误差 5.1 两个摄像机的情况 5.1.1 数据 $$\\left( P _{A,i}, P _{B,i} \\right) _{i=1,\\dots,N} \\implies N \\text{ correspondences}$$\n5.1.2 要估计的参数 摄像机的姿态以及三维点云数据集\n$$\\mathbf{R} _{W1} \\quad \\mathbf{t} _{W1} \\quad \\mathbf{R} _{W2} \\quad \\mathbf{t} _{W2} \\quad \\left\\{\\mathbf{U}^w_i \\right\\}_{i=1,\\dots,N}$$ 5.1.3 损失函数 $$\\mathcal{L} \\left( \\mathbf{R}_{w1}, \\mathbf{t}_{w1}, \\mathbf{R}_{w2}, \\mathbf{t}_{w2}, \\left\\{ \\mathbf{U}^w_i \\right\\}_{i=1,\\dots,N} \\right)= \\sum_{i=1}^{N} \\left( \\left\\lVert P_{1,i} - K_1 \\Pi \\left( \\mathbf{R}_{w1}^\\top \\mathbf{U}_i^{w} - \\mathbf{R}_{w1}^\\top \\mathbf{t}_{w1} \\right) \\right\\rVert_2^2 +\\left\\lVert P_{2,i} - K_2 \\Pi \\left( \\mathbf{R}_{w2}^\\top \\mathbf{U}_i^{w} - \\mathbf{R}_{w2}^\\top \\mathbf{t}_{w2} \\right) \\right\\rVert_2^2 \\right)$$ 其中：\n$K _A$ 和 $K _B$ 是摄像机 $A$ 和 $B$ 的内参矩阵。 $\\Pi(\\cdot)$ 是投影函数，将三维点投影到二维平面上。 $\\mathbf{R} _{w1}^\\top$ 和 $\\mathbf{R} _{w2}^\\top$ 等价于 $\\mathbf{R} _{1w}$ 和 $\\mathbf{R} _{2w}$，即将点从世界坐标系转换到摄像机坐标系。 $\\mathbf{R} _{w1}^\\top \\mathbf{t} _{w1}$ 等价于 $\\mathbf{t} _{1w}$，表示平移向量。 $\\mathbf{U} _i^{1}= \\mathbf{R} _{w1}^\\top \\cdot \\mathbf{U} _i^{w} - \\mathbf{R} _{w1}^\\top \\cdot \\mathbf{t} _{w1}$，也就是将 $\\mathbf{U} _i^{w}$ 变换到 $\\mathbf{U} _i^{1}$，即从世界坐标系变换到相机坐标系。 做差：相机 $A$ 或 $B$ 中的图像坐标（实际）减去三维空间旋转变换得来的估计图像坐标，等于重投影误差。 5.2 多个摄像机的情况 5.2.1 数据 每张图像中检测到的点为：\n$$\\left\\{ \\left\\{ P _{m,i} \\right\\} _{i=1,\\dots,N _m} \\right\\} _{m=1,\\dots,M}$$ 这些点在不同视角下的图像中可以形成轨迹（tracks）\n第 $m$ 个摄像机检测到的点，其中 $N _m$ 是第 $m$ 个摄像机检测到的点的数量\n$$\\left\\{ \\text{p2d-id} _m, \\ \\text{p3d-id} _m \\right\\} _{m=1,\\dots,M}$$ 其中：\n$\\text{p2d-id} _m$ 是二维点在图像中的索引 $\\text{p3d-id} _m$ 是对应的三维点在点云中的索引 它们的大小尺寸都是 $C _m \\times 1$ 5.2.2 要估计的参数 相机外参：\n$$ \\left\\{ \\left( \\mathbf{R} _{wm}, \\mathbf{t} _{wm} \\right) \\right\\} _{m=1,\\dots,M} $$ 三维点的位置：\n$$ \\left\\{ \\mathbf{U} _i^{w} \\right\\} _{i=1,\\dots,N} $$ 5.2.3 损失函数 代价函数扩展为对所有摄像机和所有检测到的点进行误差计算，将投影点与实际观测点之间的距离最小化：\n$$\\mathcal{L}(x) = \\sum_{m=1}^{M} \\sum_{c=1}^{C_m} \\left\\| P_{m,\\ \\text{p2d-id}_m(c)} - K_m \\Pi \\left( \\mathbf{R}_{wm}^\\top \\mathbf{U}_{\\text{p3d-id}_m(c)}^{w} - \\mathbf{R}_{wm}^\\top \\mathbf{t}_{wm} \\right) \\right\\|_2^2$$ $C _m$ 是第 $m$ 台摄像机的观测数量 $\\mathbf{U} _{\\text{p3d-id} _m(c)}^{w}$ 是与观测对应的三维点 我们可以简单地将上述代价函数简化成：\n$$\\mathcal{L}(x) = \\sum_{i=1}^{N} \\left\\| f_i(x) \\right\\|_2^2 \\quad \\left\\{ \\begin{array}{l} x \\in \\mathbb{R}^D \\\\ f_i : \\mathbb{R}^D \\rightarrow \\mathbb{R}^B \\end{array} \\right.$$ $x$ 是所有待优化的参数（摄像机参数和三维点坐标） $f _i(x)$ 是第 $i$ 个残差函数，表示第 $i$ 个观测的重投影误差 我们的目标是找到 $x$，使得 $\\mathcal{L}(x)$ 最小化。这是一个非线性最小二乘问题，通常使用迭代的方法求解 5.3 高斯牛顿算法 用于非线性最小二乘问题的一种迭代优化算法 $\\Rightarrow \\text{ iteratif } \\quad \\delta_{k+1} = \\delta_k + d_k$ $\\text{Linearisation de } f_i: \\quad f_i(x_k + d_k) \\approx f_i(x_k) + \\mathbf{J}_i(x_k)\\cdot d_k$\n$\\delta x$是参数的增量，需要求解\n对于每次迭代，我们在当前估计$x_k$附近对$f_i(x)$进行泰勒展开，并忽略高阶项\n$ f_i(\\delta_k + d_k) \\in \\mathbb{R}^B $\n$ f_i(\\delta_k) \\in \\mathbb{R}^B$ $\\mathbf{J}_i(\\delta_k) \\in \\mathbb{R}^{B \\times D} $ $ d_k \\in \\mathbb{R}^D $\n其中雅可比矩阵为:\n$$\\mathbf{J}_i(x_k) = \\frac{\\partial f_i(x_k + d_k)}{\\partial d_k} \\bigg|_{d_k=0}$$ 代表了在点 $x_k$ 处函数 $f_i$ 对于 $d_k$ 的偏导数，并且此偏导数是在 $d_k = 0$ 的条件下计算的 描述了在点$ x_k $处函数$ f_i $的线性变化率 线性最小二乘法 $$ L_k(d_k) = \\sum_{i=1}^{N} \\left| f_i(x_k) + \\mathbf{J}_i(x_k) \\cdot d_k \\right|_2^2 $$\n​\t$$\\quad \\mathbf{J}_k = \\begin{bmatrix} \\quad J_1(x_k) \\\\ \\quad J_2(x_k) \\\\ \\quad J_3(x_k) \\\\ ​ \\vdots \\\\ \\quad J_N(x_k) \\end{bmatrix} \\quad \\mathbf{b}_k = \\begin{bmatrix} \\quad f_1(x_k) \\\\ \\quad f_2(x_k) \\\\ \\quad \\vdots \\\\ \\quad f_N(x_k) \\end{bmatrix}$$\n$ \\mathbf{b}_k$是所有残差的组合 线性最小二乘问题变为： $$ L_k(d_k) = \\lVert b_k + J_k \\cdot d_k \\rVert_2^2 $$ 通过最小化$L_k(\\delta x)$，我们可以得到线性方程组：$J_k^T \\cdot J_k \\cdot d k = -J_k^T \\cdot b_k \\quad $\n$其中b_k ∝ \\text{ gradient}$\n左边的矩阵$\\mathbf{J}_k^T \\mathbf{J}_k$是海森矩阵的近似 右边的向量$-\\mathbf{J}_k^T \\mathbf{b}_k$是梯度的负值\n求解这个线性系统，得到参数更新$d_x$\nLevenberg-Marquardt算法\n在高斯-牛顿算法的基础上引入阻尼因子$\\lambda$，使得优化过程在接近解时具有高斯-牛顿的快速收敛特性，而在远离解时具有梯度下降的稳定性\n常用于非线性最小二乘问题的迭代优化算法\n目标函数: $L_k(d_k) = \\lVert b_k + J_k d_k \\rVert_2^2 + \\lambda \\lVert d_k \\rVert_2^2$ $\\quad \\quad \\Rightarrow (J_k^T J_k + \\lambda I_k)d_k = -J_k^T b_k$\n$\\lambda$是阻尼因子\n$$\\begin{cases} \\text{Si } \\lambda = 0 \u0026 \\Rightarrow \\text{Gauss-Newton} \\\\ \\text{Si } \\lambda \\rightarrow +\\infty \u0026 \\Rightarrow \\lambda d_k \\rightarrow -J_k^T b_k \\quad \\text{descente de gradient} \\end{cases}$$ 如果新的代价函数值降低了（说明更新有效），则减小$\\lambda$，使算法更接近高斯-牛顿法，加快收敛\n如果代价函数值没有降低， 则增大$\\lambda$，使算法更接近梯度下降法，保证稳定性\n5.4 算法步骤总结 在实际应用中，Levenberg-Marquardt算法的步骤如下：\n初始化：\n设定初始参数$x$和阻尼因子$\\lambda$ 计算初始代价函数$L_{\\min}$ 迭代：\n计算雅可比矩阵$\\mathbf{J}$和残差$\\mathbf{b}$\n求解线性系统：\n$ (J^T J + \\lambda I_d) d = -J^T b$\n更新参数：\n$x′=x+d$\n计算新的代价函数$L'$\n判断更新效果：\n如果$L\u0026rsquo; \u0026lt; L_{\\min}$（代价函数降低）： 接受更新：$x = x\u0026rsquo;$，$L_{\\min} = L'$ 减小$\\lambda$：$\\lambda = \\lambda / 2$ 继续迭代 否则（代价函数未降低）： 拒绝更新，不改变$x$ 增大$\\lambda$：$\\lambda = 2\\lambda$ 检查$\\lambda$是否超过最大值，若超过则停止迭代 终止条件：\n当$\\lambda$超过预设的最大值，或者参数更新的幅度小于阈值时，停止迭代 ","permalink":"http://localhost:1313/zh/posts/signal_cn/3d%E9%87%8D%E5%BB%BA%E7%90%86%E8%AE%BA/","summary":"人工智能中计算机视觉领域下科目，重点包括 相机模型校正、单应性估计、对极几何、束调整等理论内容，以实现图像等校正、拼接、光束校准等应用","title":"3D重建理论"},{"content":"应用随机梯度算法解决信道均衡问题 简介 利用优化算法并构建滤波器来解决信号通道的失真（噪声、多径传播）等问题。在数字通信中，发射机向接收机传输一个符号序列 ${s(n)}_{n\u0026gt;0}$，这些符号取值为 $1$ 或 $-1$，来编码有用的信息。但是在传输过程中，通常信号会失真，这种失真通常可被视为，与传播信道对应滤波器的卷积。\n因此，在信号接收端，需要对该传播信道进行去卷积以恢复被传输的符号。但是在去卷积之前，必须对信道进行估计，也就是了解信道的具体特性（例如信道的冲激响应或频率响应）。为了了解特性，我们需要使用假设已知的训练序列。\n训练序列是 发射机和接收机之间 预先约定的已知符号序列。接收端在收到经过信道传输的训练序列后，可以将其与原始序列进行比较，从而推断信道的特性。这些信道特性我们称之为“信道系数” $w(k)$。在信道系数估计完成后，我们就可以设计滤波器来实现去卷积操作，来减小由于信道引起的失真。\n在这种情况下，随机梯度算法（LMS）被经典地用于该方面。因此，后续我们将使用 LMS 应用于一个简化的信道均衡问题。\n传输链路的建模与仿真 $$w(k) = \\begin{cases} \\frac{1}{2} \\left( 1 + \\cos\\left(\\frac{2\\pi}{\\beta}(k - 1)\\right) \\right), \u0026 \\text{for } k = 0, 1, 2, \\\\ 0, \u0026 \\text{else} \\end{cases}$$ 此外，接收信号受到加性高斯噪声 $u(n)$ 的干扰，加性高斯噪声具有白噪音的特性（独立同分布），零均值，方差为 $\\sigma^2 = 0.001$。\n因此，信号的表达式为：\n$$x(n) = \\sum_{k=0}^{2} w(k)s(n-k) + u(n)$$ 其中， $\\beta$ 控制信道引入的失真水平。\n我们先在 Matlab 中使用函数 $stem$ 生成由符号 $1$ 和 $-1$ 组成的随机序列信号 $s(n)$，并假设 $1$ 和 $-1$ 取值是等概率的。我们可以使用函数 $randn$，并假设信号的长度为 $N = 1500$。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 % Paramètres N = 1500; % signal aléatoire +1 et -1 s = sign(randn(N,1)); % Tracé figure; stem(s); % tracer en discret title(\u0026#39;Signal s(n)\u0026#39;, \u0026#39;FontSize\u0026#39;, 16); xlabel(\u0026#39;Taille du signal N\u0026#39;, \u0026#39;FontSize\u0026#39;, 14); ylabel(\u0026#39;s(n)\u0026#39;, \u0026#39;FontSize\u0026#39;, 14); set(gca, \u0026#39;FontSize\u0026#39;, 12); % agrandir les graduations 只展示了 200 的序列\n由于信号 $s(n)$ 是由符号 $1$ 和 $-1$ 组成的随机序列，且被假设等概率的:\n$X = {+1, -1}$\n$p_{+1} = p_{-1} = \\frac{1}{2}$\n因此，$s(n)$ 的期望形式为：\n$$E[s(n)] = p_{+1} \\cdot (+1) + p_{-1} \\cdot (-1)$$ $$E[s(n)] = \\frac{1}{2} \\cdot (+1) + \\frac{1}{2} \\cdot (-1)$$ $$E[s(n)] = 0$$ 我们接下来绘制传播信道，其公式为:\n$$w(k) = \\frac{1}{2} \\left( 1 + \\cos \\left( \\frac{2\\pi}{\\beta}(k - 1) \\right) \\right)$$ 对 $\\beta$ 取不同值： $0.25$, $2$ 和 $4$ 来进行观察\n• 对于 $\\beta = 0.25$ ：\n当 $\\beta$ 较小时，例如 $0.25$，传输信道具有较短的脉冲响应，这会导致余弦函数的快速变化。在这种情况下，可以预期 $w(k)$ 在 $k = 0$ 到 $k = 2$ 之间有显著变化。\n$$w(0) = \\frac{1}{2} \\left( 1 + \\cos \\left( \\frac{2\\pi}{0.25}(-1) \\right) \\right) = \\frac{1}{2} \\left( 1 + \\cos(-8\\pi) \\right) = \\frac{1}{2}(1 + 1) = 1$$ $$w(1) = \\frac{1}{2} \\left( 1 + \\cos \\left( \\frac{2\\pi}{0.25}(0) \\right) \\right) = \\frac{1}{2} \\left( 1 + \\cos(0) \\right) = \\frac{1}{2}(1 + 1) = 1$$ $$w(2) = \\frac{1}{2} \\left( 1 + \\cos \\left( \\frac{2\\pi}{0.25}(1) \\right) \\right) = \\frac{1}{2} \\left( 1 + \\cos(8\\pi) \\right) = \\frac{1}{2}(1 + 1) = 1$$ 因此，当 $\\beta = 0.25$ 时，$w(k)$ 在点 $k = 0, 1, 2$ 处的值均为 $1$。\n可以看到传播信道在前 $3$ 个采样点上具有恒定的脉冲响应。\n• 对于 $\\beta = 2$ ：\n$$w(0) = \\frac{1}{2} \\left( 1 + \\cos \\left( \\frac{2\\pi}{2}(-1) \\right) \\right) = \\frac{1}{2} \\left( 1 + \\cos(-\\pi) \\right) = \\frac{1}{2}(1 - 1) = 0$$ $$w(1) = \\frac{1}{2} \\left( 1 + \\cos \\left( \\frac{2\\pi}{2}(0) \\right) \\right) = \\frac{1}{2} \\left( 1 + \\cos(0) \\right) = \\frac{1}{2}(1 + 1) = 1$$ $$w(2) = \\frac{1}{2} \\left( 1 + \\cos \\left( \\frac{2\\pi}{2}(1) \\right) \\right) = \\frac{1}{2} \\left( 1 + \\cos(\\pi) \\right) = \\frac{1}{2}(1 - 1) = 0$$ 因此，对于 $\\beta = 2$，$w(k)$ 在点 $k = 0$ 和 $k = 2$ 的值为 $0$，而在点 $k = 1$ 的值为 $1$。\n可以看到传播信道的脉冲响应主要集中在 $k = 1$，主要影响第一个延迟的信号。\n• 对于 $\\beta = 4$ ：\n当 $\\beta$ 更大时，例如 $4$，传输信道的脉冲响应周期进一步延长，余弦函数的变化变得更慢。在这种情况下，可以预期 $w(k)$ 在 $k = 0$ 到 $k = 2$ 之间的变化较小。\n$$w(0) = \\frac{1}{2} \\left( 1 + \\cos \\left( \\frac{2\\pi}{4}(-1) \\right) \\right) = \\frac{1}{2} \\left( 1 + \\cos\\left(-\\frac{\\pi}{2}\\right) \\right) = \\frac{1}{2}(1 + 0) = \\frac{1}{2}$$ $$w(1) = \\frac{1}{2} \\left( 1 + \\cos \\left( \\frac{2\\pi}{4}(0) \\right) \\right) = \\frac{1}{2} \\left( 1 + \\cos(0) \\right) = \\frac{1}{2}(1 + 1) = 1$$ $$w(2) = \\frac{1}{2} \\left( 1 + \\cos \\left( \\frac{2\\pi}{4}(1) \\right) \\right) = \\frac{1}{2} \\left( 1 + \\cos\\left(\\frac{\\pi}{2}\\right) \\right) = \\frac{1}{2}(1 + 0) = \\frac{1}{2}$$ 因此，对于 $\\beta = 4$，$w(k)$ 在点 $k = 0$ 和 $k = 2$ 的值为 $0.5$，而在点 $k = 1$ 的值为 $1$。\n可以看到传播信道具有一个对称的脉冲响应，其峰值位于 $k = 1$，如 $\\beta = 2$ 的情况（图 3），但它在第一个和第三个采样点上也包含非零值。即脉冲响应更加分散且对称，但主要集中在中心。\n这表明，具有均匀或扩展脉冲响应的信道需要均衡器来补偿分布在多个采样点上的失真；而具有集中脉冲响应的信道可能更容易均衡，因为失真主要局限于单个采样点。\n后续中，我们将使用 $\\beta = 0.25$，通过已经构建好的随机序列信号 $s(n)$，使用 Matlab 函数 $filter$ 来添加加性噪声，以获得信号 $x(n)$。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 %% Q4 % Réponse impulsionnelle w = [1 1 1]; % Filtrage de s(n) x = filter(w, 1, s); % filter(den, num, fonction à filtrer) sigma_u = sqrt(0.001); % ecart type du signal % Ajout bruit gaussien u = sigma_u * randn(N,1); x = x + u; % Tracé figure; stem(x); title(\u0026#39;Signal x(n)\u0026#39;, \u0026#39;FontSize\u0026#39;, 16); xlabel(\u0026#39;Taille du signal N\u0026#39;, \u0026#39;FontSize\u0026#39;, 14); ylabel(\u0026#39;x(n)\u0026#39;, \u0026#39;FontSize\u0026#39;, 14); set(gca, \u0026#39;FontSize\u0026#39;, 12); % agrandir les graduations 实现均衡 为了实现信道均衡，我们最终目标是确定最优维纳滤波器 $h_{\\text{opt}}$ 的理论表达式，以最小化均方误差。为此，首先需要计算信号 $x(n)$ 的自相关函数和 $x(n)$ 与 $d(n)$ 的互相关函数，分别记为 $r_{xx}(k)$ 和 $r_{dx}(k)$。对于真实且平稳的信号，其相关函数公式为：\n$$r_{xx}(k) = E[x(n)x(n-k)]$$ $$r_{dx}(k) = E[d(n)x(n-k)]$$ 我们首先研究（由传播信道卷积得到的）发射信号 $m(n)$ 的自相关函数 $r_{mm}(k)$，其中 $m(n) = \\sum_{k=0}^{2} w(k)s(n-k)$。首先计算 $r_{mm}(0)$、 $r_{mm}(1)$ 和 $r_{mm}(2)$ 来观察结果。同时注意，我们之前假设符号 $s(n)$ 为相互独立的。\n计算 $r_{mm}(0)$ $$r_{mm}(0) = E \\left[ \\left( \\sum_{k=0}^{2} w(k)s(n-k) \\right)^2 \\right]$$ $$r_{mm}(0) = \\sum_{k=0}^{2} w^2(k) E[s^2(n-k)]$$ 由于 $s(n)$ 是相互独立的，因此有：\n$$E[s^2(n-k)] = 1$$ 由于 $s(n)$ 的特性：\n$$E[s^2(n)] = 1 \\cdot P(s(n) = 1) + 1 \\cdot P(s(n) = -1)$$ $$E[s^2(n)] = 1 \\cdot \\frac{1}{2} + 1 \\cdot \\frac{1}{2} = 1$$ 因此：\n$$r_{mm}(0) = \\sum_{k=0}^{2} w^2(k)$$ $$r_{mm}(0) = w(0)^2 + w(1)^2 + w(2)^2 = 3$$ 计算 $r_{mm}(1)$ $$r_{mm}(1) = E[m(n)m(n-1)]$$ $$r_{mm}(1) = E \\left[ \\left( \\sum_{k=0}^{2} w(k)s(n-k) \\right) \\left( \\sum_{j=0}^{2} w(j)s(n-1-j) \\right) \\right]$$ $$r_{mm}(1) = \\sum_{k=0}^{2} \\sum_{j=0}^{2} w(k)w(j) E[s(n-k)s(n-1-j)]$$ 同样，仅当 $k = j + 1$ 时，相关函数才不为零，因此：\n$$r_{mm}(1) = w(0)w(1)E[s(n-1)s(n-1)] + w(1)w(2)E[s(n-2)s(n-2)]$$ 由于 $E[s(n)] = 0$，因此：\n$$E[s^2(n)] = 1$$ 因此：\n$$r_{mm}(1) = w(0)w(1) \\cdot 1 + w(1)w(2) \\cdot 1$$ $$r_{mm}(1) = w(0)w(1) + w(1)w(2) = 2$$ 计算 $r_{mm}(2)$ $$r_{mm}(2) = E \\left[ \\left( \\sum_{k=0}^{2} w(k)s(n-k) \\right) \\left( \\sum_{j=0}^{2} w(j)s(n-2-j) \\right) \\right]$$ $$r_{mm}(2) = \\sum_{k=0}^{2} \\sum_{j=0}^{2} w(k)w(j) E[s(n-k)s(n-2-j)]$$ 同样，仅当 $k = j + 2$ 时，相关函数才不为零，因此：\n$$r_{mm}(2) = w(0)w(2)E[s(n-2)s(n-2)]$$ 由于 $E[s(n)] = 0$，因此：\n$$E[s^2(n-2)] = 1$$ 因此：\n$$r_{mm}(2) = w(0)w(2) = 1$$ 接下来我们需要找到当 $k \u0026gt; 2$ 时，$r_{mm}(k) = 0$ 并推出任意 $k$ 时 $r_{mm}(k)$ 的表达式。\n为了验证 $r_{mm}(k) = 0$ 当 $k \u0026gt; 2$，我们先计算 $r_{mm}(3)$ 看看：\n$$r_{mm}(3) = E \\left[ \\left( \\sum_{k=0}^{2} w(k)s(n-k) \\right) \\left( \\sum_{j=0}^{2} w(j)s(n-3-j) \\right) \\right]$$ $$r_{mm}(3) = \\sum_{k=0}^{2} \\sum_{j=0}^{2} w(k)w(j) E[s(n-k)s(n-3-j)]$$ 同样，仅当 $k = j + 3$ 时，相关函数才不为零，因此：\n$$r_{mm}(3) = w(0)w(3)E[s(n-3)s(n-3)]$$ 由于：\n$$w(k) = 0 , \\text{对于 } k \\neq 0, 1, 2$$ 因此：\n$$w(3) = 0$$ 因此：\n$$r_{mm}(3) = 0$$ 结论\n$$r_{mm}(k) = \\begin{cases} w(0)^2 + w(1)^2 + w(2)^2 \u0026 \\text{当 } k = 0 \\\\ w(0)w(1) + w(1)w(2) \u0026 \\text{当 } k = 1 \\\\ w(0)w(2) \u0026 \\text{当 } k = 2 \\\\ 0 \u0026 \\text{当 } k \u003e 2 \\end{cases}$$ $$r_{mm}(k) = \\begin{cases} 3 \u0026 \\text{当 } k = 0 \\\\ 2 \u0026 \\text{当 } k = 1 \\\\ 1 \u0026 \\text{当 } k = 2 \\\\ 0 \u0026 \\text{当 } k \u003e 2 \\end{cases}$$ 计算 $r_{xx}(k)$ 我们使用已经得到的结果来计算 $r_{xx}(k)$\n我们已知 $x(n) = m(n) + u(n)$，并且噪声 $u(n)$ 被假设为独立于传输符号的。\n$$r_{xx}(k) = E[x(n)x(n-k)]$$ $$r_{xx}(k) = E\\left[(m(n) + u(n))(m(n-k) + u(n-k))\\right]$$ $$r_{xx}(k) = E[m(n)m(n-k)] + E[m(n)u(n-k)] + E[u(n)m(n-k)] + E[u(n)u(n-k)]$$ 由于 $m(n)$ 和 $u(n)$ 是独立的：\n$$E[m(n)u(n-k)] = 0$$ 并且 $u(n)$ 是零均值的白噪声：\n$$E[u(n)u(n-k)] = \\sigma_u^2\\delta(k)$$ 因此：\n$$r_{xx}(k) = r_{mm}(k) + \\sigma_u^2\\delta(k)$$ $$r_{xx}(k) = \\begin{cases} w(0)^2 + w(1)^2 + w(2)^2 + \\sigma_u^2 \u0026 \\text{当 } k = 0 \\\\ w(0)w(1) + w(1)w(2) \u0026 \\text{当 } k = 1 \\\\ w(0)w(2) \u0026 \\text{当 } k = 2 \\\\ 0 \u0026 \\text{当 } k \u003e 2 \\end{cases}$$ $$r_{xx}(k) = \\begin{cases} 3 + \\sigma_u^2 \u0026 \\text{当 } k = 0 \\\\ 2 \u0026 \\text{当 } k = 1 \\\\ 1 \u0026 \\text{当 } k = 2 \\\\ 0 \u0026 \\text{当 } k \u003e 2 \\end{cases}$$ $h_{\\text{opt}}$ 的表达式 最终我们计算 $r_{dx}(k)$ 并最终给出 $h_{\\text{opt}}$ 的表达式。\n$$r_{dx}(k) = E[d(n)x(n-k)]$$ 已知 $d(n) = s(n-2)$ 且 $x(n) = m(n) + u(n)$：\n$$r_{dx}(k) = E[s(n-2)(m(n-k) + u(n-k))]$$ 由于 $u(n)$ 和 $s(n)$ 是独立的且 $u(n)$ 的均值为 $0$：\n$$E[s(n-2)u(n-k)] = 0$$ 因此：\n$$r_{dx}(k) = E[s(n-2)m(n-k)]$$ 由于已知：\n$$m(n) = \\sum_{k=0}^{2} w(k)s(n-k)$$ 因此：\n$$r_{dx}(k) = E[s(n-2)(w(0)s(n-k) + w(1)s(n-k-1) + w(2)s(n-k-2))]$$ 当 $k = 0$ 时：\n$$r_{dx}(0) = w(2)E[s(n-2)s(n-2)] = w(2)$$ 当 $k = 1$ 时：\n$$r_{dx}(1) = w(1)E[s(n-2)s(n-2)] = w(1)$$ 当 $k = 2$ 时：\n$$r_{dx}(2) = w(0)E[s(n-2)s(n-2)] = w(0)$$ 结论 $$r_{dx}(k) = \\begin{cases} w(2) \u0026 \\text{当 } k = 0 \\\\ w(1) \u0026 \\text{当 } k = 1 \\\\ w(0) \u0026 \\text{当 } k = 2 \\\\ 0 \u0026 \\text{当 } k \u003e 2 \\end{cases}$$ 并且：\n$$h_{\\text{opt}} = R_{xx}^{-1}r_{dx}$$ $$h_{\\text{opt}} = \\begin{pmatrix} r_{xx}(0) \u0026 r_{xx}(1) \u0026 r_{xx}(2) \\\\ r_{xx}(1) \u0026 r_{xx}(0) \u0026 r_{xx}(1) \\\\ r_{xx}(2) \u0026 r_{xx}(1) \u0026 r_{xx}(0) \\end{pmatrix}^{-1} \\begin{pmatrix} r_{dx}(0) \\\\ r_{dx}(1) \\\\ r_{dx}(2) \\end{pmatrix}$$ $$h_{\\text{opt}} = \\begin{pmatrix} r_{xx}(0) \u0026 r_{xx}(1) \u0026 r_{xx}(2) \\\\ r_{xx}(1) \u0026 r_{xx}(0) \u0026 r_{xx}(1) \\\\ r_{xx}(2) \u0026 r_{xx}(1) \u0026 r_{xx}(0) \\end{pmatrix}^{-1} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$$ 实现 我们将在 Matlab 中设计一个滤波器，并使用两种方法计算自相关和互相关来构建滤波器：一种方法是基于理论公式，用 $Toeplitz$ 函数构造自相关矩阵；另一种方法是通过 Matlab 的 $xcorr$ 函数以数值方式计算这些相关函数，然后比较两种方法下得到的滤波器结果。\n我们先看第一种实现，使用上述得到的计算公式，并利用 $Toeplitz$ 函数构造自相关矩阵。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 L = 11; % r_xx(k) pour k = 1,2 et 3 % si k \u0026gt; 2 alors r_xx(k) = 0 r_xx1 = w(1)^2 + w(2)^2 + w(3)^2 + sigma_u^2; % r_xx(0) r_xx2 = w(1)*w(2) + w(2)*w(3); % r_xx(1) r_xx3 = w(1)*w(3); % r_xx(2) r_xx = [r_xx1; r_xx2; r_xx3; zeros(8,1)]; % R_xx Toeplitz avec r_xx comme première colonne R_xx = toeplitz(r_xx); % r_dx(k) pour k = 1,2 et 3 % si k \u0026gt; 2 alors r_dx(k) = 0 r_dx1 = w(3); % r_dx(0) r_dx2 = w(2); % r_dx(1) r_dx3 = w(1); % r_dx(2) r_dx = [r_dx1; r_dx2; r_dx3; zeros(8,1)]; % Wiener-Hopf h_opt = inv(R_xx)*r_dx; disp(\u0026#39;Filtre optimal de Wiener h_opt:\u0026#39;); disp(h_opt); 第二种实现，通过 Matlab 的 $xcorr$ 函数以数值方式（不依赖理论公式，直接应用样本数据）计算这些相关函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 hopt_theo = h_opt; % d(n) = s(n-2) d = s(1:end-2); x2 = x(3:end); % autocorr r_xx_num = xcorr(x2); r_xx_num = r_xx_num(N-2:N-2+L-1); % intercorr r_dx_num = xcorr(d, x2); r_dx_num = r_dx_num(N-2:N-2+L-1); R_xx_num = toeplitz(r_xx_num); h_opt_num = inv(R_xx_num)*r_dx_num; figure, stem(hopt_theo), title(\u0026#39;Filtre optimal théorique\u0026#39;, \u0026#39;FontSize\u0026#39;,16), hold on; stem(h_opt_num), title(\u0026#39;Filtre optimal numérique\u0026#39;, \u0026#39;FontSize\u0026#39;, 16) xlabel(\u0026#39;Ordre du signal L\u0026#39;,\u0026#39;FontSize\u0026#39;,14), ylabel(\u0026#39;Amplitude\u0026#39;, \u0026#39;FontSize\u0026#39;,14) set(gca, \u0026#39;FontSize\u0026#39;, 14); legend(\u0026#39;Théorique\u0026#39;,\u0026#39;Numérique\u0026#39;) 我们发现数值滤波器（Numérique）非常接近理论最优滤波器（Théorique）。\n现在，我们应用 LMS 算法来迭代解决信道估计问题，这里假设 $L = 11$。我们创建一个函数 $algoLMS$，该函数接收输入信号 $x(n)$ 和 $d(n)$，脉冲响应长度 $L$，以及步长 $\\mu$，并输出后验误差序列 $e^+(n)$ 和通过 LMS 算法迭代得到的脉冲响应 $h_n$。并通过调整步长 $\\mu$ 来测试算法，并且展示滤波器系数误差的范数 $|h_n - h_{\\text{opt}}|_2$。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 %% Q5 % pas mu = [0.001, 0.005, 0.01, 0.02]; % erreurs et coeff [err1, h1] = algoLMS(x2, d, L, mu(1)); [err2, h2] = algoLMS(x2, d, L, mu(2)); [err3, h3] = algoLMS(x2, d, L, mu(3)); [err4, h4] = algoLMS(x2, d, L, mu(4)); % norme des erreurs des coeff normeh1 = sqrt(sum((h1 - hopt_theo).^2)); normeh2 = sqrt(sum((h2 - hopt_theo).^2)); normeh3 = sqrt(sum((h3 - hopt_theo).^2)); normeh4 = sqrt(sum((h4 - h_opt).^2, 1)); figure; plot(err1); grid on; hold on; plot(err2); plot(err3); plot(err4); xlabel(\u0026#39;Taille du signal N\u0026#39;, \u0026#39;FontSize\u0026#39;, 14); ylabel(\u0026#39;e(n)\u0026#39;, \u0026#39;FontSize\u0026#39;, 14); title(\u0026#39;Erreurs a posteriori e^+(n) pour différentes valeurs de μ\u0026#39;, \u0026#39;Interpreter\u0026#39;, \u0026#39;none\u0026#39;, \u0026#39;FontSize\u0026#39;, 16); legend(\u0026#39;\\mu = 0.001\u0026#39;, \u0026#39;\\mu = 0.005\u0026#39;, \u0026#39;\\mu = 0.01\u0026#39;, \u0026#39;\\mu = 0.02\u0026#39;); figure; plot(normeh1); grid on; hold on; plot(normeh2); plot(normeh3); plot(normeh4); xlabel(\u0026#39;Taille du signal N\u0026#39;, \u0026#39;FontSize\u0026#39;, 14); ylabel(\u0026#39;||h(n)-h_{opt}||^2\u0026#39;, \u0026#39;FontSize\u0026#39;, 14); title(\u0026#39;Norme de l\u0026#39;\u0026#39;erreur à posteriori\u0026#39;, \u0026#39;Interpreter\u0026#39;, \u0026#39;none\u0026#39;, \u0026#39;FontSize\u0026#39;, 16); legend(\u0026#39;\\mu = 0.001\u0026#39;, \u0026#39;\\mu = 0.005\u0026#39;, \u0026#39;\\mu = 0.01\u0026#39;, \u0026#39;\\mu = 0.02\u0026#39;); 子函数:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 function [eplus,h_tout] = algoLMS(x,d,L,mu) N = length(x); h = ones(L,1); eplus = zeros(N,1); h_tout = zeros(L,N); for n=L:N xn = x(n:-1:n-L+1); yn = h\u0026#39;*xn; eplus(n) = d(n) - yn; h = h + mu*xn*(d(n)-xn\u0026#39;*h); h_tout(:,n) = h; end end 我们下面计算理论上使算法收敛的步长 $\\mu$ 的最大值，并和实际值做对比。\n收敛的充分必要条件为：\n$$0 \u003c \\mu \u003c \\frac{2}{\\lambda_{\\text{max}}}$$ 其中， $\\lambda_{\\text{max}}$ 是自相关矩阵 $R_{xx}$ 的最大特征值。\n我们有：\n$$E[h_n] = E[h_{n-1}] - \\mu E[x(n)x(n)^T h_{n-1}] + \\mu E[d(n)x(n)]$$ 在假设 $x(n)$ 和 $h_{n-1}$ 独立的情况下，可以写为：\n$$E[x(n)x(n)^T h_{n-1}] = E[x(n)x(n)^T]E[h_{n-1}]$$ 因此：\n$$E[h_n] = E[h_{n-1}] - \\mu E[x(n)x(n)^T]E[h_{n-1}] + \\mu E[d(n)x(n)]$$ 此外：\n$$\\mu E[d(n)x(n)] = R_{xx} \\cdot h_{\\text{opt}}$$ 因此：\n$$E[h_n] - h_{\\text{opt}} = (I - \\mu R_{xx})(E[h_{n-1}] - h_{\\text{opt}})$$ 假设：\n$$\\delta h_n = E[h_n] - h_{\\text{opt}}$$ 因此：\n$$\\delta h_n = (I - \\mu R_{xx})\\delta h_{n-1}$$ 经过对 $R_{xx}$ 的一系列变换，我们得到：\n$$\\delta h_n = (I - \\mu R_{xx})\\delta h_{n-1}$$ $$\\delta h_n(l) = (1 - \\mu \\lambda_l)^n \\delta h_0(l)$$ $$|\\mu \\lambda_l| \u003c 1 \\iff 0 \u003c \\mu \u003c \\frac{2}{\\lambda_l}$$ 结论：\n$$0 \u003c \\mu \u003c \\frac{2}{\\lambda_{\\text{min}}}$$ 理论最大值为：\n$$\\mu = \\frac{2}{\\lambda_{\\text{max}}} = \\frac{2}{8.6236} = 0.2319$$ 其中 $\\lambda_{\\text{max}}$ 由 Matlab 确定。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 % Calcul des valeurs propres de R_xx valeurs_propres = eig(R_xx); lambda_max = max(valeurs_propres); % Calcul de la valeur maximale théorique du pas mu_theorique_max = 2 / lambda_max; disp(\u0026#39;Valeur maximale théorique du pas :\u0026#39;); disp(mu_theorique_max); 由于理论计算是在以下假设条件下完成的：输入信号是平稳的，信道模型是精确的。而在实践中，真实信号并不总是满足这些假设。因此，应适当降低 $\\mu$ 的值。在我们的案例中，选择 $\\mu = 0.01$。\n我们现在改变滤波器的阶数 $L$ 和噪声水平 $\\sigma^2_u$，以测试算法的鲁棒性。特别是，绘制误差范数 $|h_n - h_{\\text{opt}}|_2$ 随 $\\sigma^2_u$ 的变化曲线。\n","permalink":"http://localhost:1313/zh/posts/signal_cn/%E5%BA%94%E7%94%A8%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95%E8%A7%A3%E5%86%B3%E4%BF%A1%E9%81%93%E5%9D%87%E8%A1%A1%E9%97%AE%E9%A2%98/","summary":"特定情况下构建滤波器","title":"应用随机梯度算法解决信道均衡问题"},{"content":"通过维纳滤波器对脉冲信号进行反卷积 在超声波无损检测、用于地球物理学的地震反射、医学中的相干光学断层扫描、天体物理学这些领域中，很多时候，未知信号是脉冲型的(即信号是由一个或多个短时间的冲击组成)，它们表现为在几乎为零的背景上具有大幅度的脉冲。尤其适用于确定一个介质中两个界面的定位以量化其厚度。\n目的: 通过使用维纳反卷积技术，重建受噪声和测量误差影响的信号。维纳滤波在频域中可以根据信号和噪声的功率谱密度来调整增益，从而优化信号的重建。反卷积和去噪的目标是逆转信号卷积的影响，并尽可能消除噪声。\n由于这种卷积和噪声的影响，观测数据的分辨率很低，相近的脉冲在观测数据中被混淆，或者淹没在强噪声中。因此，输出观测信号y和输入原始信号x差距较大。因此使用反卷积-去噪 方法，可以通过维纳滤波器实现，维纳滤波器频率增益为：具体推导过程见 随机信号(理论)\n$$ \\mathcal{W}(\\nu) = \\frac{\\mathcal{H}^*(\\nu)}{|\\mathcal{H}(\\nu)|^2 + \\frac{S_B(\\nu)}{S_X(\\nu)}} $$\n$S_B$ 代表测量噪声的功率谱密度（DSP）假设其为白噪声 $S_B (ν) = r_B$ $S_X$ 代表待重建信号的DSP，是我们掌握的先验信息\n对于DSP在不同场景下的选择:\n对于一个相对平滑、连续且正相关(当前值与之前的值有很强的正相关性)的信号，我们可以依赖于一个自回归系数接近1的信号的DSP。也就是说，如果信号通常在时间或频率上变化较慢(且正相关)，我们可以假设其自回归系数接近于1 自回归模型（AR模型）是基于当前值由前面的值加上一个随机误差来决定的。如果自回归系数接近1，意味着当前值与前一个或几个值之间的关系非常强，信号变化非常平缓。其数学形式为: $$ X_t = \\phi X_{t-1} + \\epsilon_t $$\n在处理图像时，我们通常关注的是图像的纹理，例如图像中的细节、重复的图案等。功率谱密度（DSP）可以用来反映这些纹理的频率特征，因此研究DSP就可以研究图像纹理细节。 我们不希望惩罚 脉冲信号的任何频率，即不希望限制或增强特定的频率成分，换句话说，希望所有频率成分都被同等对待。因此选择了一个常数DSP(即每个频率的强度都是相同的)，即也是白噪音，其功率谱密度是平坦的。 实现 1. 信号可视化\n加载数据文件 Observations.mat，并查看滤波器在时域和傅里叶域中的特性\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 %%Load data and visualize the filter % Load the data file Observations.mat load(\u0026#39;Observations.mat\u0026#39;); % This loads variables RI (filter) and Donnees (observed signal) % Time domain visualization of the filter figure; subplot(2,1,1); plot(RI); title(\u0026#39;Filter in Time Domain\u0026#39;); xlabel(\u0026#39;Time Samples\u0026#39;); ylabel(\u0026#39;Amplitude\u0026#39;); % Compute the Fourier Transform of the filter H = MyFFT(RI); % Frequency axis setup N = length(RI); freq = (-N/2:N/2-1)/N; % Normalized frequency axis from -0.5 to 0.5 % Frequency domain visualization of the filter magnitude subplot(2,1,2); plot(freq, abs(H)); title(\u0026#39;Filter in Frequency Domain (Magnitude)\u0026#39;); xlabel(\u0026#39;Normalized Frequency\u0026#39;); ylabel(\u0026#39;Magnitude\u0026#39;); ​\t在时间域，滤波器的特性通过其**冲激响应（impulse response）**展现出来。冲激响应是指滤波器对单位冲激信号（Dirac delta 函数）的响应，完全描述了该滤波器的行为。\n在频率域，滤波器的特性通过其**频率响应（H）的幅值图（magnitude plot）**来表示。幅值图表明了滤波器如何处理不同的频率分量，是高通还是低通。这里明显低通\n然后绘制观测信号\n1 2 3 4 5 6 7 %%Visualize the observed signal % Since the original signal x is not provided, we\u0026#39;ll plot only the observed signal figure; plot(Donnees); title(\u0026#39;Observed Signal\u0026#39;); xlabel(\u0026#39;Time Samples\u0026#39;); ylabel(\u0026#39;Amplitude\u0026#39;); ​\t信号经过滤波器处理时，卷积（convolution）使信号变得模糊，也就是我们观测到的信号是模糊的，再加上噪声的干扰，非常不好，使得单脉冲变得不易区分，甚至有可能出现邻近脉冲的重叠问题，这里的观测信号出现了尿分叉的现象，需要使用反卷积来处理，那就要使用维纳滤波器来\n2. 维纳滤波器\n我们先固定一个 λ 的值来 绘制维纳滤波器的增益，然后将该增益曲线与滤波器 H 的特性放在一起看一下。其中，λ 是噪声功率谱密度与信号功率谱密度之间的比率: $$ λ= S_B (ν)/S_X (ν) $$\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 %% Compute and visualize the Wiener filter gain % Fix the value of lambda (lambda = S_B / S_X) lambda = 0.9; % Compute the Wiener filter gain W = conj(H) ./ (abs(H).^2 + lambda); % Plot the Wiener filter gain and overlay with the filter H figure; plot(freq, abs(W), \u0026#39;LineWidth\u0026#39;, 2); hold on; plot(freq, abs(H), \u0026#39;--\u0026#39;, \u0026#39;LineWidth\u0026#39;, 2); title([\u0026#39;Wiener Filter Gain and Filter H (\\lambda = \u0026#39;, num2str(lambda), \u0026#39;)\u0026#39;]); xlabel(\u0026#39;Normalized Frequency\u0026#39;); ylabel(\u0026#39;Magnitude\u0026#39;); legend(\u0026#39;Wiener Filter Gain\u0026#39;, \u0026#39;Filter H\u0026#39;); grid on; ​\t2b. 改变 λ 的值，每次绘制维纳滤波器的增益。推导出 λ 对滤波器特性的影响。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 %%Effect of varying lambda on the Wiener filter % Define a range of lambda values lambda_values = [ 0.01, 0.1, 0.9]; % Plot the Wiener filter gains for different lambda values figure; hold on; for i = 1:length(lambda_values) lambda = lambda_values(i); W = conj(H) ./ (abs(H).^2 + lambda); plot(freq, abs(W), \u0026#39;LineWidth\u0026#39;, 1.5); end title(\u0026#39;Wiener Filter Gains for Different \\lambda Values\u0026#39;); xlabel(\u0026#39;Normalized Frequency\u0026#39;); ylabel(\u0026#39;Magnitude\u0026#39;); legend(\u0026#39;\\lambda=0.01\u0026#39;, \u0026#39;\\lambda=0.1\u0026#39;, \u0026#39;\\lambda=0.9\u0026#39;); grid on; ​\t增益是指滤波器对信号的增强或衰减程度，λ的本质其实就是 信-噪比 ，$\\lambda = \\frac{\\sigma_{\\text{noise}}^2}{\\sigma_{\\text{signal}}^2}$ ，也就是噪声的方差比上信号的方差。因此根据前置公式$ \\mathcal{W}(\\nu) = \\frac{\\mathcal{H}^*(\\nu)}{|\\mathcal{H}(\\nu)|^2 + \\frac{S_B(\\nu)}{S_X(\\nu)}}$ ，如果不加 $\\lambda$ 直接进行逆滤波（即除以H(f)）会导致幅值响应低的频率区域的的噪声被放大，为了抑制噪声被放大，维纳滤波器通过均衡信号和噪声的比例，如果信号功率远高于噪声功率，那么直接让$\\lambda \\to 0$ 都是可以的，维纳滤波器趋于完全恢复信号，但是如果$\\lambda $ 很大，维纳滤波器会更加注重抑制噪声。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 %% Reconstruction of the signal using the Wiener filter % Choose an appropriate lambda value for reconstruction lambda = 0.1; % Recompute the Wiener filter gain with the chosen lambda W = conj(H) ./ (abs(H).^2 + lambda); % Compute the Fourier Transform of the observed signal Y = MyFFT(Donnees); % Apply the Wiener filter in the frequency domain X_hat = Y.*abs(W); % Reconstruct the signal in the time domain x_hat = MyIFFT(X_hat); % Visualize the observed and reconstructed signals figure; subplot(2,1,1); plot(Donnees); title(\u0026#39;Observed Signal\u0026#39;); xlabel(\u0026#39;Time Samples\u0026#39;); ylabel(\u0026#39;Amplitude\u0026#39;); subplot(2,1,2); plot(real(x_hat)); title(\u0026#39;Reconstructed Signal using Wiener Filter\u0026#39;); xlabel(\u0026#39;Time Samples\u0026#39;); ylabel(\u0026#39;Amplitude\u0026#39;); ​\t根据结果可见，维纳滤波器很好的从被模糊或被卷积后的信号中还原原始信号形状，让脉冲特征更加易于辨别 ，但是由于我们的原始信号不可知，所以没办法直接比较真实的原始信号来评估性能，非常遗憾\n","permalink":"http://localhost:1313/zh/posts/signal_cn/%E9%9A%8F%E6%9C%BA%E4%BF%A1%E5%8F%B7tp3/","summary":"使用维纳反卷积技术，重建受噪声和测量误差影响的信号。","title":"随机信号反卷积"},{"content":"所有理论内容均为相关课程记录，课上手写记录，课后转为电子版，并对内容作出了进一步的翻译和解释，因此本人只有劳动成果，所有学术成果均归指导教师所有，每篇文章的最后均有标注指导教师以及对应的个人网站(如果有的话)。\n所有实践内容均为实验课后报告的中文版，理论和代码部分基本都是由我完成的。\n","permalink":"http://localhost:1313/zh/about/","summary":"\u003cp\u003e所有理论内容均为相关课程记录，课上手写记录，课后转为电子版，并对内容作出了进一步的翻译和解释，因此本人只有劳动成果，所有学术成果均归指导教师所有，每篇文章的最后均有标注指导教师以及对应的个人网站(如果有的话)。\u003c/p\u003e\n\u003cp\u003e所有实践内容均为实验课后报告的中文版，理论和代码部分基本都是由我完成的。\u003c/p\u003e","title":""}]