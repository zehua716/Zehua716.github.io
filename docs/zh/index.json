[{"content":"这是关乎概率论的测试\n","permalink":"https://zehua716.github.io/zh/posts/probability_cn/test/","summary":"summary","title":"tst title"},{"content":"VAE 1. 无监督学习概述 1.1 有监督学习 有监督学习，就是数据集有样本有标签，其目标是学习 $x$ 到 $y$ 的映射，相当于告诉正确答案，如果训练中做错了就根据答案调整一下。\n1.2 无监督学习 无监督学习，数据没有类标，那就不是映射了，其目标就是找到隐含在数据里的模式和结构，常见任务有：\n聚类任务 降维: 三维到二维，且同时保留有用信息。最常用的降维就是主成分分析，是一种线性降维方式 特征学习: 也是一种降维，使用了自编码器结构，将一个输入图像通过神经网络映射到一个低维空间，同时注意，保留的主要信息还得能够重构自己，区别在于这是非线性方法 密度估计：这就是我们下面要重点讲的方面——概率密度函数估计 2. 生成模型与概率密度建模 2.1 生成模型 生成模型的定义\n我们给机器一个训练样本集，希望他能产生与训练样本集同分布的新样本。\n比如说有一组训练数据，服从其概率密度分布为 $p_{\\text{data}}(x)$，我们希望找到一个模型，其概率密度分布为 $p_{\\text{model}}(x)$ ，并且和训练样本的概率密度分布相近（即概率密度函数一样），因此可以从这个 (新构造的) 模型中采样数据从而产生新的样本，并且新样本服从 $p_{\\text{model}}(x)$这个概率密度分布。\n换句话说，我们用了一个模型去逼近这个原始真实输入数据分布 $p_{\\text{data}}(x)$ ，但是注意，真实分布 $p_{\\text{data}}(x)$ 是没有办法知道的，因为它太高维了，只能用一个模型去逼近。有了这个模型我们就可以从其中采样了。\n总而言之，生成模型通过学习输入数据的的联合概率密度分布，学习数据的生成过程，来产生新的且合理的数据样本\n生成模型分类\n根据概率密度估计方法的不同分为两类：\n显式的：估计的分布方程是可以写出来的。换句话说，模型可见。 RNN 的概率密度函数可定义、也可解。 VAE 密度函数能定义但是不可解。 隐式的：不要问模型长什么样子，也不要问概率分布是什么样的，反正我能生成样本，你要什么我就给你什么样的新样本。模型不可见。 GAN 完全不需要密度函数。 2.2 概率密度估计建模 机器学习中的不确定性建模\n机器学习，特别是生成模型中的核心理念之一：世界上的一切都是不确定的，任何事实发生的背后都有一个对应的概率分布。机器学习模型（如深度神经网络、生成模型）试图去建模真实世界中的不确定性。换句话说，我们认为数据（无论是图像、文本、音频等）都是由某种潜在的概率分布生成的。在生成模型中，我们希望通过学习这个分布来进行新的数据生成。\n任务目标\n因此这也引出了无监督模型里的一个核心问题，概率密度估计问题。\n初始的输入数据是有限的样本集，我们的目标是利用这些样本来构建一个生成模型，该模型能够生成与原始数据分布相似的无限新数据。这相当于用一个模型来近似原始数据分布。\n因此，我们面临的核心问题是：如何通过有限的样本去近似这个未知的、且可能是高维的原始数据分布。这也是密度估计问题的本质：在已知样本的前提下，估计出样本所属的概率密度函数（PDF）。即 密度估计的任务就是近似原始数据的概率密度分布。\n概率建模定义\n对于给定的输入数据，我们通过构建模型来推断出数据背后的潜在生成机制，并推断其最可能的输出数据。\n基本组成部分\n随机变量： 是一个不确定的变量，离散或连续都可以，它们的值可以通过概率分布来描述。 概率分布： 概率分布定义了随机变量取值的可能性。 对于随机变量，其概率分布是未知的，但是我们可以先入为主 假设其概率分布，例如正态分布、离散分布或指数分布。这种假定的分布就是我们构建的假设空间。 联合分布和条件分布： **联合分布 **表示多个随机变量同时出现的概率。例如，如果有两个变量 $X$ 和 $Y$，那么联合分布 $P(X, Y)$ 描述了 $X$ 和 $Y$ 同时发生的可能性。 条件分布 描述在已知某些变量取值的情况下，其他变量的概率分布。比如，$P(X|Y)$ 表示在已知 $Y$ 的情况下，$X$ 的分布。 监督学习中的概率建模\n在监督学习中，我们希望通过已知的数据样本 $D = {(x_i, y_i)}$ 来学习 $x$ 和 $y$ 之间的条件概率分布 $P(y|x)$。因此在给定新数据 $x$ 时，可以进行预测 $y$ (比如说给定一个猫的图片$x$ ，然后判断是 +1 的概率是多少 0 的概率是多少)。典型的基于概率密度建模的监督学习方法有逻辑回归、朴素贝叶斯和贝叶斯网络。\n非监督学习中的概率建模\n没有标签 y 与数据 x 的对应关系。因此我们需要从 未标注的数据样本 $D = {x_i}$ 中挖掘数据的潜在结构、分布或者规律。因此概率建模的目标是估计 $P(x)$ ，而不再是条件概率。典型的基于概率密度建模的非监督学习方法有 高斯混合模型 (GMM)、 核密度估计 (KDE) 和 变分自动编码器 (VAE)\n概率建模的步骤\n定义问题：找到随机变量，并且确定随机变量之间的相关性 (独立与否)，同时找到可观测的随机变量及不可观测的随机变量。 选择模型：选择一个合适描述这些随机变量分布的概率模型。比如，线性回归、逻辑回归、高斯分布、混合高斯模型等。 推断与学习：根据观测数据 $X$ 来估计模型参数$\\theta$ ，可以通过贝叶斯推断或最大似然估计等方法。 预测与决策：用训练好的模型对新数据进行预测、分类，或生成与数据分布相似的新样本。 贝叶斯推断\n贝叶斯推断是基于概率建模的重要推断方法。贝叶斯推断方法通过先验分布 $P(\\theta)$ 和似然函数 $P(X | \\theta)$ 来更新对参数的相信度： $$ P(\\theta | X) = \\frac{P(X|\\theta)P(\\theta)}{P(X)} $$ 其中，$\\theta$ 是模型的参数，$X$ 是观测数据。贝叶斯推断是一种动态的过程：从“先验假设”出发，结合“观测证据”，更新“后验认知”。\n朴素贝叶斯分类器 (监督学习中的分类问题)\n在朴素贝叶斯模型中，$Y$ 是目标变量，表示分类标签 (对于二分类是 0 或 1；或者离散值集合$ {C_1, C_2, \\dots, C_k} $ 的多分类)。 $X = (x_1, x_2, \\dots, x_n)$ ：输入特征，假设互相自独立。我们希望通过学习条件概率 $P(Y|X)$ 来对样本 X 进行分类。根\n据贝叶斯定理 即条件概率： $$ P(Y | X) = \\frac{P(X | Y) P(Y)}{P(X)} $$ 由于假设特征独立，我们可以将 $P(X|Y)$ 拆解为： $$ P(X | Y) = P(x_1, x_2, \\dots, x_n | Y) = \\prod_{i=1}^{n} P(x_i | Y) $$ 将多维特征联合分布的计算转化为单个特征的条件概率的乘积。现在只需要分别计算每个特征 $x_i$ 的条件概率 $P(x_i|Y)$，简化计算。\n最后选择能够最大后验概率的分类标签 Y $$ \\hat{Y} = \\arg\\max_Y P(Y) \\prod_{i=1}^n P(x_i | Y) $$ 独立性假设 是朴素贝叶斯的核心假设，现实中不可能完全独立。\n变分自编码器（VAE）\nVAE 是生成模型的一个典型例子，它通过概率建模来学习数据的隐含表示和生成过程。VAE 使用变分推断近似后验分布 Q(Z|X) 来计算对数似然的下界（Evidence Lower Bound，ELBO）： $$ \\log P(X) \\geq \\mathbb{E}{Q(Z|X)} \\left[\\log P(X|Z)\\right] - D{\\text{KL}}(Q(Z|X) | P(Z)) $$ ​\t•\t第一项：重构损失（$ \\mathbb{E}_{Q(Z|X)}[\\log P(X|Z)]$）\n表示使用潜在变量 Z 重构原始数据 X 的好坏，衡量生成数据与真实数据的接近程度。\n​\t•\t第二项：KL 散度项 （$ D_{\\text{KL}}(Q(Z|X) | P(Z)$ )\n表示近似后验分布 Q(Z|X) 与先验分布 P(Z) 的差异。通过最小化它，使潜在变量的分布接近设定的先验（通常是标准正态分布）。\n先不做展开，后续会极大篇幅讲解VAE原理及数学背景以及必要的证明。\n到这里\n概率密度建模\n由于后续会大规模讲解潜在变量，因此概率建模会细致到概率密度建模\n比如在VAE中，假设潜在变量 $Z$ 服从某个概率密度函数（例如标准正态分布 $P(Z)$），VAE 需要学习 $P(Z)$ 的条件下观测数据 $X$ 的概率密度，即学习 $P(X|Z)$，这属于概率密度建模的部分\nVAE使用了变分推断，这是一种通过优化来近似复杂后验分布 $P(Z|X)$ 的方法。这个过程实际上是在隐变量 $Z$ 的概率空间中寻找最优的概率密度分布 $Q(Z|X)$ 来逼近真实后验分布。这也属于概率密度建模的部分\n概率密度函数的约束条件\n概率密度建模的任务是找到数据的概率密度函数 $P_{\\theta}(x)$，但是概率密度函数必须满足以下两个基本条件。\n约束条件： $P_{\\theta}(x) \\geq 0, \\forall x \\in \\mathcal{X}$：概率密度函数必须为非负的。 $\\int P_{\\theta}(x) dx = 1, \\forall \\theta \\in \\Theta$：概率密度函数需要归一化，使得其积分为1（确保总概率为1）。 计算对数概率密度的困难\n$\\nabla_{\\theta} \\log P_{\\theta}(x)$ 不容易计算：\n当我们使用生成模型时，往往需要对概率密度函数的对数进行求导（如用于梯度优化），但是计算复杂，尤其当模型复杂时。 计算边缘概率的困难\n$P(x) = \\int P(x, z) dz$ 不容易计算： 边缘概率表示通过积分消去潜在变量 $z$ 的影响来得到 $P(x)$。这一步非常困难，因为在实际应用中，求解这个积分往往没有解析解，特别是当数据维度高、模型复杂时（如深度神经网络）。因此，直接计算 $P(x)$ 是不可行的。 高维数据采样生成的复杂性\n采样生成： 生成模型的重要任务之一是从学到的概率分布中采样生成新的数据点。在某些模型中，直接从分布中采样可能是困难的，尤其是在高维空间或复杂分布的情况下。 Autoregressive Model 不适合应用在图像像素方面。 ","permalink":"https://zehua716.github.io/zh/posts/machinelearning_cn/vae/","summary":"VAE是一种生成式模型，其核心思想就是概率密度估计问题，属于无监督学习。","title":"VAE"},{"content":"Wiener-Hunt 方法：无监督方面 在上一个实践内容中，我们介绍了去卷积问题的困难，即在应用卷积或者低通滤波器后所导致的观测数据缺失高频相关信息的情况。我们使用了 $Wiener$-$Hunt$ 方法：将量化解的误差的二次项和数据相结合，并在损失函数中使用带有正则化系数的二次惩罚标准以量化解的粗糙度。我们得到了相对来说不错的结果。但是这个方法的缺点是它需要调节一个参数，即正则化参数 $\\mu$。我们最开始由经验选择到观察选择，一直到最后循环找到 $\\mu$ 的最优值，使得去卷积后的图像既不过于不规则也不过于平滑。下面的工作重点在于介绍一种自动调节超参数的方法。\n1. 超参数与全后验分布 这个方法基于 $Wiener$-$Hunt$ 解的贝叶斯解释。该解释本身基于关于误差 $e$ 和关于图像 $x$ 的两个高斯概率模型。\n1.1 误差分布 误差被建模为白色、零均值同质高斯向量。白色指的是像白噪音一样，在其频谱特性中，所有频率分量有相同的功率密度，即信号在不同频率上的能量分布是均匀的，在数学层面，它具有零相关性，即不同时间点的误差值是统计独立的（不相关的）。\n对于高斯分布，选择了一个涉及所谓精度参数 $\\gamma_e$（方差的倒数）的替代参数化。根据这种参数化，其表达式可写为：\n$$f(e \\mid \\gamma_e) = (2\\pi)^{-N/2} \\gamma_e^{N/2} \\exp\\left( -\\frac{\\gamma_e \\|e\\|^2}{2} \\right)$$ 根据 $y = Hx + e$，数据 $y$ 和感兴趣信号 $x$ 的似然函数表达式：\n$$f(y \\mid x, \\gamma_e) = (2\\pi)^{-N/2} \\gamma_e^{N/2} \\exp\\left( -\\frac{\\gamma_e \\|y - Hx\\|^2}{2} \\right)$$ 根据所给的似然表达式 $f(y \\mid x, \\gamma_e)$，量化重建物体 $x$ 相对于观测数据 $y$ 的充分性可以通过协对数（log-likelihood）的形式表示。这种表达经常用于概率模型中，特别是最大似然估计（MLE）或贝叶斯推断中，用于评估模型参数的适配程度。\n再补充一下协对数相关内容，就是似然函数取对数，对于上述似然函数表达式，取其对数后为：\n$$\\log f(y \\mid x, \\gamma_e) = \\log\\left( (2\\pi)^{-N/2} \\gamma_e^{N/2} \\exp\\left( -\\frac{\\gamma_e \\|y - Hx\\|^2}{2} \\right) \\right)$$ 利用对数的性质：$\\ln(a \\cdot b) = \\ln a + \\ln b$，将三部分拆分开：\n$$\\log f(y \\mid x, \\gamma_e) = \\log\\left( (2\\pi)^{-N/2} \\right) + \\log\\left( \\gamma_e^{N/2} \\right) + \\log\\left( \\exp\\left( -\\frac{\\gamma_e \\|y - Hx\\|^2}{2} \\right) \\right)$$ 逐项计算：\n$$\\log\\left( (2\\pi)^{-N/2} \\right) = -\\frac{N}{2} \\log(2\\pi)$$ $$\\log\\left( \\gamma_e^{N/2} \\right) = \\frac{N}{2} \\log(\\gamma_e)$$ $$\\log\\left( \\exp\\left( -\\frac{\\gamma_e \\|y - Hx\\|^2}{2} \\right) \\right) = -\\frac{\\gamma_e}{2} \\|y - Hx\\|^2$$ 合并得到：\n$$\\log f(y \\mid x, \\gamma_e) = -\\frac{N}{2} \\log(2\\pi) + \\frac{N}{2} \\log(\\gamma_e) - \\frac{\\gamma_e}{2} \\|y - Hx\\|^2$$ 回到之前的内容，我们使用了协对数来表达数据能否充分重建原信号，我们给出这种拟合程度的量化指标：\n$$\\mathcal{J}_{LS}(x) = \\|y - Hx\\|^2 = -k_y \\log f(y \\mid x, \\gamma_e) + C_y$$ ​\t•\t$|y - Hx|^2$ 是重建信号（模型参数 $x$）与观测数据 $y$ 的误差平方和，称为残差平方和（Residual Sum of Squares, RSS）。\n​\t•\t$-k_y \\log f(y \\mid x, \\gamma_e)$ 是协对数的负加权形式，其中 $k_y \u0026gt; 0$，是一个常数。\n​\t•\t$C_y$ 也是一个常数。\n我们现在给出两个常数 $k_y$ 和 $C_y$ 的对应表达式：\n前面得到：\n$$\\log f(y \\mid x, \\gamma_e) = -\\frac{N}{2} \\log(2\\pi) + \\frac{N}{2} \\log(\\gamma_e) - \\frac{\\gamma_e \\|y - Hx\\|^2}{2}$$ 将上述结果带入 $\\mathcal{J}_{LS}(x) = -k_y \\log f(y \\mid x, \\gamma_e) + C_y$ 得：\n$$\\mathcal{J}_{LS}(x) = -k_y \\left( -\\frac{N}{2} \\log(2\\pi) + \\frac{N}{2} \\log(\\gamma_e) - \\frac{\\gamma_e \\|y - Hx\\|^2}{2} \\right) + C_y$$ $$\\mathcal{J}_{LS}(x) = k_y \\left( \\frac{N}{2} \\log(2\\pi) - \\frac{N}{2} \\log(\\gamma_e) + \\frac{\\gamma_e \\|y - Hx\\|^2}{2} \\right) + C_y$$ 将上述结果和原公式 $\\mathcal{J}_{LS}(x) = |y - Hx|^2$ 做对比得到结果：\n$$k_y = \\frac{2}{\\gamma_e} \\quad\\quad\\quad C_y = -\\frac{N}{\\gamma_e} \\left( \\log(2\\pi) - \\log(\\gamma_e) \\right)$$ 1.2 感兴趣信号的分布 贝叶斯解释要求为感兴趣的信号 $x$ 提供一个概率分布。其模型也是高斯分布，只是这里它不是白色的，也就是说，它的各组成部分之间存在相关性。在接下来的内容中，相关性实际上是通过协方差矩阵 $R$ 来建模的。\n贝叶斯解释的核心思想是为感兴趣的信号 $x$ 提供一个概率分布，而不是一个确定值。这种概率分布反映了我们对 $x$ 的不确定性以及其任何可能的取值。因此我们假设在模型中，$x$ 服从一个高斯分布。\n但是它是一个非白色的高斯分布，也就是它的各组成部分之间存在相关性，协方差矩阵 $R$ 是一个非对角矩阵，其非零非对角元素表示信号的不同分量之间的相关性。后续我们就使用这个协方差矩阵 $R$ 在贝叶斯框架中建模信号的相关性。\n补充一下关于协方差矩阵的相关内容\n协方差矩阵 $R$ 提供了对信号相关性的精确描述。元素 $R_{ij}$ 表示信号第 $i$ 和第 $j$ 个分量之间的协方差：\n$$R_{ij} = \\mathbb{E}[(x_i - \\mu_i)(x_j - \\mu_j)]$$ 根据相关性，$R$ 可能是一个稀疏矩阵或者满矩阵。\n在贝叶斯建模中：\n​\t1.\t信号 $x$ 的 先验 分布 $p(x)$ 使用协方差矩阵 $R$ 的描述公式为：\n$$p(x) = \\frac{1}{(2\\pi)^{N/2} |R|^{1/2}} \\exp\\left( -\\frac{1}{2} x^T R^{-1} x \\right)$$ 其中，$R^{-1}$ 是协方差矩阵的逆，也称为精度矩阵，定义了 $x$ 的相关性强度。\n​\t2.\t最大后验估计（MAP）：\n利用先验分布 $p(x)$ 和观测数据 $y$ 的似然函数 $p(y \\mid x)$，可以通过贝叶斯法则计算 $x$ 的后验概率分布 $p(x \\mid y)$，并基于该分布选择最优解。\n回到之前，我们通过逆矩阵 $R^{-1} = \\gamma_x \\Pi$ 来表示建模信号的相关性，\n其中：\n​\t•\t精度参数 $\\gamma_x$ 控制相关性的强度\n​\t•\t矩阵 $\\Pi$ 决定了相关性的结构\n我们将 $R^{-1} = \\gamma_x \\Pi$ 带入之前的 $f(x \\mid \\gamma_x)$ 公式可得：\n$$f(x \\mid \\gamma_x) = (2\\pi)^{-N/2} \\det[\\Pi]^{1/2} \\gamma_x^{N/2} \\exp\\left( -\\frac{\\gamma_x}{2} x^T \\Pi x \\right)$$ 也就是说：\n$$f(x \\mid \\gamma_x) \\propto \\exp\\left( -\\frac{\\gamma_x}{2} x^T \\Pi x \\right)$$ 量化物体相对于先验信息充分性的项表现为密度的协对数：\n$$\\mathcal{J}_0(x) = -k_x \\log f(x \\mid \\gamma_x) + C_x = x^T \\Pi x$$ 其中：\n​\t•\t$\\mathcal{J}_0(x)$ 是对信号 $x$ 的量化，用来描述 $x$ 相对于先验信息（即对 $x$ 的已知假设或统计特性）是否充分匹配。其形式以密度的协对数（具体是取对数的负数）表示，结合了贝叶斯模型中的先验分布。\n​\t•\t$f(x \\mid \\gamma_x)$ 是 $x$ 的先验概率密度函数，反映了 $x$ 如何符合假设的先验模型，我们之前在假设 $x$ 服从高斯分布的前提下，得到了其表达式（见上面）。\n​\t•\t正则化项 $x^T \\Pi x$ 描述了信号 $x$ 的“复杂度”或“平滑性”，由 $\\Pi$ 决定其结构，精度参数 $\\gamma_x$ 控制正则化的强度，当 $\\gamma_x$ 较大时，正则化约束更强。\n同样，在上述公式中，我们添加了加法常数 $C_x$ 和乘法常数 $k_x$。为了与之前已经计算过的 Wiener-Hunt 方法联系起来，只需选择 $\\Pi = D^T D$。\n现在我们要给出两个常数的对应表达式。\n已知原公式：\n$$f(x \\mid \\gamma_x) = (2\\pi)^{-N/2} \\det(\\Pi)^{1/2} \\gamma_x^{N/2} \\exp\\left( -\\frac{\\gamma_x x^T \\Pi x}{2} \\right)$$ 两边取对数：\n$$\\log f(x \\mid \\gamma_x) = \\log\\left( (2\\pi)^{-N/2} \\det(\\Pi)^{1/2} \\gamma_x^{N/2} \\exp\\left( -\\frac{\\gamma_x x^T \\Pi x}{2} \\right) \\right)$$ $$\\log f(x \\mid \\gamma_x) = -\\frac{N}{2} \\log(2\\pi) + \\frac{1}{2} \\log\\left( \\det(\\Pi) \\right) + \\frac{N}{2} \\log(\\gamma_x) - \\frac{\\gamma_x x^T \\Pi x}{2}$$ 根据公式：\n$$\\mathcal{J}_0(x) = -k_x \\log f(x \\mid \\gamma_x) + C_x$$ 将上述结果带入其中得到：\n$$\\mathcal{J}_0(x) = -k_x \\left( -\\frac{N}{2} \\log(2\\pi) + \\frac{1}{2} \\log\\left( \\det(\\Pi) \\right) + \\frac{N}{2} \\log(\\gamma_x) - \\frac{\\gamma_x x^T \\Pi x}{2} \\right) + C_x$$ $$\\mathcal{J}_0(x) = k_x \\left( \\frac{N}{2} \\log(2\\pi) - \\frac{1}{2} \\log\\left( \\det(\\Pi) \\right) - \\frac{N}{2} \\log(\\gamma_x) + \\frac{\\gamma_x x^T \\Pi x}{2} \\right) + C_x$$ 对比：\n$$\\mathcal{J}_0(x) = x^T \\Pi x$$ 得到：\n$$k_x = \\frac{2}{\\gamma_x} \\quad\\quad\\quad C_x = \\frac{N}{\\gamma_x} \\log(2\\pi) - \\frac{1}{\\gamma_x} \\log\\left( \\det(\\Pi) \\right) - \\frac{N}{\\gamma_x} \\log(\\gamma_x)$$ 但是严格来说，上述解释并不完全正确，因为 $D^T D$。常量图像只不过是对应于特征值为零的特征向量（这对应于零频率）。严格的发展要求引入一个用于零频率下能量的惩罚项（通过一个可以设定为任意小值的参数）。不懂，后面补充\n上面提到的这个 $f(x \\mid \\gamma_x) = (2\\pi)^{-N/2} \\det(\\Pi)^{1/2} \\gamma_x^{N/2} \\exp\\left( -\\frac{\\gamma_x x^T \\Pi x}{2} \\right)$ 先验分布（a priori），因为它使人们能够处理先验信息，从而更倾向于具有更高规则性的图像。对于给定图像的概率越高，则图像越规则。\n其中的 $\\gamma_x$ 精度参数我们非常关注，因为它控制着图像的平滑度，进而影响着概率分布的整体趋势。\n​\t•\t当 $\\gamma_x$ 较大时，指数项中的 $x^T \\Pi x$ 会被放大。\n​\t•\t当 $\\gamma_x$ 较小时，指数项中的 $x^T \\Pi x$ 对总的概率密度的影响较小。待补充\n1.3 后验分布 借助前面定义的两个成分，并使用概率的乘法规则，现在可以构造重构信号 $x$ 和数据 $y$ 的联合密度：\n$$f(x, y \\mid \\gamma_e, \\gamma_x) = f(y \\mid x, \\gamma_e) f(x \\mid \\gamma_x)$$ 将之前得到的结果代入：\n$$f(x, y \\mid \\gamma_e, \\gamma_x) = (2\\pi)^{-N} \\det[\\Pi]^{1/2} \\gamma_x^{N/2} \\gamma_e^{N/2} \\exp\\left( -\\left[ \\gamma_e \\|y - Hx\\|^2 + \\gamma_x x^T \\Pi x \\right] / 2 \\right)$$ 这个表达式由两个精度参数 $\\gamma_e$ 和 $\\gamma_x$ 参数化。可以注意到在指数项内部，我们得到了加权最小二乘准则的表达式：\n$$\\mathcal{J}_{PLS}(x) = \\mathcal{J}_{LS}(x) + \\mu \\mathcal{J}_0(x)$$ $$\\mathcal{J}_{PLS}(x) = \\|y - Hx\\|^2 + \\mu x^T \\Pi x$$ 其中，正则化参数 $\\mu$ 表示为信噪比的倒数 $\\mu = \\gamma_x / \\gamma_e$。正则化参数 $\\mu$ 在 $\\gamma_e$ 和 $\\gamma_x$ 中的作用是？\n待补充\n通过贝叶斯定理可以确定感兴趣信号的后验分布（后验概率分布）：\n$$f(x \\mid y, \\gamma_e, \\gamma_x) = \\frac{f(x, y \\mid \\gamma_e, \\gamma_x)}{f(y \\mid \\gamma_e, \\gamma_x)} \\propto \\exp\\left( -\\gamma_e \\mathcal{J}_{PLS}(x) / 2 \\right)$$ 这就是给定数据（已观测到的）和参数下的感兴趣信号的分布。\n我们希望为感兴趣信号构造的任何估计器都基于上述分布。最常见的估计器是后验分布的均值、中位数或众数（即后验的最大化者）。在当前情况下，当后验分布是高斯分布时，这三者是相等的。众数或后验最大化者（MAP），记为 $\\hat{x} _{MAP}$ 。也就是最小化准则 $\\mathcal{J} _{PLS}(x)$ 的解\n$$\\hat{x}_{MAP} = \\arg \\max_{x} f(x \\mid y, \\gamma_e, \\gamma_x) = \\arg \\min_{x} \\mathcal{J}_{PLS}(x) = \\hat{x}_{PLS}$$ 结论是，最小二乘准则的解 $\\hat{x} _{MAP}$ 就是之前的工作中推导出来的后验分布的众数 $ \\hat{x} _{MAP}$。\n1.4 扩展的后验分布 到目前为止，贝叶斯方法只允许我们对已经存在的超参数值的估计给出概率解释。将之前的框架扩展到包含超参数的估计，需要为两个精度参数 $\\gamma_e$ 和 $\\gamma_x$ 引入一个先验分布。在多种可选方案中，接下来我们将重点关注伽马分布：\n$$f(\\gamma) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\gamma^{\\alpha - 1} \\exp[-\\beta \\gamma] \\cdot 1_{\\mathbb{R}^+}(\\gamma)$$ 它由两个正实数参数 $(\\alpha, \\beta)$ 驱动，具有均值 $\\alpha / \\beta$ 和方差 $\\alpha / \\beta^2$。这种选择的理由如下：\n​\t•\t选择伽马分布作为先验分布确保了条件后验分布也是伽马分布（我们正在讨论共轭先验）。在算法上，这意味着只需要更新分布参数的值（具体见下面）。\n​\t•\t这种选择允许在参数值的信息较少（也称为“非信息先验”）或精确（如名义值或某种不确定性）的情况下进行处理。该工作中特别感兴趣的是“非信息先验”的极限情况，即 $(\\alpha, \\beta) = (0, 0)$。\n此外，关于变量 $\\gamma_e$ 和 $\\gamma_x$ 的组合，它们被建模为独立的。\n从伽马分布：\n$$f(\\gamma) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\gamma^{\\alpha - 1} \\exp[-\\beta \\gamma] \\cdot 1_{\\mathbb{R}^+}(\\gamma)$$ 和部分联合分布：\n$$f(x, y \\mid \\gamma_e, \\gamma_x) = f(y \\mid x, \\gamma_e) f(x \\mid \\gamma_x) = (2\\pi)^{-N} \\det[\\Pi]^{1/2} \\gamma_x^{N/2} \\gamma_e^{N/2} \\exp\\left( -\\left[ \\gamma_e \\|y - Hx\\|^2 + \\gamma_x x^T \\Pi x \\right] / 2 \\right)$$ 的表达式出发，我们推导出对于 $y, x, \\gamma_e$ 和 $\\gamma_x$ 的完整联合分布的表达式为：\n$$f(y, x, \\gamma_e, \\gamma_x) = f(y, x \\mid \\gamma_e, \\gamma_x) f(\\gamma_e) f(\\gamma_x)$$ 其明确表达为：\n$$f(x, y, \\gamma_e, \\gamma_x) = (2\\pi)^{-N} \\det[\\Pi]^{1/2} \\frac{\\beta_e^{\\alpha_e} \\beta_x^{\\alpha_x}}{\\Gamma(\\alpha_e) \\Gamma(\\alpha_x)} \\gamma_e^{\\alpha_e + N/2 - 1} \\gamma_x^{\\alpha_x + N/2 - 1} \\exp\\left( -\\left[ \\gamma_e \\left( \\beta_e + \\|y - Hx\\|^2 / 2 \\right) + \\gamma_x \\left( \\beta_x + x^T \\Pi x / 2 \\right) \\right] \\right)$$ 注意: 这个密度非常重要，因为它允许推导出所有相关的联合、边缘和条件密度。\n现在我们可以通过贝叶斯规则推导出完整的后验分布，即给定观测数据$y$时，感兴趣信号$x$和超参数$\\gamma_e$, $\\gamma_x$的分布：\n$$f(x, \\gamma_e, \\gamma_x | y) = \\frac{f(x, y, \\gamma_e, \\gamma_x)}{f(y)}$$ $$f(x, \\gamma_e, \\gamma_x | y) \\quad \\propto \\quad \\gamma_e^{\\alpha_e + N/2 - 1} \\gamma_x^{\\alpha_x + N/2 - 1} \\exp \\left( - \\left[ \\gamma_e \\left( \\beta_e + \\|y - Hx\\|^2 / 2 \\right) + \\gamma_x \\left( \\beta_x + \\|x\\|_\\Pi^2 / 2 \\right) \\right] \\right)$$ 这汇总了所有关于感兴趣信号和超参数在数据视角下的可用信息：对于三重项 $x$, $\\gamma_e$, $\\gamma_x$，它量化了后验密度，即在给定观测数据下三重项的概率。感兴趣信号和超参数的估计器是从这个分布中构造出来的。我们可以查看后验分布的均值、中位数或众数。每种方法都有其优缺点。在接下来的内容中，我们将重点讨论均值。\n1.5 后验均值 考虑到后验分布（上面这个）的复杂性，获得均值的解析公式是不可行的。为了计算后验均值，有几种方法可用，在这里我们将重点关注随机采样技术。最终，它归结为对后验分布进行随机采样，然后取样本的经验均值，从而近似后验均值。\n后验分布的采样可以通过马尔可夫链蒙特卡洛（MCMC）方法来实现。它要求构建一个迭代过程，以生成随机样本，经过一定的时间（称为 burn-in），这些样本将根据目标分布进行分布。构建这样一个过程并不容易，但在当前情况下，存在一个标准算法可以轻松使用：Gibbs 采样算法。它将对三重项 $x$, $\\gamma_e$, $\\gamma_x$ 的后验分布进行采样的问题，转换为它们三个各自的更简单分布的采样问题。每个分布实际上是条件分布，给定其余参数的条件下对其中一个参数进行采样。该算法的工作原理在下表中给出，接下来的部分我们将详细说明这些步骤。\n$$\\begin{aligned} \u0026\\bullet \\, \\text{Initialize } x^{[0]} = y \\\\ \u0026\\bullet \\, \\text{For } k = 1, 2, \\dots \\, \\text{repeat} \\\\ \u0026\\quad \\text{(a) \\ sample } \\gamma_e^{[k]} \\text{ under } f(\\gamma_e \\mid \\gamma_x^{[k-1]}, x^{[k-1]}, y) \\\\ \u0026\\quad \\text{(b) \\ sample } \\gamma_x^{[k]} \\text{ under } f(\\gamma_x \\mid \\gamma_e^{[k]}, x^{[k-1]}, y) \\\\ \u0026\\quad \\text{(c) \\ sample } x^{[k]} \\text{ under } f(x \\mid \\gamma_e^{[k]}, \\gamma_x^{[k]}, y) \\end{aligned}$$ 1.5.1 采样逆误差功率 采样对应于步骤 (a) 的超参数 $\\gamma_e$ 需要从条件后验分布 $f(\\gamma_e | x, \\gamma_x, y)$ 中采样。该分布由完整的联合分布 $f(x, y, \\gamma_e, \\gamma_x)$ 获得，如下所示：\n$$\\text{posterior distribution }: \\quad f(\\gamma_e | x, \\gamma_x, y) = \\frac{f(x, y, \\gamma_e, \\gamma_x)}{f(x, \\gamma_x, y)}$$ 仅保留包含 $\\gamma_e$ 的项（与 $\\gamma_e$ 相关的部分），并且由于分母不依赖于 $\\gamma_e$，我们得到\n$$f(\\gamma_e | x, \\gamma_x, y) \\propto f(x, y, \\gamma_e, \\gamma_x)$$ $$f(\\gamma_e | x, \\gamma_x, y) \\quad \\propto \\quad \\gamma_e^{\\alpha_e + N/2 - 1} \\exp \\left( - \\gamma_e \\left( \\beta_e + \\| y - Hx \\|^2 / 2 \\right) \\right)$$ 由此获得的条件分布实际上是具有参数 $(\\alpha, \\beta)$ 的伽马分布：\n$$\\alpha = \\alpha_e + N/2 \\quad \\text{and} \\quad \\beta = \\beta_e + \\| y - Hx \\|^2 / 2$$ 在先验参数 $(\\alpha_e, \\beta_e)$ 等于 $(0, 0)$ 的极限情况下，后验的参数为：\n$$\\alpha = N/2 \\quad \\text{and} \\quad \\beta = \\| y - Hx \\|^2 / 2$$ 因此我们关注于这个 $f(\\gamma_e | x, \\gamma_x, y)$ 分布的均值和方差表达式，并将它与输出误差 $y - Hx$ 的功率相关联。\n已知伽马分布的概率密度函数：\n$$f(\\gamma) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\gamma^{\\alpha - 1} \\exp[-\\beta \\gamma] \\cdot 1_{\\mathbb{R}+}(\\gamma)$$ 其中：$\\alpha$ 是形状参数，$\\beta$ 是尺度参数。\n对于伽马分布 $\\text{Gamma}(\\alpha, \\beta)$，其均值和方差的标准表达式分别为：\n均值：\n$$\\mathbb{E}[\\gamma_e] = \\frac{\\alpha}{\\beta}$$ 方差：\n$$\\text{Var}[\\gamma_e] = \\frac{\\alpha}{\\beta^2}$$ 已知：\n$$\\alpha = N/2 \\quad \\text{and} \\quad \\beta = \\| y - Hx \\|^2 / 2$$ 误差精度参数 $\\gamma_e$ 的均值：\n$$\\mathbb{E}[\\gamma_e] = \\frac{\\alpha_e + N/2}{\\beta_e + \\frac{\\| y - Hx \\|^2}{2}}$$ 误差精度参数 $\\gamma_e$ 的方差：\n$$\\text{Var}[\\gamma_e] = \\frac{\\alpha_e + N/2}{\\left( \\beta_e + \\frac{\\| y - Hx \\|^2}{2} \\right)^2}$$ 伽马分布中的 $\\beta$ 参数直接依赖于 $| y - Hx |^2$。\n当误差 $| y - Hx |^2$ 较大时，$\\beta$ 参数也会增大，表示模型拟合较差，这意味着伽马分布的均值和方差会减小，反映了误差增加时对数据的信任度降低。 反之，误差 $| y - Hx |^2$ 较小时，精度参数 $\\gamma_e$ 的均值增大，表示对数据的信任度较高。 因此为了实现步骤 (a)：$\\gamma_e^{[k]}$ 的样本从具有上述两个参数 $\\alpha$ 和 $\\beta$ 的伽马分布中抽取，我们可以使用 Matlab 中的 RNDGamma(Alpha, Beta); 函数，具体代码为：\n1 2 3 4 function SamplePrecision = RNDGamma(Alpha,Beta)\t% The Precision variable is a sample of the gamma distribution with parameters Alpha and Beta % Tirage d\u0026#39;un échantillon Gamma approché par du Gauss (JFG+TBC) SamplePrecision = Alpha/Beta + sqrt( Alpha/(Beta*Beta) ) * randn; 注意：计算参数 $\\beta$ 涉及计算建模误差 $| y - Hx |^2$ 的范数。计算空间域中的范数通常成本较高，但是可以在傅里叶域中进行计算以降低成本。\n1.5.2 采样感兴趣信号的逆功率 现在我们将重点放在采样超参数 $\\gamma_x$ 上，对应于表 1 中算法的步骤 (b)。\n这需要采样条件后验分布 $f(\\gamma_x | x, \\gamma_e, y)$。使用与上一节类似的方法，我们得到：\n$$f(\\gamma_x | x, \\gamma_e, y) \\propto \\gamma_x^{\\alpha_x + N/2 - 1} \\exp \\left( - \\gamma_x \\left( \\beta_x + \\| x \\|_\\Pi^2 / 2 \\right) \\right)$$ 可以看出，这个条件后验分布也是伽马分布。在先验参数 $(\\alpha_x, \\beta_x)$ 等于 $(0, 0)$ 的极限情况下，后验参数为：\n$$\\alpha = N/2 \\quad \\text{and} \\quad \\beta = \\| x \\|_\\Pi^2 / 2$$ 因此我们关注于 $f(\\gamma_x | x, \\gamma_e, y)$ 这个分布的均值和方差表达式，并将它与输出误差 $y - Hx$ 的功率相关联。\n均值：\n$$\\mathbb{E}[\\gamma_x] = \\frac{\\alpha_x}{\\beta_x}$$ 方差：\n$$\\text{Var}[\\gamma_x] = \\frac{\\alpha_x}{\\beta_x^2}$$ $$\\mathbb{E}[\\gamma_x] = \\frac{\\alpha_x + N/2}{\\beta_x + \\frac{\\| x \\|_{\\Pi}^2}{2}}$$ $$\\text{Var}[\\gamma_x] = \\frac{\\alpha_x + N/2}{\\left( \\beta_x + \\frac{\\| x \\|_{\\Pi}^2}{2} \\right)^2}$$ 当图像 $x$ 的二次形式较小时（即图像较为平滑），均值会相应增大。这表明我们更信任较为平滑的图像。\n因此为了实现步骤 (b) $\\gamma_x^{[k]}$ 的样本从具有上述两个参数的伽马分布中抽取，同样使用 RNDGamma 函数。\n注意：计算参数 $\\beta$ 涉及计算建模误差 $| y - Hx |^2$ 的范数。计算空间域中的范数通常成本较高，但是可以在傅里叶域中进行计算以降低成本。\n1.6 采样感兴趣的物体 最后但同样重要的是，我们将处理对应于表 1 中算法步骤 (c) 的感兴趣物体 $x$ 的采样。这意味着采样条件后验分布 $f(x | \\gamma_x, \\gamma_e, y)$，其表达式已经推导出\n$$f(x | \\gamma_e, \\gamma_x, y) = \\frac{f(x, y | \\gamma_e, \\gamma_x)}{f(y | \\gamma_e, \\gamma_x)} \\propto \\exp \\left( - \\gamma_e \\mathcal{J}_{PLS}(x) / 2 \\right)$$ 并且它便捷地重新写为：\n$$f(x | \\gamma_e, \\gamma_x, y) \\propto \\exp \\left( - \\left[ \\gamma_e \\| y - Hx \\|^2 + \\gamma_x \\| x \\|_\\Pi^2 \\right] / 2 \\right) = \\exp \\left( - \\gamma_e \\mathcal{J}_{PLS}(x) / 2 \\right)$$ 该密度本身是高斯分布，因为指数项内的变量 $x$ 是正定的二次项。它由均值和协方差矩阵完全表征。在这种情况下：\n均值 $\\mu_{x|*}$ 也是众数，作为最小化 $\\mathcal{J}_{PLS}(x)$ 的解，即之前实践工作中讨论的 Wiener-Hunt 解。 协方差矩阵 $\\Sigma_{x|*}$ 通过计算 $\\mathcal{J}_{PLS}(x)$ 的 Hessian（即二阶导数矩阵）获得。 我们得到以下表达式：\n$$\\mu_{x|*} = \\gamma_e \\Sigma_{x|*} H^T y$$ $$\\Sigma_{x|*} = \\left( \\gamma_e H^T H + \\gamma_x \\Pi \\right)^{-1}$$ 当前面临的数值问题是对可能具有高维的高斯分布进行采样。这个高维度性阻止了协方差矩阵 $\\Sigma_{x|*}$ 的求逆或分解，这意味着没有简单的采样方案可用。为了解决这个问题，我们采用循环矩阵的近似方法，从而能够在傅里叶域中进行快速的矩阵运算。给出了以下表达式：\n$$\\overset{\\circ}{\\mu}_{x|*} = \\gamma_e \\Lambda_{x|*} \\Lambda_H^{\\dagger} \\overset{\\circ}{y}$$ $$\\Lambda_{x|*} = \\left( \\gamma_e \\Lambda_H^\\dagger \\Lambda_H + \\gamma_x \\Lambda_D^\\dagger \\Lambda_D \\right)^{-1}$$ 在傅里叶域中，协方差矩阵是对角线形式的，这意味着其各个分量是解相关的。因此，每个分量是独立的，这使得可以并行采样。\n接下来我们给出上面两个表达式的推导过程，基于循环矩阵对角化的思想。\n由于矩阵 $H$ 是实数矩阵，因此它也是它的复共轭矩阵，且 $H^t = H^\\dagger$。$D$ 同理。\n因此我们将利用循环矩阵的对角化性质，由于 $H$ 和 $D$ 是循环矩阵，且循环矩阵是具有特殊结构的方阵，矩阵的每一行元素是前一行元素循环右移一位。基于这一性质，循环矩阵的一个重要性质是，它可以通过傅里叶变换对角化。具体来说，任何 $N \\times N$ 的循环矩阵 $C$ 都可以写成：\n$$C = F \\Lambda F^\\dagger$$ $F$ 是离散傅里叶变换矩阵，$F^\\dagger$ 是其共轭转置（逆傅里叶变换）。 $\\Lambda$ 是一个对角矩阵，其元素为矩阵 $C$ 的特征值（傅里叶系数）。 之前我们得到的方程为：\n$$\\mu_{x|*} = \\gamma_e \\Sigma_{x|*} H^T y$$ $$\\Sigma_{x|*} = \\left( \\gamma_e H^T H + \\gamma_x \\Pi \\right)^{-1}$$ 我们的目标是利用循环矩阵的性质将其转换到频域，从而得到新的表达式。先将 $H$ 和 $D$ 对角化：\n$$H = F \\Lambda_H F^\\dagger$$ $$\\Pi = F \\Lambda_\\Pi F^\\dagger$$ 带入原方程：\n$$\\Sigma_{x|*} = \\left( \\gamma_e H^T H + \\gamma_x \\Pi \\right)^{-1}$$ 将 $F$ 和 $F^\\dagger$ 提到外面：\n$$\\Sigma_{x|*} = F \\left( \\gamma_e \\Lambda_H^\\dagger \\Lambda_H + \\gamma_x \\Lambda_\\Pi \\right)^{-1} F^\\dagger$$ 第一个公式证明完毕：\n$$\\Lambda_{x|*} = \\left( \\gamma_e \\Lambda_H^\\dagger \\Lambda_H + \\gamma_x \\Lambda_D^\\dagger \\Lambda_D \\right)^{-1}$$ 回到开始，我们有：\n$$H = F \\Lambda_H F^\\dagger$$ $$\\Pi = F \\Lambda_\\Pi F^\\dagger$$ 把上述公式带入原方程：\n$$\\mu_{x|*} = \\gamma_e \\Sigma_{x|*} H^T y$$ 计算得：\n$$\\mu_{x|*} = \\gamma_e F \\left( \\gamma_e \\Lambda_H^\\dagger \\Lambda_H + \\gamma_x \\Lambda_D^\\dagger \\Lambda_D \\right)^{-1} F^\\dagger F \\Lambda_H^\\dagger \\hat{y}$$ 由于 $F^\\dagger F = I$，可以简化为：\n$$\\mu_{x|*} = \\gamma_e \\Lambda_{x|*} \\Lambda_H^\\dagger \\hat{y}$$ 第二个公式证毕。\n在进行步骤 (c) 时，对图像 $x^{[k]}$ 的样本应从具有在傅里叶域中给定的第一和第二矩中的高斯分布中抽取。可以使用自定义的 Matlab 函数 RNDGauss(Moy, Cov)，Moy 和 Cov 必须在傅里叶域中给出，函数具体内容为：\n1 2 3 4 5 6 7 8 9 10 11 12 13 function SampleImage = RNDGauss(MoyGauss,Covariance)\t% Generate an image sample under a Gaussian distribution, with the mean given by Moy and the covariance given by Cov. % The parameters Moy and Cov, and Image, are all in the Fourier domain, not in the spatial domain. % Paramètre de Taille Taille = length(MoyGauss); % Tirage d\u0026#39;un bruit blanc avec la bonne symétrie BoutGauss = randn(Taille,Taille) + sqrt(-1) * randn(Taille,Taille); BoutGauss = MyFFT2( real( MyIFFT2(BoutGauss) ) ); % Filtrage du bruit blanc SampleImage = MoyGauss + BoutGauss .* sqrt(Covariance); 2 实现 在 Matlab 实践中，我们使用和上次内容相同的数据集，最后结果理论上应该相似。因为我们做出的改进只是自动调整正则化参数。我们之前介绍了算法步骤，并且详细解释了涉及两个超参数 $\\gamma_e$ 和 $\\gamma_x$ 的条件分布采样，以及图像 $x$ 的条件分布采样，所以这里直接给代码。\n","permalink":"https://zehua716.github.io/zh/posts/signal_cn/%E5%8F%8D%E9%97%AE%E9%A2%98-tp2/","summary":"第二次实验内容","title":"反问题 TP2"},{"content":"1. 图像投影模型与3D重建理论 1.1. $SLAM$的概念 $SLAM$（Simultaneous Localization and Mapping，即同步定位与地图构建）。 通过估计每个相机的位置和场景的三维点，实现对场景的重建。 1.2. 逆向二维图像 从二维图像中提取三维场景的信息，即进行$3D$重建\n逆向投影\n图像是三维场景经过投影后的二维表示，要恢复三维信息，需要逆转这个投影过程。 建立一个数学模型，描述三维场景如何投影到二维图像中，然后尝试逆向求解。 1.3 针孔相机模型（Modèle Sténopé） 1.3.1 模型概述 定义：针孔相机模型假设所有的光线都通过一个公共点，即光心（光学中心）。 优点：模型简单，易于逆向计算，在三维重建中广泛使用。 1.3.2 坐标系和符号约定 摄像机坐标系： 原点 $O _C$：光学中心，坐标为 $(0, 0, 0)$。 轴方向：建立右手坐标系，$X _C$ 向右，$Y _C$ 向下，$Z _C$ 指向后方（场景深度方向）。 优势：$Z$ 轴指向后方，物体深度为正，符合直觉。 1.3.3 三维点的投影到归一化焦平面 三维点表示： 点 $U$：坐标为 $(U _X, U _Y, U _Z)$，表示空间中的一个三维点。 归一化焦平面： 一个与光心 $O _C$ 距离为 $1$ 的平面（$Z _C = 1$），称为归一化聚焦平面。 将远处的三维点投影到此平面上。 1.3.4 齐次坐标与非齐次坐标 齐次坐标（Homogeneous Coordinates）：\n定义：在原有坐标后增加一个维度（通常为 $1$），方便表示投影和变换。 表示：对于二维点 $m = (m _X, m _Y)^\\top$，其齐次坐标为 $\\bar{m} = (m _X, m _Y, 1)^\\top$ 作用：齐次坐标方便矩阵运算，尤其是在投影和变换过程中。\n非齐次坐标（Inhomogeneous Coordinates）： 标准的笛卡尔坐标表示法，不包含额外的维度。 1.4 摄像机的线性校准（Calibration） 1.4.1 从归一化焦平面到图像平面 图像平面：\n坐标系：像素坐标系，通常以图像左上角为原点，向右为 $X$ 轴（列索引），向下为 $Y$ 轴。 目的：将归一化焦平面上的点映射到实际的图像像素坐标上。 线性变换：\n变换公式：\n$$\\begin{cases} P _U = f \\cdot m _X + U _0 \\\\ P _V = f \\cdot m _Y + V _0 \\end{cases}$$ 焦距 $f$ $m _X$, $m _Y$ 是归一化焦平面上的点 光学中心在图像平面中的坐标 $(U _0, V _0)$\n1.4.2 摄像机内参矩阵（矩阵 $K$） 将上述线性变换表示为矩阵形式 $$K = \\begin{pmatrix} f \u0026 0 \u0026 U _0 \\\\ 0 \u0026 f \u0026 V _0 \\\\ 0 \u0026 0 \u0026 1 \\end{pmatrix}$$ 矩阵映射关系： $$\\underline{P} = K \\cdot \\underline{m}$$ 其中，$\\underline{P}$ 是图像平面中的点的齐次坐标\n1.4.3 逆向过程 从图像平面到归一化焦平面： $$\\underline{M} = K^{-1} \\cdot \\underline{P}$$ 1.4.4 可视锥（Cone of Visibility） 表示相机能够看到的空间范围。通过将图像的四个角点转换到归一化焦平面，然后连接光学中心，形成视锥。 1.5 畸变建模与校正 1.5.1 相机畸变的来源 实际相机镜头的光学缺陷，尤其在广角镜头中，导致图像出现畸变，直线变曲，图像边缘出现拉伸或压缩。 1.5.2 畸变模型 畸变函数：\n将归一化焦平面上的理想点经过畸变函数（从理想图像到畸变图像），得到畸变后的点，此点为 $2D$ 实际畸变聚焦平面\n$$\\underline{m} _d = d(\\underline{m}, k)$$ 其中，$k$ 是畸变参数\n举个例子: 多项式径向畸变模型\n$$M _d = \\left(1 + k _1 \\|m\\| _2^2 + k _2 \\|m\\| _2^4 + \\dots \\right) m$$ 其中: $|m| _2^2 = m _x^2 + m _y^2$\n1.6 TP1：畸变校正的实现 1.6.1 任务描述 目标：将畸变的实际图像校正为理想的无畸变图像 1.6.2 实现步骤 定义参数: 理想的摄像机内参矩阵 $K _{\\text{ideal}}$ ; 畸变的摄像机内参矩阵 $K _{\\text{real}}$; 失真参数 $k$ 。\n对于每个理想图像的像素坐标，执行以下步骤\n将像素坐标转换到归一化焦平面：\n$$\\underline{m} _{\\text{ideal}} = K _{\\text{ideal}}^{-1} \\cdot \\underline{P} _{\\text{ideal}}$$ 应用畸变函数：\n$$\\underline{m} _d = d(\\underline{m} _{\\text{ideal}}, k)$$ 映射回实际图像坐标系：\n$$\\underline{P} _{\\text{real}} = K _{\\text{real}} \\cdot \\underline{m} _d$$ 插值：\n对 $\\underline{P} _{\\text{real}}$ 进行插值（由于坐标可能为非整数，可能要用双线性插值）\n生成校正后的图像\n2. 二维刚性变换和单应性 2.1 二维刚性变换 二维刚性变换包括平移和旋转\n2.1.1 旋转： $$\\mathbf{U}^c = \\overrightarrow{O _c U}^c \\quad \\mathbf{U}^w = \\overrightarrow{O _w U}^w$$ $$\\mathbf{R} _{wc} \\underline{\\mathbf{U}}^c = \\mathbf{R} _{wc} \\cdot \\overrightarrow{O _c U}^c = \\overrightarrow{O _w U}^w$$ 从一个参考系中选取一个向量然后转换到另一个坐标系中\n$\\mathbf{R} _{wc}$ 是一个正交矩阵\n2.1.2 平移： $$\\mathbf{T} _{wc} = \\overrightarrow{O _w O _c}^{w}$$ 2.1.3 刚性变换公式： $$\\mathbf{U}^w = \\mathbf{R} _{wc} \\cdot \\mathbf{U}^c + \\mathbf{T} _{wc}$$ 证明：\n$$\\mathbf{R} _{wc} \\cdot \\mathbf{U}^c + \\mathbf{T} _{wc} = \\mathbf{R} _{wc} \\cdot \\overrightarrow{O _c U}^c + \\overrightarrow{O _w O _c}^w = \\overrightarrow{O _c U}^w + \\overrightarrow{O _w O _c}^w = \\overrightarrow{O _w U}^w = \\mathbf{U}^w$$ 2.1.4 齐次坐标： $$\\underline{\\mathbf{U}}^w = \\begin{bmatrix} \\mathbf{U}^w \\\\ 1 \\end{bmatrix}$$ $$\\mathbf{M} _{wc} = \\begin{bmatrix} \\mathbf{R} _{wc} \u0026 \\mathbf{T} _{wc} \\\\ 0 \u0026 1 \\end{bmatrix} = \\begin{bmatrix} r _{11} \u0026 r _{12} \u0026 r _{13} \u0026 t _{x} \\\\ r _{21} \u0026 r _{22} \u0026 r _{23} \u0026 t _{y} \\\\ r _{31} \u0026 r _{32} \u0026 r _{33} \u0026 t _{z} \\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\end{bmatrix}$$ 2.1.5 反变换： $$\\mathbf{M} _{cw} = \\mathbf{M} _{wc}^{-1}$$ 2.1.6 变换的组合性： $$\\mathbf{M} _{ab} \\cdot \\mathbf{M} _{bc} = \\mathbf{M} _{ac}$$ 2.2 单应性 2.2.1 平面场景假设 $$\\mathbf{U} _i^A = z _i^A \\cdot \\underline{\\mathbf{m}} _{Ai}$$ 这个方程的意思就是，$ \\mathbf{U} _i ^ A $ 这个点可以由 $ \\underline{\\mathbf{m}} _{Ai}$ 来表示，怎么表示呢？$\\Rightarrow$ 乘它的深度即可（因为 $\\underline{\\mathbf{m}} _{Ai}$ 是单位深度）。\n2.2.2 寻找 $\\underline{\\mathbf{m}} _{Ai}$ 和 $\\underline{\\mathbf{m}} _{Bi}$ 之间的对应关系 光有这个方程，我们怎么找到 $\\underline{\\mathbf{m}} _{Ai}$ 和 $\\underline{\\mathbf{m}} _{Bi}$ 之间的对应关系呢，通俗来讲，怎么进行坐标对应变换呢？\n法线关键公式\n我们需要先回顾一个性质，来得到一个法线和平面间的关键公式\n在参考系 $A$ 中，平面 $P$ 的方程为：$ax + by + cz + d = 0$ 其中 $a, b, c$ 是平面法向量分量，$d$ 是常数项，代表平面 $P$ 和原点 $O _A$ 相对距离\n在向量形式中，平面方程可以化简为：\n$$\\mathbf{n} _A^\\top \\mathbf{U} _i^A + d = 0$$ $\\mathbf{n} _A^\\top$ 代表：向量 $P$ 在参考系 $A$ 中的法向量\n通过这个平面方程的向量形式，我们得到了一个带有法向量的一个很重要的公式。\n利用变量代换得到深度表达式\n将 $\\mathbf{U} _i^A = z _i^A \\cdot \\underline{\\mathbf{m}} _{A,i}$ 带入上式中\n$$\\mathbf{n} _A^\\top \\cdot z _i^A \\cdot \\underline{\\mathbf{m}} _{A,i} + d = 0 \\quad \\Rightarrow \\quad z _i^A = -\\dfrac{d}{\\mathbf{n} _A^\\top \\cdot \\underline{\\mathbf{m}} _{A,i}}$$ 这样，我们就把 $\\underline{\\mathbf{m}} _{Ai}$ 给引进来了，其中 $z _i^A = -\\dfrac{d}{\\mathbf{n} _A^\\top \\cdot \\underline{\\mathbf{m}} _{A,i}}$ 代表了深度。换句话说，我们利用 $\\mathbf{U} _i^A$ 的两个方程，将 $\\mathbf{U} _i^A$ 替换掉了，这样就得到 $z _i^A$ 深度，可是仍然解决不了问题 $\\Rightarrow$ 也就是说光有关于 $\\underline{\\mathbf{m}} _{Ai}$ 的方程是不够的，还需要从 $\\underline{\\mathbf{m}} _{Bi}$ 入手\n接下来我们找 $B$ 坐标系下的点 $\\underline{\\mathbf{m}} _{Bi}$\n我们从刚性变换公式入手 $\\mathbf{U}^w = \\mathbf{R} _{wc} \\cdot \\mathbf{U}^c + \\mathbf{T} _{wc}$ 可见从 $c$ 投影到 $w$ 只需要对 $\\mathbf{U}^c$ 进行变换即可，也就是说，为了得到 $\\underline{\\mathbf{m}} _{Bi}$ 只需要对 $\\underline{\\mathbf{m}} _{Ai}$ 进行刚性变换即可\n$$\\underline{\\mathbf{m}} _{B,i} = \\Pi \\left( \\mathbf{R} _{BA} \\mathbf{U} _i^A + \\mathbf{t} _{BA} \\right)$$ 其中 $\\Pi(\\cdot)$ 是投影函数\n$$\\underline{\\mathbf{m}} _{B,i}= \\Pi \\left( \\mathbf{R} _{BA} \\left( -\\dfrac{d}{\\mathbf{n} _A^\\top \\cdot \\underline{\\mathbf{m}} _{A,i}} \\right) \\cdot \\underline{\\mathbf{m}} _{A,i} + \\mathbf{t} _{BA} \\right)$$ 将上公式左右两边都乘 $-\\dfrac{\\mathbf{n} _A^\\top \\cdot \\underline{\\mathbf{m}} _{A,i}}{d}$\n$$\\underline{\\mathbf{m}} _{B,i}= \\Pi \\left( \\mathbf{R} _{BA} \\cdot \\underline{\\mathbf{m}} _{A,i} - \\dfrac{\\mathbf{n} _A^\\top \\cdot \\underline{\\mathbf{m}} _{A,i}}{d} \\cdot \\mathbf{t} _{BA} \\right)$$ $$\\underline{\\mathbf{m}} _{B,i} = \\Pi \\left( \\left( \\mathbf{R} _{BA} - \\dfrac{\\mathbf{t} _{BA} \\cdot \\mathbf{n} _A^\\top}{d} \\right) \\cdot \\underline{\\mathbf{m}} _{A,i} \\right)$$ 也就得到了各自归一化平面上 $A$ 点到 $B$ 点的对应关系\n问题：上述公式中左右两边都乘了 $-\\dfrac{\\mathbf{n} _A^\\top \\cdot \\underline{\\mathbf{m}} _{A,i}}{d}$，为什么保持不变？\n投影函数 $\\Pi(\\cdot)$ 的特点是它是一个比例不变的操作（即只看方向和相对位置，不看绝对尺度）。因此，即使我们在右边乘上 $-\\dfrac{\\mathbf{n} _A^\\top \\cdot \\underline{\\mathbf{m}} _{A,i}}{d}$，也不会影响等式成立的条件，因为投影结果相同\n2.2.3 寻找 $\\underline{\\mathbf{P}} _{A,i}$ 和 $\\underline{\\mathbf{P}} _{B,i}$ 之间的对应关系 我们已知：\n$$\\left\\{ \\begin{aligned} \\underline{\\mathbf{m}} _{A,i} = K _A^{-1} \\cdot \\underline{\\mathbf{P}} _{A,i}\\\\ \\underline{\\mathbf{m}} _{B,i} = K _B^{-1} \\cdot \\underline{\\mathbf{P}} _{B,i} \\end{aligned} \\right.$$ $\\underline{\\mathbf{P}} _{B,i} = K _B \\cdot \\underline{\\mathbf{m}} _{B,i} \\Rightarrow$ 将上面得到的 $\\underline{\\mathbf{m}} _{B,i}$ 带入\n$$\\underline{\\mathbf{P}} _{B,i} = K _B \\cdot \\Pi \\left( \\left( \\mathbf{R} _{BA} - \\dfrac{\\mathbf{t} _{BA} \\cdot \\mathbf{n} _A^\\top}{d} \\right) \\cdot \\underline{\\mathbf{m}} _{A,i} \\right)$$ $$\\underline{\\mathbf{P}} _{B,i} = K _B \\cdot \\Pi \\left( \\left( \\mathbf{R} _{BA} - \\dfrac{\\mathbf{t} _{BA} \\cdot \\mathbf{n} _A^\\top}{d} \\right) \\cdot K _A^{-1} \\cdot \\underline{\\mathbf{P}} _{A,i} \\right)$$ 回顾性质：\n$$K \\cdot \\Pi \\left( \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} \\right) = \\Pi \\left( K \\cdot \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} \\right)$$ 利用此性质，可得：\n$$\\underline{\\mathbf{P}} _{B,i} = \\Pi \\left( K _B \\cdot \\left( \\mathbf{R} _{BA} - \\dfrac{\\mathbf{t} _{BA} \\cdot \\mathbf{n} _A^\\top}{d} \\right) \\cdot K _A^{-1} \\cdot \\underline{\\mathbf{P}} _{A,i}\\right)$$ 2.2.4 得到单应性矩阵 $\\mathbf{H} _{AB}$ 假设 $\\mathbf{H} _{AB} = K _B \\cdot \\left( \\mathbf{R} _{BA} - \\dfrac{\\mathbf{t} _{BA} \\cdot \\mathbf{n} _A^\\top}{d} \\right) \\cdot K _A^{-1}$\n因此：\n$$\\left\\{ \\begin{aligned} \u0026\\underline{\\mathbf{P}} _{B,i} = \\Pi \\left( \\mathbf{H} _{BA} \\cdot \\underline{\\mathbf{P}} _{A,i} \\right) \\quad \\quad A \\Rightarrow B\\\\ \u0026\\underline{\\mathbf{P}} _{A,i} = \\Pi \\left( \\mathbf{H} _{BA}^{-1} \\cdot \\underline{\\mathbf{P}} _{B,i} \\right) = \\Pi \\left( \\mathbf{H} _{AB} \\cdot \\underline{\\mathbf{P}} _{B,i} \\right) \\quad \\quad B \\Rightarrow A \\\\ \\end{aligned} \\right.$$ 通过单应性矩阵我们可以将某点从一个相机图片坐标系变换到另一个相机图片坐标系，也就是点映射关系\n2.2.5 单应性矩阵估计求解 $$\\mathbf{H} _{AB} = \\begin{bmatrix} h _1 \u0026 h _4 \u0026 h _7 \\\\ h _2 \u0026 h _5 \u0026 h _8 \\\\ h _3 \u0026 h _6 \u0026 h _9 \\end{bmatrix}$$ 这是一个齐次矩阵，它有 $9$ 个参数 $h _1$ 到 $h _9$，齐次矩阵在尺度上具有冗余性，所以会导致自由度的丢失\n简单的解法\u0026ndash;参数化（要估计的参数 = 自由参数） $$\\mathbf{H} _{AB} = \\begin{bmatrix} h _1 \u0026 h _4 \u0026 h _7 \\\\ h _2 \u0026 h _5 \u0026 h _8 \\\\ h _3 \u0026 h _6 \u0026 1 \\end{bmatrix}$$ $$\\mathbf{h} = \\begin{bmatrix} h _1 \\\\ \\vdots \\\\ h _8 \\end{bmatrix}$$ 如何估计 $\\mathbf{h}$？\n在这种情况下只要我们了解一个对应点就可以求得 $h _1$ 到 $h _8$ $$\\underline{\\mathbf{P}} _{A,i} = \\Pi \\left( \\begin{bmatrix} h _1 \u0026 h _4 \u0026 h _7 \\\\ h _2 \u0026 h _5 \u0026 h _8 \\\\ h _3 \u0026 h _6 \u0026 1 \\end{bmatrix} \\cdot \\underline{\\mathbf{P}} _{B,i} \\right)$$ 由于 $\\underline{\\mathbf{P}} _{A,i}$，$\\underline{\\mathbf{P}} _{B,i}$ 是齐次坐标，我们将其展开：\n$$\\begin{bmatrix} P _{A,i,x} \\\\ P _{A,i,y} \\\\ 1 \\end{bmatrix} = \\Pi \\left( \\begin{bmatrix} h _1 \u0026 h _4 \u0026 h _7 \\\\ h _2 \u0026 h _5 \u0026 h _8 \\\\ h _3 \u0026 h _6 \u0026 1 \\end{bmatrix} \\cdot \\begin{bmatrix} P _{B,i,x} \\\\ P _{B,i,y} \\\\ 1 \\end{bmatrix} \\right)$$ $$\\left\\{ \\begin{aligned} P _{A,i,x} = \\dfrac{h _1 \\cdot P _{B,i,x} + h _4 \\cdot P _{B,i,y} + h _7}{h _3 \\cdot P _{B,i,x} + h _6 \\cdot P _{B,i,y} + 1} \\\\ P _{A,i,y} = \\dfrac{h _2 \\cdot P _{B,i,x} + h _5 \\cdot P _{B,i,y} + h _8}{h _3 \\cdot P _{B,i,x} + h _6 \\cdot P _{B,i,y} + 1} \\end{aligned} \\right.$$ $$\\left\\{ \\begin{aligned} P _{A,i,x} \\cdot \\left( h _3 \\cdot P _{B,i,x} + h _6 \\cdot P _{B,i,y} + 1 \\right) = h _1 \\cdot P _{B,i,x} + h _4 \\cdot P _{B,i,y} + h _7 \\\\ P _{A,i,y} \\cdot \\left( h _3 \\cdot P _{B,i,x} + h _6 \\cdot P _{B,i,y} + 1 \\right) = h _2 \\cdot P _{B,i,x} + h _5 \\cdot P _{B,i,y} + h _8 \\end{aligned} \\right.$$ $$\\begin{bmatrix} P _{B,i,x} \u0026 0 \u0026 -P _{A,i,x} \\cdot P _{B,i,x} \u0026 P _{B,i,y} \u0026 0 \u0026 -P _{A,i,x} \\cdot P _{B,i,y} \u0026 1 \u0026 0 \\\\ 0 \u0026 P _{B,i,x} \u0026 -P _{A,i,y} \\cdot P _{B,i,x} \u0026 0 \u0026 P _{B,i,y} \u0026 -P _{A,i,y} \\cdot P _{B,i,y} \u0026 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} h _1 \\\\ h _2 \\\\ h _3 \\\\ h _4 \\\\ h _5 \\\\ h _6 \\\\ h _7 \\\\ h _8 \\end{bmatrix} = \\begin{bmatrix} P _{A,i,x} \\\\ P _{A,i _y} \\end{bmatrix}$$ 因为有 $8$ 个未知数，需要八个独立的线性方程，而每对对应点可以提供两个对应方程（即方程 59），因此需要至少四对对应点。即需要四个匹配 $\\left( P _{A,i}, P _{B,i} \\right) \\quad i = 1, 2, 3, 4$。\n$\\Rightarrow \\mathbf{h}^* = \\arg\\min _{\\mathbf{h}} \\sum _{i=1}^{4} \\left\\lVert M _i \\mathbf{h} - P _{A,i} \\right\\rVert _2^2 \\Rightarrow$ 线性最小二乘法\n3. 使用 $RANSAC$ 算法进行稳健的单应性估计 3.1 目标 图像对齐与拼接：通过估计两幅图像之间的单应性（Homography），实现图像的自动拼接。 3.2 自动建立对应关系\u0026mdash;$SIFT$ 算法 兴趣点检测： 使用 $SIFT$ 等算法在两幅图像中检测特征点（这段代码由老师提供），无需手动标记对应点，利用算法自动建立图像间的对应关系。 因此我们可以找到两幅图像中最相似的点对，但注意，点对并不一定正确对应。 也就是可能会出现错误匹配（离群点），这种情况下不可以直接用对应关系，我们将使用另一种算法叫做 $RANSAC$ 来自动评估对应点之间的正确性，并得到最理想的 $H$ 矩阵并输出。 3.3 $RANSAC$ 算法进行稳健估计 算法思想：\n随机抽样一致性（Random Sample Consensus） 是一种在存在离群点（错误点）的情况下估计模型参数（$H$）的稳健算法。 通过反复随机抽样，寻找最符合的模型。 $RANSAC$ 流程：\n重复 $N$ 次\n（迭代次数根据经验或计算确定）：\n随机选取 4 对匹配点：\n4 是估计单应性矩阵所需的最小匹配点数。 从所有的匹配点中随机选四个，不确定哪个对应关系正确，所以后续中有一个估计评判标准（欧几里得距离）。 估计单应性矩阵 $H^k$：\n使用选取的 4 对匹配点，通过 $DLT$ 算法（上个实验做过，其目的与作用是，在已知对应点的情况下，将一个相机视角转换到另一个相机视角）估计单应性矩阵。 计算误差并评估模型：\n对于所有匹配点（包括未选取的），将第二幅图像的点 $P _{B _i}$ 通过估计的 $H^k$ 转换，即 $H^k P _{B _i}$。\n计算变换后的点（估计点）与第一幅图像实际点 $P _{A _i}$ 之间的欧氏距离。\n定义代价函数\n：使用二值核（要么为 0 要么为 1）函数 $\\phi _c(d)$：\n当距离 $d \u0026lt; \\tau$ 时，认为匹配正确，代价为 0。 当距离 $d \\geq \\tau$ 时，认为匹配错误，代价为 1。 总代价 $L^k = \\sum _{i} \\phi _c(|P _{A _i} - H^k P _{B _i}|)$\n更新最佳模型：\n如果当前代价 $L^k$ 小于之前的最小代价 $L$，则更新 $L$ 和对应的 $H$。 最终输出：\n具有最小代价的单应性矩阵 $H$。 阈值 $\\tau$ 的选择：\n$\\tau$ 是判断匹配是否为内点的距离阈值，通常根据图像分辨率和匹配精度选择，一般在 $0.5$ 到 $3$ 个像素之间。 选择过大会增加错误匹配，过小会忽略正确匹配。 3.4 为什么不用传统的二次代价函数 敏感性问题： 二次代价函数（如最小二乘法）对离群点非常敏感，如果某个点的误差很大，会导致代价函数值过大，这时即使其他点的误差很小也没有用。 稳健性： 二值核函数对那些特别大、离谱的点不敏感（都等于 1），能够有效抑制离群点的影响，使得估计结果更稳健。 其他核函数： 除了二值核函数，还存在其他稳健核函数，如 $Huber$ 核、$Lorentzian$ 核等，可以在一定程度上兼顾误差大小和稳健性。 3.5 $RANSAC$ 算法的局限性 参数数量影响： 当模型参数数量增加时，所需的随机采样次数会指数增长，计算成本显著提高。 适用范围： $RANSAC$ 适用于参数数量较少的情况，如直线拟合、基础矩阵和单应性估计等。 4. 立体视觉中的对极几何 $Géométrie$ épipolaire 到目前为止，我们已经研究了平面场景的情况，使用了单应性（Homography）来描述两个视图之间的关系。然而，对于一般的三维场景，平面假设不再成立。为此，我们引入了对极几何（Epipolar Geometry）。\n4.1 对极几何（Epipolar Geometry） 对极几何可以通过一个示意图很好地解释：\n考虑两个相机，分别位于参考系 1 和参考系 2\n相机 1 的光心为 $O _1$，相机 2 的光心为 $O _2$，空间中的一点 $U$ 投影到两个相机的图像平面上，得到点 $\\underline{m} _1$ 和 $\\underline{m} _2$。\n问题描述：\n在一般情况下，我们无法对点 $U$ 做出任何假设（与之前的平面场景不同）。 我们需要找到一种方法，在不知道 $U$ 的情况下，建立 $\\underline{m} _1$ 和 $\\underline{m} _2$ 之间的关系。 4.2 对极平面和对极线 对极平面（Epipolar Plane）\n$U$ 和光心 $O _1$、$O _2$ 定义了一个平面 $\\Rightarrow$ 点 $m _1$、$m _2$、$O _1$、$O _2$ 共面 $\\Rightarrow$ 称为对极平面。\n$\\text{Contrainte épipolaire} = \\text{coplanarité}$，即 $\\underline{m} _1$、$\\underline{m} _2$、$O _1$、$O _2$ 共面。\n在立体视觉中，基础矩阵 $F$ 和本质矩阵 $E$ 都依赖于共面性条件来计算。\n对极线（Epipolar Lines）\n对极平面与两个相机的图像平面相交，分别得到对极线 $l _1$ 和 $l _2$。\n$m _2$ 是三维点 $U$ 在第二个图像平面的投影，但根据对极几何的约束，$m _2$ 必须位于对极线 $l _2$ 上。\n$\\Rightarrow$ 给定点 $m _1$ 的位置，可以通过基础矩阵 $F$ 确定对应的对极线 $l _2$：$l _2 = F \\cdot m _1$\n基础矩阵 $F$ 捕捉了两个相机之间的相对姿态和内在参数信息。这个公式表明，给定点 $m _1$，可以计算出 $m _2$ 必须位于的对极线 $l _2$。\n4.3 对极约束（Epipolar Constraint） 目标：利用上述几何关系，形式化对极约束，建立 $m_1$ 和 $m_2$ 之间的数学关系。\n定义向量：\n$$\\left\\{ \\begin{aligned} \u0026\\mathbf{\\underline{m}_1} \\text{ 是从光心 } O_1 \\text{ 到图像点 } \\underline{m}_1 \\text{ 的向量} \\quad \\overrightarrow{O_1 m_1}^{1}\\\\ \u0026\\mathbf{\\underline{m}_2} \\text{ 是从光心 } O_2 \\text{ 到图像点 } \\underline{m}_2 \\text{ 的向量} \\quad \\overrightarrow{O_2 m_2}^{2}\\\\ \u0026\\mathbf{t_{12}} = \\overrightarrow{O_1 O_2}^{1} \\text{ 是两个相机光心之间的平移向量} \\end{aligned} \\right.$$ 定义对极平面的法向量：\n$$\\left\\{ \\begin{aligned} \u0026\\text{在参考系 } 1 \\text{ 中，} \\overrightarrow{\\mathbf{n}_1}^{1} = \\underline{\\mathbf{m}}_1 \\times \\mathbf{t}_{12} \\\\ \u0026\\text{在参考系 } 2 \\text{ 中，} \\overrightarrow{\\mathbf{n}_2}^{2} = \\mathbf{R}_{21} \\overrightarrow{\\mathbf{n}_1}^{1}，\\text{ 其中 } \\mathbf{R} \\text{ 是相机间的旋转矩阵} \\end{aligned} \\right.$$ 注意：\n其中 $\\times$ 表示两个向量之间的叉积运算。叉积的结果是一个向量，它垂直于运算的两个向量，方向由右手定则决定，大小为这两个向量构成的平行四边形的面积。 法向量的坐标系变换不用考虑平移部分，因为单位法向量并不是坐标位置，方向向量在旋转过程中大小不变，不受平移的影响。总而言之，法向量只考虑旋转矩阵，而点则需要考虑旋转加平移。 $$\\overrightarrow{\\mathbf{n}_2}^{2} = \\mathbf{R}_{21} \\cdot \\overrightarrow{\\mathbf{n}_1}^{1} = \\mathbf{R}_{21} \\cdot \\left( \\underline{\\mathbf{m}}_1 \\times \\mathbf{t}_{12} \\right) = \\mathbf{R}_{21} \\cdot \\underline{\\mathbf{m}}_1 \\times \\mathbf{R}_{21} \\cdot \\mathbf{t}_{12}$$ 由于之前我们已知 $\\mathbf{t} _{21} = \\mathbf{R} _{21} \\cdot \\mathbf{t} _{12}$，所以上式变为\n$$\\overrightarrow{\\mathbf{n}_2}^{2} = \\mathbf{t}_{21} \\times \\left( \\mathbf{R}_{21} \\cdot \\underline{\\mathbf{m}}_1 \\right)$$ 回顾叉积运算性质：\n$$\\mathbf{a} \\times \\mathbf{b} = \\begin{bmatrix} a_x \\\\ a_y \\\\ a_z \\end{bmatrix} \\times \\begin{bmatrix} b_x \\\\ b_y \\\\ b_z \\end{bmatrix} = \\begin{bmatrix} a_y b_z - a_z b_y \\\\ a_z b_x - a_x b_z \\\\ a_x b_y - a_y b_x \\end{bmatrix}_{3 \\times 1} \\Rightarrow \\left[\\mathbf{a}\\right]_{\\times} = \\begin{bmatrix} 0 \u0026 -a_z \u0026 a_y \\\\ a_z \u0026 0 \u0026 -a_x \\\\ -a_y \u0026 a_x \u0026 0 \\end{bmatrix}$$ $$\\mathbf{a} \\times \\mathbf{b} = \\left[\\mathbf{a}\\right]_{\\times} \\mathbf{b} = \\begin{bmatrix} 0 \u0026 -a_z \u0026 a_y \\\\ a_z \u0026 0 \u0026 -a_x \\\\ -a_y \u0026 a_x \u0026 0 \\end{bmatrix} \\begin{bmatrix} b_x \\\\ b_y \\\\ b_z \\end{bmatrix}$$ 利用上述性质，我们可以看出，叉积运算可以变成矩阵运算，因此我们利用以上性质得到：\n$$\\overrightarrow{\\mathbf{n}_2}^{2} = \\mathbf{t}_{21} \\times \\left( \\mathbf{R}_{21} \\cdot \\underline{\\mathbf{m}}_1 \\right) = \\left[ \\mathbf{t}_{21} \\right]_{\\times} \\cdot \\mathbf{R}_{21} \\cdot \\underline{\\mathbf{m}}_1$$ 因为 $\\overrightarrow{\\mathbf{n}_2}^{2}$ 是 $\\mathbf{m}_2$ 的法线 $\\Rightarrow \\mathbf{m}_2^\\top \\cdot \\overrightarrow{\\mathbf{n}_2}^{2} = 0$\n$$\\mathbf{m}_2^\\top \\cdot \\left[ \\mathbf{t}_{21} \\right]_{\\times} \\cdot \\mathbf{R}_{21} \\cdot \\underline{\\mathbf{m}}_1 = 0$$ $$\\mathbf{m}_2^\\top \\cdot \\left( \\left[ \\mathbf{t}_{21} \\right]_{\\times} \\cdot \\mathbf{R}_{21} \\right) \\cdot \\underline{\\mathbf{m}}_1 = 0$$ 4.4 本质矩阵（Essential Matrix） 公式\n假设 $\\mathbf{E} _{21} = \\left[ \\mathbf{t} _{21} \\right] _{\\times} \\cdot \\mathbf{R} _{21} \\quad \\Rightarrow \\quad \\text{matrice essentielle}$\n它包含了两个相机之间的相对旋转 $\\mathbf{R}$ 和平移 $\\mathbf{t}$ 的信息。\n原式 $ = \\underline{\\mathbf{m}} _2^\\top \\cdot \\mathbf{E} _{21} \\cdot \\underline{\\mathbf{m}} _1 = 0$\n自由度\n$$ 5 \\text{ degre de liberte} \\\\ \\downarrow\\\\ 5 \\text{ DDL} \\left( \\begin{array}{c} 3 , \\mathbf{R} _{21} \\quad \\text{rotation} \\\\ \\quad 2 , \\mathbf{t} _{21} \\quad \\text{translation} \\end{array} \\right)\\\\ \\downarrow\\\\ \\quad \\quad | \\mathbf{t} _{21} |_2 \\quad \\text{ inconnue} $$ 自由度：\n$$\\left\\{ \\begin{aligned} \u0026\\text{旋转矩阵 } \\mathbf{R} \\text{ 有 3 个自由度} \\\\ \u0026\\text{平移向量 } \\mathbf{t} \\text{ 的方向有 2 个自由度（因为尺度未知）} \\\\ \u0026\\text{因此，} \\mathbf{E} \\text{ 有 5 个自由度} \\end{aligned} \\right.$$ 自由度（degree of freedom, DoF）是指描述本质矩阵所需的独立参数数量。在几何和线性代数中，自由度反映了系统在不受限制的情况下可以独立变化的方向或方式。 旋转矩阵具有 3 个自由度，描述了三维空间中的旋转。 平移向量理论上在三维空间中有 3 个自由度。但是本质矩阵中的平移向量一般只关注方向，对长度忽略（未知），所以平移向量只剩下 2 个有效的自由度，描述了平移的方向。 4.5 基础矩阵（Fundamental Matrix） 对上述公式继续变换：\n$$\\underline{\\mathbf{m}}_2^\\top \\cdot \\mathbf{E}_{21} \\cdot \\underline{\\mathbf{m}}_1 = 0$$ 已知：\n$$\\left\\{ \\begin{aligned} \\underline{\\mathbf{m}}_2 = K^{-1} \\cdot \\underline{\\mathbf{P}}_2 \\\\ \\underline{\\mathbf{m}}_1 = K^{-1} \\cdot \\underline{\\mathbf{P}}_1 \\end{aligned} \\right.$$ $$\\underline{\\mathbf{P}}_2^\\top \\cdot (K^{-1})^\\top \\cdot \\mathbf{E}_{21} \\cdot K^{-1} \\cdot \\underline{\\mathbf{P}}_1 = 0$$ 当相机内参未知或未被考虑时，我们引入一个基础矩阵 $\\mathbf{F}$ 来覆盖 $K$。\n假设 $\\mathbf{F} _{21}= (K^{-1})^\\top \\cdot \\mathbf{E} _{21} \\cdot K^{-1}$\n$$\\mathbf{F}_{21} : \\text{ matrice fondamentale} \\quad \\Rightarrow \\quad 7 \\text{ DDL}\\quad \\left\\{ \\begin{aligned} \u0026 \\text{- matrice homogène} \\\\ \u0026 \\text{- rang}(\\mathbf{F}_{21}) = 2 \\quad \\Rightarrow \\quad \\det(\\mathbf{F}_{21}) = 0 \\end{aligned} \\right.$$ 性质：\n$$\\left\\{ \\begin{aligned} \u0026\\text{齐次：基础矩阵 } \\mathbf{F} \\text{ 是齐次矩阵，可以乘以任意非零标量而不改变其性质} \\\\ \u0026\\text{秩约束：} \\mathbf{F} \\text{ 的秩为 } 2 \\end{aligned} \\right.$$ 原式 $= \\underline{\\mathbf{P}} _2^\\top \\cdot \\mathbf{F} _{21} \\cdot \\underline{\\mathbf{P}} _1 = 0$\n$$\\text{设：} \\quad \\mathbf{L}_2 = \\mathbf{F}_{21} \\cdot \\underline{\\mathbf{P}}_1 = \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix}$$ $$\\underline{\\mathbf{P}}_2^\\top \\cdot \\mathbf{L}_2 = 0 \\quad \\Leftrightarrow \\quad a P_{2,x} + b P_{2,y} + c = 0$$ 这就是相机 2 的图像平面中的直线方程 $\\Rightarrow$ 对极线\n4.6 本质和基础矩阵的估计（L’estimation de la matrice） $\\text{Caméra calibrée} \\Rightarrow \\text{estimation de la matrice essentielle (5 DDL)} \\Rightarrow \\text{l\u0026rsquo;algorithme des 5 correspondances}$\n相机已经校准 $\\Rightarrow$ 本质矩阵 $\\mathbf{E}$ 的估计（5 个自由度）$\\Rightarrow$ 5 点对应算法\n$\\text{Caméra non-calibrée} \\Rightarrow \\text{estimation de la matrice fondamentale (7 DDL)} \\Rightarrow \\text{l\u0026rsquo;algorithme des 7 correspondances}$\n相机未校准 $\\Rightarrow$ 基础矩阵 $\\mathbf{F}$ 的估计（7 个自由度）$\\Rightarrow$ 7 点对应算法\n$\\text{Solution} \\Rightarrow \\text{l\u0026rsquo;algorithme 8 correspondances} \\Rightarrow \\text{omettre intentionnellement la contrainte } \\det(\\mathbf{F}) = 0$\n求解 $\\Rightarrow$ 8 点对应算法 $\\Rightarrow$ 故意忽略约束条件 $\\det(\\mathbf{F}) = 0$\n4.7 算法 8 点对应算法步骤：\n收集匹配点对：\n$\\mathbf{F}$ 有 7 个自由度，但在算法中忽略了秩为 2 的约束，因此需要至少 8 对匹配点来估计 $\\mathbf{F}$。\n构建线性方程组\n对于每一对匹配点 $(\\mathbf{m} _1, \\mathbf{m} _2)$，构建方程：\n$$\\underline{\\mathbf{m}}_2^\\top \\cdot \\mathbf{E}_{21} \\cdot \\underline{\\mathbf{m}}_1 = 0$$ $$\\underline{\\mathbf{P}}_2^\\top \\cdot \\mathbf{F}_{21} \\cdot \\underline{\\mathbf{P}}_1 = 0$$ 求解：\n将方程组表示为：$\\underline{\\mathbf{P}} _2^\\top \\cdot \\mathbf{L} _2 = 0$\n$RANSAC$ 算法步骤\n处理匹配点对中的离群点（错误匹配），稳健地估计 $\\mathbf{F}$\n随机采样：利用 8 点对应算法 来估计 $\\mathbf{F}$ 评估模型：利用估计得到的 $\\mathbf{F}$ 来计算所有匹配点对的对极约束误差，即点到对应对极线的距离 判断内点：根据设定的距离阈值，判断哪些匹配点是内点 迭代：重复上述过程，直到找到内点数量最多的模型 5. Ajustement de faisceaux (束调整) 束调整是一种同时优化摄像机参数（包括位置、姿态和内参）和场景中三维点位置的技术 其核心思想是通过最小化三维点在图像上的重投影误差，使得优化后的模型与实际观测更加吻合 记住五个字：最小化投影误差 5.1 两个摄像机的情况 5.1.1 数据 $\\left( P _{A,i}, P _{B,i} \\right) _{i=1,\\dots,N} \\implies N \\text{ correspondences}$\n5.1.2 要估计的参数 摄像机的姿态以及三维点云数据集\n$$\\mathbf{R} _{W1} \\quad \\mathbf{t} _{W1} \\quad \\mathbf{R} _{W2} \\quad \\mathbf{t} _{W2} \\quad \\left\\{\\mathbf{U}^w_i \\right\\}_{i=1,\\dots,N}$$ 5.1.3 损失函数 $$\\mathcal{L} \\left( \\mathbf{R}_{w1}, \\mathbf{t}_{w1}, \\mathbf{R}_{w2}, \\mathbf{t}_{w2}, \\left\\{ \\mathbf{U}^w_i \\right\\}_{i=1,\\dots,N} \\right)= \\sum_{i=1}^{N} \\left( \\left\\lVert P_{1,i} - K_1 \\Pi \\left( \\mathbf{R}_{w1}^\\top \\mathbf{U}_i^{w} - \\mathbf{R}_{w1}^\\top \\mathbf{t}_{w1} \\right) \\right\\rVert_2^2 +\\left\\lVert P_{2,i} - K_2 \\Pi \\left( \\mathbf{R}_{w2}^\\top \\mathbf{U}_i^{w} - \\mathbf{R}_{w2}^\\top \\mathbf{t}_{w2} \\right) \\right\\rVert_2^2 \\right)$$ 其中：\n$K _A$ 和 $K _B$ 是摄像机 $A$ 和 $B$ 的内参矩阵。 $\\Pi(\\cdot)$ 是投影函数，将三维点投影到二维平面上。 $\\mathbf{R} _{w1}^\\top$ 和 $\\mathbf{R} _{w2}^\\top$ 等价于 $\\mathbf{R} _{1w}$ 和 $\\mathbf{R} _{2w}$，即将点从世界坐标系转换到摄像机坐标系。 $\\mathbf{R} _{w1}^\\top \\mathbf{t} _{w1}$ 等价于 $\\mathbf{t} _{1w}$，表示平移向量。 $\\mathbf{U} _i^{1}= \\mathbf{R} _{w1}^\\top \\cdot \\mathbf{U} _i^{w} - \\mathbf{R} _{w1}^\\top \\cdot \\mathbf{t} _{w1}$，也就是将 $\\mathbf{U} _i^{w}$ 变换到 $\\mathbf{U} _i^{1}$，即从世界坐标系变换到相机坐标系。 做差：相机 $A$ 或 $B$ 中的图像坐标（实际）减去三维空间旋转变换得来的估计图像坐标，等于重投影误差。 5.2 多个摄像机的情况 5.2.1 数据 每张图像中检测到的点为：\n$$\\left\\{ \\left\\{ P _{m,i} \\right\\} _{i=1,\\dots,N _m} \\right\\} _{m=1,\\dots,M}$$ 这些点在不同视角下的图像中可以形成轨迹（tracks）\n第 $m$ 个摄像机检测到的点，其中 $N _m$ 是第 $m$ 个摄像机检测到的点的数量\n$$\\left\\{ \\text{p2d-id} _m, \\ \\text{p3d-id} _m \\right\\} _{m=1,\\dots,M}$$ 其中：\n$\\text{p2d-id} _m$ 是二维点在图像中的索引 $\\text{p3d-id} _m$ 是对应的三维点在点云中的索引 它们的大小尺寸都是 $C _m \\times 1$ 5.2.2 要估计的参数 相机外参：\n$$ \\left\\{ \\left( \\mathbf{R} _{wm}, \\mathbf{t} _{wm} \\right) \\right\\} _{m=1,\\dots,M} $$ 三维点的位置：\n$$ \\left\\{ \\mathbf{U} _i^{w} \\right\\} _{i=1,\\dots,N} $$ 5.2.3 损失函数 代价函数扩展为对所有摄像机和所有检测到的点进行误差计算，将投影点与实际观测点之间的距离最小化：\n$$\\mathcal{L}(x) = \\sum_{m=1}^{M} \\sum_{c=1}^{C_m} \\left\\| P_{m,\\ \\text{p2d-id}_m(c)} - K_m \\Pi \\left( \\mathbf{R}_{wm}^\\top \\mathbf{U}_{\\text{p3d-id}_m(c)}^{w} - \\mathbf{R}_{wm}^\\top \\mathbf{t}_{wm} \\right) \\right\\|_2^2$$ $C _m$ 是第 $m$ 台摄像机的观测数量 $\\mathbf{U} _{\\text{p3d-id} _m(c)}^{w}$ 是与观测对应的三维点 我们可以简单地将上述代价函数简化成：\n$$\\mathcal{L}(x) = \\sum_{i=1}^{N} \\left\\| f_i(x) \\right\\|_2^2 \\quad \\left\\{ \\begin{array}{l} x \\in \\mathbb{R}^D \\\\ f_i : \\mathbb{R}^D \\rightarrow \\mathbb{R}^B \\end{array} \\right.$$ $x$ 是所有待优化的参数（摄像机参数和三维点坐标） $f _i(x)$ 是第 $i$ 个残差函数，表示第 $i$ 个观测的重投影误差 我们的目标是找到 $x$，使得 $\\mathcal{L}(x)$ 最小化。这是一个非线性最小二乘问题，通常使用迭代的方法求解 本文内容均为 \u0026lsquo;Vidéo 3D - Vision par ordinateur\u0026rsquo; 课程下的理论部分，均为指导教师 Guillaume Bourmaud 的教学内容，所有版权归老师所有。\n具体实验内容及完整代码见 Guillaume Bourmaud 老师官方网站：https://gbourmaud.github.io/teaching/\n","permalink":"https://zehua716.github.io/zh/posts/signal_cn/3d%E9%87%8D%E5%BB%BA%E7%90%86%E8%AE%BA/","summary":"人工智能中计算机视觉领域下科目，重点包括 相机模型校正、单应性估计、对极几何、束调整等理论内容，以实现图像等校正、拼接、光束校准等应用","title":"3D重建理论"},{"content":"应用随机梯度算法解决信道均衡问题 简介 利用优化算法并构建滤波器来解决信号通道的失真（噪声、多径传播）等问题。在数字通信中，发射机向接收机传输一个符号序列 ${s(n)}_{n\u0026gt;0}$，这些符号取值为 $1$ 或 $-1$，来编码有用的信息。但是在传输过程中，通常信号会失真，这种失真通常可被视为，与传播信道对应滤波器的卷积。\n因此，在信号接收端，需要对该传播信道进行去卷积以恢复被传输的符号。但是在去卷积之前，必须对信道进行估计，也就是了解信道的具体特性（例如信道的冲激响应或频率响应）。为了了解特性，我们需要使用假设已知的训练序列。\n训练序列是 发射机和接收机之间 预先约定的已知符号序列。接收端在收到经过信道传输的训练序列后，可以将其与原始序列进行比较，从而推断信道的特性。这些信道特性我们称之为“信道系数” $w(k)$。在信道系数估计完成后，我们就可以设计滤波器来实现去卷积操作，来减小由于信道引起的失真。\n在这种情况下，随机梯度算法（LMS）被经典地用于该方面。因此，后续我们将使用 LMS 应用于一个简化的信道均衡问题。\n传输链路的建模与仿真 $$w(k) = \\begin{cases} \\frac{1}{2} \\left( 1 + \\cos\\left(\\frac{2\\pi}{\\beta}(k - 1)\\right) \\right), \u0026 \\text{for } k = 0, 1, 2, \\\\ 0, \u0026 \\text{else} \\end{cases}$$ 此外，接收信号受到加性高斯噪声 $u(n)$ 的干扰，加性高斯噪声具有白噪音的特性（独立同分布），零均值，方差为 $\\sigma^2 = 0.001$。\n因此，信号的表达式为：\n$$x(n) = \\sum_{k=0}^{2} w(k)s(n-k) + u(n)$$ 其中， $\\beta$ 控制信道引入的失真水平。\n我们先在 Matlab 中使用函数 $stem$ 生成由符号 $1$ 和 $-1$ 组成的随机序列信号 $s(n)$，并假设 $1$ 和 $-1$ 取值是等概率的。我们可以使用函数 $randn$，并假设信号的长度为 $N = 1500$。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 % Paramètres N = 1500; % signal aléatoire +1 et -1 s = sign(randn(N,1)); % Tracé figure; stem(s); % tracer en discret title(\u0026#39;Signal s(n)\u0026#39;, \u0026#39;FontSize\u0026#39;, 16); xlabel(\u0026#39;Taille du signal N\u0026#39;, \u0026#39;FontSize\u0026#39;, 14); ylabel(\u0026#39;s(n)\u0026#39;, \u0026#39;FontSize\u0026#39;, 14); set(gca, \u0026#39;FontSize\u0026#39;, 12); % agrandir les graduations 只展示了 200 的序列\n由于信号 $s(n)$ 是由符号 $1$ 和 $-1$ 组成的随机序列，且被假设等概率的:\n$X = {+1, -1}$\n$p_{+1} = p_{-1} = \\frac{1}{2}$\n因此，$s(n)$ 的期望形式为：\n$$E[s(n)] = p_{+1} \\cdot (+1) + p_{-1} \\cdot (-1)$$ $$E[s(n)] = \\frac{1}{2} \\cdot (+1) + \\frac{1}{2} \\cdot (-1)$$ $$E[s(n)] = 0$$ 我们接下来绘制传播信道，其公式为:\n$$w(k) = \\frac{1}{2} \\left( 1 + \\cos \\left( \\frac{2\\pi}{\\beta}(k - 1) \\right) \\right)$$ 对 $\\beta$ 取不同值： $0.25$, $2$ 和 $4$ 来进行观察\n• 对于 $\\beta = 0.25$ ：\n当 $\\beta$ 较小时，例如 $0.25$，传输信道具有较短的脉冲响应，这会导致余弦函数的快速变化。在这种情况下，可以预期 $w(k)$ 在 $k = 0$ 到 $k = 2$ 之间有显著变化。\n$$w(0) = \\frac{1}{2} \\left( 1 + \\cos \\left( \\frac{2\\pi}{0.25}(-1) \\right) \\right) = \\frac{1}{2} \\left( 1 + \\cos(-8\\pi) \\right) = \\frac{1}{2}(1 + 1) = 1$$ $$w(1) = \\frac{1}{2} \\left( 1 + \\cos \\left( \\frac{2\\pi}{0.25}(0) \\right) \\right) = \\frac{1}{2} \\left( 1 + \\cos(0) \\right) = \\frac{1}{2}(1 + 1) = 1$$ $$w(2) = \\frac{1}{2} \\left( 1 + \\cos \\left( \\frac{2\\pi}{0.25}(1) \\right) \\right) = \\frac{1}{2} \\left( 1 + \\cos(8\\pi) \\right) = \\frac{1}{2}(1 + 1) = 1$$ 因此，当 $\\beta = 0.25$ 时，$w(k)$ 在点 $k = 0, 1, 2$ 处的值均为 $1$。\n可以看到传播信道在前 $3$ 个采样点上具有恒定的脉冲响应。\n• 对于 $\\beta = 2$ ：\n$$w(0) = \\frac{1}{2} \\left( 1 + \\cos \\left( \\frac{2\\pi}{2}(-1) \\right) \\right) = \\frac{1}{2} \\left( 1 + \\cos(-\\pi) \\right) = \\frac{1}{2}(1 - 1) = 0$$ $$w(1) = \\frac{1}{2} \\left( 1 + \\cos \\left( \\frac{2\\pi}{2}(0) \\right) \\right) = \\frac{1}{2} \\left( 1 + \\cos(0) \\right) = \\frac{1}{2}(1 + 1) = 1$$ $$w(2) = \\frac{1}{2} \\left( 1 + \\cos \\left( \\frac{2\\pi}{2}(1) \\right) \\right) = \\frac{1}{2} \\left( 1 + \\cos(\\pi) \\right) = \\frac{1}{2}(1 - 1) = 0$$ 因此，对于 $\\beta = 2$，$w(k)$ 在点 $k = 0$ 和 $k = 2$ 的值为 $0$，而在点 $k = 1$ 的值为 $1$。\n可以看到传播信道的脉冲响应主要集中在 $k = 1$，主要影响第一个延迟的信号。\n• 对于 $\\beta = 4$ ：\n当 $\\beta$ 更大时，例如 $4$，传输信道的脉冲响应周期进一步延长，余弦函数的变化变得更慢。在这种情况下，可以预期 $w(k)$ 在 $k = 0$ 到 $k = 2$ 之间的变化较小。\n$$w(0) = \\frac{1}{2} \\left( 1 + \\cos \\left( \\frac{2\\pi}{4}(-1) \\right) \\right) = \\frac{1}{2} \\left( 1 + \\cos\\left(-\\frac{\\pi}{2}\\right) \\right) = \\frac{1}{2}(1 + 0) = \\frac{1}{2}$$ $$w(1) = \\frac{1}{2} \\left( 1 + \\cos \\left( \\frac{2\\pi}{4}(0) \\right) \\right) = \\frac{1}{2} \\left( 1 + \\cos(0) \\right) = \\frac{1}{2}(1 + 1) = 1$$ $$w(2) = \\frac{1}{2} \\left( 1 + \\cos \\left( \\frac{2\\pi}{4}(1) \\right) \\right) = \\frac{1}{2} \\left( 1 + \\cos\\left(\\frac{\\pi}{2}\\right) \\right) = \\frac{1}{2}(1 + 0) = \\frac{1}{2}$$ 因此，对于 $\\beta = 4$，$w(k)$ 在点 $k = 0$ 和 $k = 2$ 的值为 $0.5$，而在点 $k = 1$ 的值为 $1$。\n可以看到传播信道具有一个对称的脉冲响应，其峰值位于 $k = 1$，如 $\\beta = 2$ 的情况（图 3），但它在第一个和第三个采样点上也包含非零值。即脉冲响应更加分散且对称，但主要集中在中心。\n这表明，具有均匀或扩展脉冲响应的信道需要均衡器来补偿分布在多个采样点上的失真；而具有集中脉冲响应的信道可能更容易均衡，因为失真主要局限于单个采样点。\n后续中，我们将使用 $\\beta = 0.25$，通过已经构建好的随机序列信号 $s(n)$，使用 Matlab 函数 $filter$ 来添加加性噪声，以获得信号 $x(n)$。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 %% Q4 % Réponse impulsionnelle w = [1 1 1]; % Filtrage de s(n) x = filter(w, 1, s); % filter(den, num, fonction à filtrer) sigma_u = sqrt(0.001); % ecart type du signal % Ajout bruit gaussien u = sigma_u * randn(N,1); x = x + u; % Tracé figure; stem(x); title(\u0026#39;Signal x(n)\u0026#39;, \u0026#39;FontSize\u0026#39;, 16); xlabel(\u0026#39;Taille du signal N\u0026#39;, \u0026#39;FontSize\u0026#39;, 14); ylabel(\u0026#39;x(n)\u0026#39;, \u0026#39;FontSize\u0026#39;, 14); set(gca, \u0026#39;FontSize\u0026#39;, 12); % agrandir les graduations 实现均衡 为了实现信道均衡，我们最终目标是确定最优维纳滤波器 $h_{\\text{opt}}$ 的理论表达式，以最小化均方误差。为此，首先需要计算信号 $x(n)$ 的自相关函数和 $x(n)$ 与 $d(n)$ 的互相关函数，分别记为 $r_{xx}(k)$ 和 $r_{dx}(k)$。对于真实且平稳的信号，其相关函数公式为：\n$$r_{xx}(k) = E[x(n)x(n-k)]$$ $$r_{dx}(k) = E[d(n)x(n-k)]$$ 我们首先研究（由传播信道卷积得到的）发射信号 $m(n)$ 的自相关函数 $r_{mm}(k)$，其中 $m(n) = \\sum_{k=0}^{2} w(k)s(n-k)$。首先计算 $r_{mm}(0)$、 $r_{mm}(1)$ 和 $r_{mm}(2)$ 来观察结果。同时注意，我们之前假设符号 $s(n)$ 为相互独立的。\n计算 $r_{mm}(0)$ $$r_{mm}(0) = E \\left[ \\left( \\sum_{k=0}^{2} w(k)s(n-k) \\right)^2 \\right]$$ $$r_{mm}(0) = \\sum_{k=0}^{2} w^2(k) E[s^2(n-k)]$$ 由于 $s(n)$ 是相互独立的，因此有：\n$$E[s^2(n-k)] = 1$$ 由于 $s(n)$ 的特性：\n$$E[s^2(n)] = 1 \\cdot P(s(n) = 1) + 1 \\cdot P(s(n) = -1)$$ $$E[s^2(n)] = 1 \\cdot \\frac{1}{2} + 1 \\cdot \\frac{1}{2} = 1$$ 因此：\n$$r_{mm}(0) = \\sum_{k=0}^{2} w^2(k)$$ $$r_{mm}(0) = w(0)^2 + w(1)^2 + w(2)^2 = 3$$ 计算 $r_{mm}(1)$ $$r_{mm}(1) = E[m(n)m(n-1)]$$ $$r_{mm}(1) = E \\left[ \\left( \\sum_{k=0}^{2} w(k)s(n-k) \\right) \\left( \\sum_{j=0}^{2} w(j)s(n-1-j) \\right) \\right]$$ $$r_{mm}(1) = \\sum_{k=0}^{2} \\sum_{j=0}^{2} w(k)w(j) E[s(n-k)s(n-1-j)]$$ 同样，仅当 $k = j + 1$ 时，相关函数才不为零，因此：\n$$r_{mm}(1) = w(0)w(1)E[s(n-1)s(n-1)] + w(1)w(2)E[s(n-2)s(n-2)]$$ 由于 $E[s(n)] = 0$，因此：\n$$E[s^2(n)] = 1$$ 因此：\n$$r_{mm}(1) = w(0)w(1) \\cdot 1 + w(1)w(2) \\cdot 1$$ $$r_{mm}(1) = w(0)w(1) + w(1)w(2) = 2$$ 计算 $r_{mm}(2)$ $$r_{mm}(2) = E \\left[ \\left( \\sum_{k=0}^{2} w(k)s(n-k) \\right) \\left( \\sum_{j=0}^{2} w(j)s(n-2-j) \\right) \\right]$$ $$r_{mm}(2) = \\sum_{k=0}^{2} \\sum_{j=0}^{2} w(k)w(j) E[s(n-k)s(n-2-j)]$$ 同样，仅当 $k = j + 2$ 时，相关函数才不为零，因此：\n$$r_{mm}(2) = w(0)w(2)E[s(n-2)s(n-2)]$$ 由于 $E[s(n)] = 0$，因此：\n$$E[s^2(n-2)] = 1$$ 因此：\n$$r_{mm}(2) = w(0)w(2) = 1$$ 接下来我们需要找到当 $k \u0026gt; 2$ 时，$r_{mm}(k) = 0$ 并推出任意 $k$ 时 $r_{mm}(k)$ 的表达式。\n为了验证 $r_{mm}(k) = 0$ 当 $k \u0026gt; 2$，我们先计算 $r_{mm}(3)$ 看看：\n$$r_{mm}(3) = E \\left[ \\left( \\sum_{k=0}^{2} w(k)s(n-k) \\right) \\left( \\sum_{j=0}^{2} w(j)s(n-3-j) \\right) \\right]$$ $$r_{mm}(3) = \\sum_{k=0}^{2} \\sum_{j=0}^{2} w(k)w(j) E[s(n-k)s(n-3-j)]$$ 同样，仅当 $k = j + 3$ 时，相关函数才不为零，因此：\n$$r_{mm}(3) = w(0)w(3)E[s(n-3)s(n-3)]$$ 由于：\n$$w(k) = 0 , \\text{对于 } k \\neq 0, 1, 2$$ 因此：\n$$w(3) = 0$$ 因此：\n$$r_{mm}(3) = 0$$ 结论\n$$r_{mm}(k) = \\begin{cases} w(0)^2 + w(1)^2 + w(2)^2 \u0026 \\text{当 } k = 0 \\\\ w(0)w(1) + w(1)w(2) \u0026 \\text{当 } k = 1 \\\\ w(0)w(2) \u0026 \\text{当 } k = 2 \\\\ 0 \u0026 \\text{当 } k \u003e 2 \\end{cases}$$ $$r_{mm}(k) = \\begin{cases} 3 \u0026 \\text{当 } k = 0 \\\\ 2 \u0026 \\text{当 } k = 1 \\\\ 1 \u0026 \\text{当 } k = 2 \\\\ 0 \u0026 \\text{当 } k \u003e 2 \\end{cases}$$ 计算 $r_{xx}(k)$ 我们使用已经得到的结果来计算 $r_{xx}(k)$\n我们已知 $x(n) = m(n) + u(n)$，并且噪声 $u(n)$ 被假设为独立于传输符号的。\n$$r_{xx}(k) = E[x(n)x(n-k)]$$ $$r_{xx}(k) = E\\left[(m(n) + u(n))(m(n-k) + u(n-k))\\right]$$ $$r_{xx}(k) = E[m(n)m(n-k)] + E[m(n)u(n-k)] + E[u(n)m(n-k)] + E[u(n)u(n-k)]$$ 由于 $m(n)$ 和 $u(n)$ 是独立的：\n$$E[m(n)u(n-k)] = 0$$ 并且 $u(n)$ 是零均值的白噪声：\n$$E[u(n)u(n-k)] = \\sigma_u^2\\delta(k)$$ 因此：\n$$r_{xx}(k) = r_{mm}(k) + \\sigma_u^2\\delta(k)$$ $$r_{xx}(k) = \\begin{cases} w(0)^2 + w(1)^2 + w(2)^2 + \\sigma_u^2 \u0026 \\text{当 } k = 0 \\\\ w(0)w(1) + w(1)w(2) \u0026 \\text{当 } k = 1 \\\\ w(0)w(2) \u0026 \\text{当 } k = 2 \\\\ 0 \u0026 \\text{当 } k \u003e 2 \\end{cases}$$ $$r_{xx}(k) = \\begin{cases} 3 + \\sigma_u^2 \u0026 \\text{当 } k = 0 \\\\ 2 \u0026 \\text{当 } k = 1 \\\\ 1 \u0026 \\text{当 } k = 2 \\\\ 0 \u0026 \\text{当 } k \u003e 2 \\end{cases}$$ $h_{\\text{opt}}$ 的表达式 最终我们计算 $r_{dx}(k)$ 并最终给出 $h_{\\text{opt}}$ 的表达式。\n$$r_{dx}(k) = E[d(n)x(n-k)]$$ 已知 $d(n) = s(n-2)$ 且 $x(n) = m(n) + u(n)$：\n$$r_{dx}(k) = E[s(n-2)(m(n-k) + u(n-k))]$$ 由于 $u(n)$ 和 $s(n)$ 是独立的且 $u(n)$ 的均值为 $0$：\n$$E[s(n-2)u(n-k)] = 0$$ 因此：\n$$r_{dx}(k) = E[s(n-2)m(n-k)]$$ 由于已知：\n$$m(n) = \\sum_{k=0}^{2} w(k)s(n-k)$$ 因此：\n$$r_{dx}(k) = E[s(n-2)(w(0)s(n-k) + w(1)s(n-k-1) + w(2)s(n-k-2))]$$ 当 $k = 0$ 时：\n$$r_{dx}(0) = w(2)E[s(n-2)s(n-2)] = w(2)$$ 当 $k = 1$ 时：\n$$r_{dx}(1) = w(1)E[s(n-2)s(n-2)] = w(1)$$ 当 $k = 2$ 时：\n$$r_{dx}(2) = w(0)E[s(n-2)s(n-2)] = w(0)$$ 结论 $$r_{dx}(k) = \\begin{cases} w(2) \u0026 \\text{当 } k = 0 \\\\ w(1) \u0026 \\text{当 } k = 1 \\\\ w(0) \u0026 \\text{当 } k = 2 \\\\ 0 \u0026 \\text{当 } k \u003e 2 \\end{cases}$$ 并且：\n$$h_{\\text{opt}} = R_{xx}^{-1}r_{dx}$$ $$h_{\\text{opt}} = \\begin{pmatrix} r_{xx}(0) \u0026 r_{xx}(1) \u0026 r_{xx}(2) \\\\ r_{xx}(1) \u0026 r_{xx}(0) \u0026 r_{xx}(1) \\\\ r_{xx}(2) \u0026 r_{xx}(1) \u0026 r_{xx}(0) \\end{pmatrix}^{-1} \\begin{pmatrix} r_{dx}(0) \\\\ r_{dx}(1) \\\\ r_{dx}(2) \\end{pmatrix}$$ $$h_{\\text{opt}} = \\begin{pmatrix} r_{xx}(0) \u0026 r_{xx}(1) \u0026 r_{xx}(2) \\\\ r_{xx}(1) \u0026 r_{xx}(0) \u0026 r_{xx}(1) \\\\ r_{xx}(2) \u0026 r_{xx}(1) \u0026 r_{xx}(0) \\end{pmatrix}^{-1} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$$ 实现 我们将在 Matlab 中设计一个滤波器，并使用两种方法计算自相关和互相关来构建滤波器：一种方法是基于理论公式，用 $Toeplitz$ 函数构造自相关矩阵；另一种方法是通过 Matlab 的 $xcorr$ 函数以数值方式计算这些相关函数，然后比较两种方法下得到的滤波器结果。\n我们先看第一种实现，使用上述得到的计算公式，并利用 $Toeplitz$ 函数构造自相关矩阵。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 L = 11; % r_xx(k) pour k = 1,2 et 3 % si k \u0026gt; 2 alors r_xx(k) = 0 r_xx1 = w(1)^2 + w(2)^2 + w(3)^2 + sigma_u^2; % r_xx(0) r_xx2 = w(1)*w(2) + w(2)*w(3); % r_xx(1) r_xx3 = w(1)*w(3); % r_xx(2) r_xx = [r_xx1; r_xx2; r_xx3; zeros(8,1)]; % R_xx Toeplitz avec r_xx comme première colonne R_xx = toeplitz(r_xx); % r_dx(k) pour k = 1,2 et 3 % si k \u0026gt; 2 alors r_dx(k) = 0 r_dx1 = w(3); % r_dx(0) r_dx2 = w(2); % r_dx(1) r_dx3 = w(1); % r_dx(2) r_dx = [r_dx1; r_dx2; r_dx3; zeros(8,1)]; % Wiener-Hopf h_opt = inv(R_xx)*r_dx; disp(\u0026#39;Filtre optimal de Wiener h_opt:\u0026#39;); disp(h_opt); 第二种实现，通过 Matlab 的 $xcorr$ 函数以数值方式（不依赖理论公式，直接应用样本数据）计算这些相关函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 hopt_theo = h_opt; % d(n) = s(n-2) d = s(1:end-2); x2 = x(3:end); % autocorr r_xx_num = xcorr(x2); r_xx_num = r_xx_num(N-2:N-2+L-1); % intercorr r_dx_num = xcorr(d, x2); r_dx_num = r_dx_num(N-2:N-2+L-1); R_xx_num = toeplitz(r_xx_num); h_opt_num = inv(R_xx_num)*r_dx_num; figure, stem(hopt_theo), title(\u0026#39;Filtre optimal théorique\u0026#39;, \u0026#39;FontSize\u0026#39;,16), hold on; stem(h_opt_num), title(\u0026#39;Filtre optimal numérique\u0026#39;, \u0026#39;FontSize\u0026#39;, 16) xlabel(\u0026#39;Ordre du signal L\u0026#39;,\u0026#39;FontSize\u0026#39;,14), ylabel(\u0026#39;Amplitude\u0026#39;, \u0026#39;FontSize\u0026#39;,14) set(gca, \u0026#39;FontSize\u0026#39;, 14); legend(\u0026#39;Théorique\u0026#39;,\u0026#39;Numérique\u0026#39;) 我们发现数值滤波器（Numérique）非常接近理论最优滤波器（Théorique）。\n现在，我们应用 LMS 算法来迭代解决信道估计问题，这里假设 $L = 11$。我们创建一个函数 $algoLMS$，该函数接收输入信号 $x(n)$ 和 $d(n)$，脉冲响应长度 $L$，以及步长 $\\mu$，并输出后验误差序列 $e^+(n)$ 和通过 LMS 算法迭代得到的脉冲响应 $h_n$。并通过调整步长 $\\mu$ 来测试算法，并且展示滤波器系数误差的范数 $|h_n - h_{\\text{opt}}|_2$。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 %% Q5 % pas mu = [0.001, 0.005, 0.01, 0.02]; % erreurs et coeff [err1, h1] = algoLMS(x2, d, L, mu(1)); [err2, h2] = algoLMS(x2, d, L, mu(2)); [err3, h3] = algoLMS(x2, d, L, mu(3)); [err4, h4] = algoLMS(x2, d, L, mu(4)); % norme des erreurs des coeff normeh1 = sqrt(sum((h1 - hopt_theo).^2)); normeh2 = sqrt(sum((h2 - hopt_theo).^2)); normeh3 = sqrt(sum((h3 - hopt_theo).^2)); normeh4 = sqrt(sum((h4 - h_opt).^2, 1)); figure; plot(err1); grid on; hold on; plot(err2); plot(err3); plot(err4); xlabel(\u0026#39;Taille du signal N\u0026#39;, \u0026#39;FontSize\u0026#39;, 14); ylabel(\u0026#39;e(n)\u0026#39;, \u0026#39;FontSize\u0026#39;, 14); title(\u0026#39;Erreurs a posteriori e^+(n) pour différentes valeurs de μ\u0026#39;, \u0026#39;Interpreter\u0026#39;, \u0026#39;none\u0026#39;, \u0026#39;FontSize\u0026#39;, 16); legend(\u0026#39;\\mu = 0.001\u0026#39;, \u0026#39;\\mu = 0.005\u0026#39;, \u0026#39;\\mu = 0.01\u0026#39;, \u0026#39;\\mu = 0.02\u0026#39;); figure; plot(normeh1); grid on; hold on; plot(normeh2); plot(normeh3); plot(normeh4); xlabel(\u0026#39;Taille du signal N\u0026#39;, \u0026#39;FontSize\u0026#39;, 14); ylabel(\u0026#39;||h(n)-h_{opt}||^2\u0026#39;, \u0026#39;FontSize\u0026#39;, 14); title(\u0026#39;Norme de l\u0026#39;\u0026#39;erreur à posteriori\u0026#39;, \u0026#39;Interpreter\u0026#39;, \u0026#39;none\u0026#39;, \u0026#39;FontSize\u0026#39;, 16); legend(\u0026#39;\\mu = 0.001\u0026#39;, \u0026#39;\\mu = 0.005\u0026#39;, \u0026#39;\\mu = 0.01\u0026#39;, \u0026#39;\\mu = 0.02\u0026#39;); 子函数:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 function [eplus,h_tout] = algoLMS(x,d,L,mu) N = length(x); h = ones(L,1); eplus = zeros(N,1); h_tout = zeros(L,N); for n=L:N xn = x(n:-1:n-L+1); yn = h\u0026#39;*xn; eplus(n) = d(n) - yn; h = h + mu*xn*(d(n)-xn\u0026#39;*h); h_tout(:,n) = h; end end 我们下面计算理论上使算法收敛的步长 $\\mu$ 的最大值，并和实际值做对比。\n收敛的充分必要条件为：\n$$0 \u003c \\mu \u003c \\frac{2}{\\lambda_{\\text{max}}}$$ 其中， $\\lambda_{\\text{max}}$ 是自相关矩阵 $R_{xx}$ 的最大特征值。\n我们有：\n$$E[h_n] = E[h_{n-1}] - \\mu E[x(n)x(n)^T h_{n-1}] + \\mu E[d(n)x(n)]$$ 在假设 $x(n)$ 和 $h_{n-1}$ 独立的情况下，可以写为：\n$$E[x(n)x(n)^T h_{n-1}] = E[x(n)x(n)^T]E[h_{n-1}]$$ 因此：\n$$E[h_n] = E[h_{n-1}] - \\mu E[x(n)x(n)^T]E[h_{n-1}] + \\mu E[d(n)x(n)]$$ 此外：\n$$\\mu E[d(n)x(n)] = R_{xx} \\cdot h_{\\text{opt}}$$ 因此：\n$$E[h_n] - h_{\\text{opt}} = (I - \\mu R_{xx})(E[h_{n-1}] - h_{\\text{opt}})$$ 假设：\n$$\\delta h_n = E[h_n] - h_{\\text{opt}}$$ 因此：\n$$\\delta h_n = (I - \\mu R_{xx})\\delta h_{n-1}$$ 经过对 $R_{xx}$ 的一系列变换，我们得到：\n$$\\delta h_n = (I - \\mu R_{xx})\\delta h_{n-1}$$ $$\\delta h_n(l) = (1 - \\mu \\lambda_l)^n \\delta h_0(l)$$ $$|\\mu \\lambda_l| \u003c 1 \\iff 0 \u003c \\mu \u003c \\frac{2}{\\lambda_l}$$ 结论：\n$$0 \u003c \\mu \u003c \\frac{2}{\\lambda_{\\text{min}}}$$ 理论最大值为：\n$$\\mu = \\frac{2}{\\lambda_{\\text{max}}} = \\frac{2}{8.6236} = 0.2319$$ 其中 $\\lambda_{\\text{max}}$ 由 Matlab 确定。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 % Calcul des valeurs propres de R_xx valeurs_propres = eig(R_xx); lambda_max = max(valeurs_propres); % Calcul de la valeur maximale théorique du pas mu_theorique_max = 2 / lambda_max; disp(\u0026#39;Valeur maximale théorique du pas :\u0026#39;); disp(mu_theorique_max); 由于理论计算是在以下假设条件下完成的：输入信号是平稳的，信道模型是精确的。而在实践中，真实信号并不总是满足这些假设。因此，应适当降低 $\\mu$ 的值。在我们的案例中，选择 $\\mu = 0.01$。\n我们现在改变滤波器的阶数 $L$ 和噪声水平 $\\sigma^2_u$，以测试算法的鲁棒性。特别是，绘制误差范数 $|h_n - h_{\\text{opt}}|_2$ 随 $\\sigma^2_u$ 的变化曲线。\n","permalink":"https://zehua716.github.io/zh/posts/signal_cn/%E5%BA%94%E7%94%A8%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95%E8%A7%A3%E5%86%B3%E4%BF%A1%E9%81%93%E5%9D%87%E8%A1%A1%E9%97%AE%E9%A2%98/","summary":"特定情况下构建滤波器","title":"应用随机梯度算法解决信道均衡问题"},{"content":"所有理论内容均为相关课程记录，课上手写记录，课后转为电子版，并对内容作出了进一步的翻译和解释，因此本人只有劳动成果，所有学术成果均归指导教师所有，每篇文章的最后均有标注指导教师以及对应的个人网站(如果有的话)。\n所有实践内容均为实验课后报告的中文版，理论和代码部分基本都是由我完成的。\n","permalink":"https://zehua716.github.io/zh/about/","summary":"\u003cp\u003e所有理论内容均为相关课程记录，课上手写记录，课后转为电子版，并对内容作出了进一步的翻译和解释，因此本人只有劳动成果，所有学术成果均归指导教师所有，每篇文章的最后均有标注指导教师以及对应的个人网站(如果有的话)。\u003c/p\u003e\n\u003cp\u003e所有实践内容均为实验课后报告的中文版，理论和代码部分基本都是由我完成的。\u003c/p\u003e","title":""}]