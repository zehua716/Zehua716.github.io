---
title: "论文阅读 - DiffuseVAE 详解"
author: "zehua"
date: "2024-11-27T16:25:17+01:00"
lastmod: "2024-11-27T17:12:35+08:00"
draft: false
summary: ""
description: ""
tags: ["机器学习","生成模型","统计分析"]
lang: "zh"
# categories: "posts"
# cover:
#     image: "images/.jpg"
# comments: true
# hideMeta: false
# searchHidden: false
# ShowBreadCrumbs: true
# ShowReadingTime: false
---

# DiffuseVAE 

## **背景问题**：

- **扩散模型**：生成质量高，但存在两个缺点：① 缺乏低维、可解释的潜在空间；② 生成速度较慢。
- **VAE**：具有低维潜在空间，可以解释和操控生成，但生成图像质量较差。

因此提出了 **DiffuseVAE**，将VAE嵌入到扩散模型框架中，并设计了新的条件参数化方法，使扩散模型能够利用VAE推导的低维潜在编码。在标准基准（如 CIFAR-10 和 CelebA-64）上，生成质量优于大多数基于 VAE 的方法，并与最先进模型接近，并独特地保留了对低维潜在空间的访问能力，这是其他模型没有的。



## **VAE和扩散模型的优缺点** 

- **VAE 的特点**：
  - **显式似然生成模型**：以概率分布的方式描述数据生成过程。
  - **低维潜在表示**：通过学习隐空间表示，可以捕捉数据的结构和特征。
  - **灵活性**：适用于多种任务，包括：
    - 解耦表示学习（分离数据中的不同特征）。
    - 半监督学习（结合少量有标注数据进行学习）。
    - 异常检测（识别偏离正常分布的样本）。

- **VAE 的局限性**：
  - **模糊的生成样本**：生成图像往往缺乏高频细节和清晰度。
  - **结构复杂性**：最近的一些改进（如层次化潜在结构）尽管提高了生成质量，但增加了模型复杂度（需要大规模的潜在代码层次化结构）。
  - **与GAN的差距**：相比隐式生成模型（如GAN），VAE在样本质量上仍显逊色，特别是在视觉感知方面。



- **扩散模型（DDPM）的优势和局限性**：
  - **生成质量高**：在多个图像合成任务中表现优异，甚至在某些基准上超越了GANs。
  - **高昂的计算成本**：生成样本时需要多次迭代的逐步采样过程，计算开销大，生成速度慢。
  - **缺乏低维潜在表示**：无法像VAE那样学习和利用低维潜在空间，这限制了其在某些下游任务（如表示解耦、异常检测、可控生成）中的应用。



## **VAE和DDPM复习回顾**

### **变分自编码器 (VAE )**

VAE  是一种利用深度神经网络进行变分推断的生成模型，使用编码器将数据 $x$ 映射到潜在空间表示 $z$，再通过解码器从 $z$ 重建 $x$。它通过最大化数据对数似然 $\log p(x)$ 来学习模型，但由于直接计算对数似然通常不可行，VAE 使用 **证据下界（ELBO）** 来近似优化。 

VAE 的损失函数（ELBO）定义如下：
$$
\mathcal{L}(\theta, \phi) = \underbrace{\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]}_{\text{重建误差}} - \underbrace{D_{KL}[q_\phi(z|x) \| p(z)]}_{\text{KL 散度正则化}}
$$
为了使模型的优化过程可微分，VAE 引入了重参数化技巧，将采样过程从随机变量 $z$ 中分离。例如，将随机变量 $z$ 表示为 $z = \mu + \sigma \cdot \epsilon$，其中 $\epsilon$ 是从标准正态分布采样的噪声。默认先验分布 $p(z)$ 通常设为标准高斯分布，但可以扩展到更复杂的分布（如混合分布或归一化流）以提升模型的表达能力。

更详细内容请看 [VAE](https://zehua.eu/zh/posts/machinelearning_cn/vae/)

### **DDPM**

DDPM 是一种基于潜变量的生成模型， 利用正向过程将数据逐步加噪到高斯分布，再通过逆向过程从噪声中逐步还原数据。包含两个过程：

#### **正向加噪过程**

通过一个固定的高斯马尔可夫链，逐步对数据 $x_0$ 加噪，使其最终接近各向同性高斯分布。正向过程的核心是逐步破坏数据结构，每一步的加噪过程使用高斯分布：
$$
q(x_{1:T}|x_0) = \prod_{t=1}^{T} q(x_t|x_{t-1})
$$
这意味着当前状态 $x_t$ 仅依赖于前一个状态 $x_{t-1}$，整个加噪过程的联合分布可以通过各步的条件分布相乘得到，每一步的加噪过程由高斯分布建模：
$$
q(x_t|x_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t}x_{t-1}, \beta_t\mathbf{I})
$$

- 其中 $\beta_t$ 是噪声调度参数，决定每一步加噪的程度。

通过数学推导，可以从原始数据 $x_0$ 直接生成第 $t$ 步数据：
$$
q(x_t|x_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t}x_0, (1-\bar{\alpha}_t)\mathbf{I})
$$

- 其中 $\bar{\alpha}_t = \prod_{i=1}^t (1-\beta_i)$ 是累积衰减系数。

正向过程是固定的，不需要学习，最终会将数据 $x_0$ 转化为接近高斯分布的 $x_T$。

#### **逆向去噪过程**

逆向过程试图逐步还原数据，采用可学习的高斯马尔可夫链：
$$
p_\theta(x_{t-1}|x_t) = \mathcal{N}(\mu_\theta(x_t, t), \Sigma_\theta(x_t, t)) 
$$

- 其中均值 $\mu_\theta$ 和协方差 $\Sigma_\theta$ 是通过神经网络学习的。

采样时，先从 $p(x_T)$（通常选为各向同性高斯分布）中采样一个噪声向量，再运行逆向过程逐步去噪，生成最终样本 $x_0$。

噪声调度参数 $\beta_t$ 是正向加噪过程的核心，影响生成样本的质量和采样速度。可以是固定值，也可以通过优化学习得到。整个模型通过变分推断优化，目标是最小化正向过程和逆向过程的分布差异。

DDPM 主要优势是生成样本质量高，但缺点是采样速度较慢（需要多次迭代）。这是许多扩散模型后续改进的基础，例如提高采样效率的 DDIM。



## **DiffuseVAE: VAE与扩散模型的结合**

提出的 **DiffuseVAE** 将 VAE 和 DDPM 结合，使生成过程既拥有 VAE 提供的低维潜在空间，又有 DDPM 生成高质量细节的能力。框架中通过两阶段建模，先利用 VAE 的低维潜在空间对样本进行全局特征建模生成粗略样本，再用 DDPM 细化提高质量。通过两阶段建模解决了 VAE 样本模糊和 DDPM 缺乏低维表示的问题。

### **算法框架**：

- 该框架可归纳为生成器-细化器框架

  - **生成器**：第一阶段通过VAE  对训练数据 $x$ 拟合，生成重构样本 $\hat{x}$。
  - **细化器**：第二阶段通过条件 DDPM 对 VAE 的重构样本 $\hat{x}$ 进行细化建模，从而生成更高质量的样本。

- **第一阶段 **：VAE 建模

  给定训练数据 $(x_0, y)$，其中 $x_0$ 是高分辨率数据（例如图像），$y$ 是辅助条件信号（例如一些特征描述信息）。  

  - **编码器**：从 $(x_0, y)$ 中提取潜在表示 $z$，即 $q_\psi(z|y, x_0)$。这是一种条件后验分布，因为在给定 $(y,x_0)$ 的条件下，我们对 $z$ 的分布进行估计。  
  - **解码器**：在给定 $z$ 的前提下生成 $y$，即 $p_\theta(y|z)$。这里 $y$ 是由潜在表示 $z$ 解码而来的。

  注意，这里 VAE 最后的输出不是直接生成 $x_0$ ，而是生成 条件辅助信号 $y$ ，随后我们会在第二阶段使用扩散模型来从 $y$ 和 $z$ 再还原出 $x_0$ ，即生成 $x_0$ 是扩散模型DDPM算法的任务。形象一点来讲，VAE 的作用是给DDPM提供一个充满了信息（潜在特征）的藏宝图（低维潜在空间），真正一步步（逐渐加噪去噪）找到宝藏（恢复图片）还是要靠DDPM算法来实现

- **第二阶段：DDPM 建模**

  在有了 VAE 产生的条件信号 $y$ 以及对应的潜在表示 $z$ 后，我们利用条件扩散模型（DDPM）从 $y,z$ 出发，对 $x_0$ 进行细化生成。

  这里和单纯扩散模型DDPM算法过程类似，依旧是加噪和去噪过程

  - **前向加噪过程**：

    从真实数据 $x_0$ 开始，不断向其添加噪声，经过 $T$ 步得到一系列加噪样本 $x_{1:T}$。这一过程用 $q(x_{1:T}|y,z,x_0)$ 表示，该分布是已知的（通常是固定高斯加噪过程）。

  - **逆向去噪过程**：

    扩散模型要学习从纯噪声逐步去噪回到 $x_0$ 的逆向分布 $p_\phi(x_{0:T}|y,z)$。这里 $p_\phi$ 由参数 $\phi$ 决定，需要通过训练来拟合。在条件 $y$ 和潜在表示 $z$ 下逐步还原出 $x_0$。

这里需要从真实数据 $x_0$ 开始加噪，然后得到 $x_{1:T}$ ，这里从真实数据开始而不是直接从  $y,z$ 开始的原因是；真实数据提供了逆向去噪的正确答案，即当我们用真实数据加噪后，逆向去噪应该能从 $x_T$ 最终得到原始的 $x_0$，那么就说明去噪过程是正确的。

![image-20241211100919496](/Users/zehua/Library/Application Support/typora-user-images/image-20241211100919496.png)



DiffuseVAE 的低维潜在空间能够捕捉生成样本的全局特征(内容、形状或结构)，这就意味着用户可以通过修改这个潜在空间中的变量来直接影响生成样本的主要特性。并且 DiffuseVAE 在速度和质量之间取得了更好的平衡。例如，仅需 10 步采样就能生成质量合理的样本，比传统 DDPM 的效率更高，且生成效果优于相同采样次数的其他方法（如 DDIM）。DiffuseVAE 经过预训练后，对条件信号中的不同噪声类型表现出较好的泛化能力，证明其框架不仅适合特定场景，还能应对多种不确定性条件。



### **元素表达**

- $x_0$：我们想生成的高分辨率图像；
- $y$：通过 VAE 建模的辅助条件信号；
- $z$：与 $y$ 相关联的潜在表示；
- $x_{1:T}$：扩散模型学习到的 $T$ 个加噪表示。

### **联合分布的分解**

$$
p(x_{0:T}, y, z) = p(z)p_\theta(y|z)p_\phi(x_{0:T}|y, z)
$$

- **$p(z)$** 是潜在变量的先验分布，通常设为标准高斯分布；

- **$p_\theta(y|z)$** 表示在潜在变量 $z$ 下生成条件信号 $y$ 的概率，这是由 VAE 的解码器学习到的；

- **$p_\phi(x_{0:T}|y, z)$** 是扩散模型的逆向去噪过程，用来从条件信号 $y$ 和潜在表示 $z$ 逐步恢复数据。

### 后验分布的近似

在给定数据 $(x_0, y)$ 的情况下，潜在表示 $z$ 和中间加噪样本 $x_{1:T}$ 的后验分布：
$$
p(x_{1:T}, z|y, x_0)
$$
该后验分布没有显式解析解，于是我们用一个近似分布 $q(x_{1:T}, z|y, x_0)$ 来替代：
$$
q(x_{1:T}, z|y, x_0) = q_\psi(z|y, x_0)q(x_{1:T}|y, z, x_0)
$$

这里的近似分布由两部分构成：  

- $q_\psi(z|y, x_0)$ 是 VAE 的识别网络，它负责从 $y$ 和 $x_0$ 中推断潜在变量 $z$

- $q(x_{1:T}|y, z, x_0)$ 是扩散模型的前向加噪过程，从 $x_0$ 开始加噪到 $x_{1:T}$。这个分布是已知且不需要参数化，因为前向过程是人为设定的加噪机制。

因此我们将复杂的后验分布分解成了两部分：一部分是 VAE 的后验（学习 $z$），另一部分是一个确定的加噪过程（生成 $x_{1:T}$）

### **数据对数似然**

我们的最终目标是最大化数据 $(x_0, y)$ 的对数似然 $\log p(x_0, y)$：
$$
\log p(x_0, y) = \log \int p(x_{0:T}, y, z) dx_{1:T} dz
$$
这表示，我们需要对联合分布 $p(x_{0:T}, y, z)$ 进行积分，边缘化掉 $x_{1:T}$ 和 $z$。但是，和 VAE 情况类似，在实际中非常困难计算，为此，我们引入了 **证据下界(ELBO)** 来对 $\log p(x_0, y)$ 进行下界估计，以便通过优化 ELBO 来最大化 $\log p(x_0, y)$。

### **证据下界（ELBO）**

ELBO 的定义是：
$$
\begin{aligned}
\log p(x_0, y) \geq & \underbrace{\mathbb{E}_{q_\psi(z|y,x_0)} \big[ p_\theta(y|z) \big] - \mathcal{D}_{KL} \big(q_\psi(z|y,x_0) \| p(z)\big)}_{\mathcal{L}_{\text{VAE}}} 
+ \mathbb{E}_{z \sim q(z|y,x_0)} \left[\underbrace{ \mathbb{E}_{q(x_{1:T}|y,z,x_0)} \left[ \frac{p_\phi(x_{0:T}|y,z)}{q(x_{1:T}|y,z,x_0)} \right] }_{\mathcal{L}_{\text{DDPM}}}\right]
\end{aligned}
$$

上式可以分为两大部分：

1. **VAE 部分 ($\mathcal{L}_{\text{VAE}}$)**：

$$
\mathcal{L}_{\text{VAE}} = \mathbb{E}_{q_\psi(z|y,x_0)} \big[ p_\theta(y|z) \big] - \mathcal{D}_{KL} \big(q_\psi(z|y,x_0) \| p(z)\big)
$$


- 这是标准的 VAE 损失项，包括一个重构项（期望条件下 $z$ 对 $y$ 的重构对数似然）以及 KL 散度项（约束 $q_\psi(z|y,x_0)$ 接近先验 $p(z)$）。
- VAE 部分的优化确保了潜在空间 $z$ 有意义，同时能让解码器在给定 $z$ 的情况下生成合理的 $y$。

2. **DDPM 部分 ($\mathcal{L}_{\text{DDPM}}$)**：

$$
\mathbb{E}_{z \sim q(z|y,x_0)} \left[\underbrace{ \mathbb{E}_{q(x_{1:T}|y,z,x_0)} \left[ \frac{p_\phi(x_{0:T}|y,z)}{q(x_{1:T}|y,z,x_0)} \right] }_{\mathcal{L}_{\text{DDPM}}}\right]
$$

这个公式看起来复杂，但本质上就是衡量逆向去噪分布 $p_\phi$ 和正向加噪分布 $q$ 的匹配程度，通过最大化这个项，扩散模型会学习在给定 $z,y$ 的情况下，成功从高噪声状态“逆推”回 $x_0$。

- **内层期望**：通过对数似然比 $\log \frac{p_\phi}{q}$ 衡量逆向过程和正向过程的分布差异；
- **外层期望**：考虑不同的潜在表示 $z$，确保扩散模型对各种潜在空间的情况都能处理得很好。
- DDPM 部分的训练保证了扩散模型能够在潜在表示与条件信号的辅助下生成高质量的样本 $x_0$

最后，ELBO 即为两部分之和：

$$
\log p(x_0, y) \geq \mathcal{L}_{\text{VAE}} + \mathbb{E}_{z \sim q(z|y,x_0)}[\mathcal{L}_{\text{DDPM}}],
$$

这为联合训练提供了一个可优化的目标函数。

总的来说，DiffuseVAE 的训练目标通过联合分布、后验分布近似以及 ELBO 分解，将 VAE 和扩散模型的损失函数结合起来。这个设计不仅让模型能够生成高质量的样本，还利用了低维潜在空间，提升了生成的控制性和效率。训练过程中，VAE 负责潜在表示的学习，扩散模型负责生成过程的细节还原，两者相辅相成。





### **简化设计**

#### **条件信号 $y$ 的简化**

在原始 DiffuseVAE 的设计中，$y$ 是一个与 $x_0$​ 有关但不完全等同的条件信号。这样做的目的是为扩散模型提供附加条件，从而在逆向去噪过程中能够利用这些信息对生成样本进行更精细的控制。然而，这种设计在实际实现中可能会变得复杂：  

- 模型需要在去噪的每个步骤处理 $(y,z)$ 的条件信息，这增加了运算和实现上的难度。  
- $y$ 的选取和设计也会影响最终模型的性能与适用性，开发者需要思考如何选择合适的 $y$。

因此做了一个重要简化，**假设 $y = x_0$**，也就是说，条件信号直接等于目标图像本身。我们简单来解释他，我们最开始的假想是在去噪的过程中额外输入一个与 $x_0$ 有关但不同的 $y$ 作为去噪的补充信息，比如说，VAE 生成了一个图像，模模糊糊的，我们现在告诉他，你要生成的应该是一只猫，现在我们做了这个简化，相当于去掉了补充信息，而直接将原图像作为补充信息，换句话说，直接给去噪过程标准答案，告诉他，你应该生成的就是这个猫图像类似的新图像。相当于压根就没有什么 $y$ ，研究到最后发现 $y$ 这个创新点没用，但是提都提了，就这么草率的把输入图像作为补充信息了。其实本质上就是用了两次输入图像，一份用来给 VAE 提取特征，一份给DDPM 作为参考答案



#### **潜在表示 $z$ 的简化**

潜在表示$z$ 是由 VAE 编码器从 $(x_0)$ 中获得的潜在表示。在原本的框架中，扩散模型可能需要同时考虑 $(y,z)$ 的条件输入，这意味着扩散模型输入要依赖潜在空间的表达，但是把一个潜在空间作为输入怎么做？没法做! 这就是为什么对潜在空间 $z$ 进行了简化。

简化思路是：既然 $z$ 是从 $x_0$ 映射得到的，那么 VAE 的解码器在给定 $z$ 时生成的 $\hat{x}_0$ 已经是 $z$ 的一个确定性函数。也就是说，$\hat{x}_0$ 完整地代表了 $z$ 中包含的信息。这就使得扩散模型可以直接使用 $\hat{x}_0$ 作为条件参考，而无需显式地输入 $z$。  

我们还是简单的来解释他，其实思路是非常漂亮的，通过 VAE 得到输入，再传给 Diffusion 相当于引入了潜在空间，让 diffusion 充分利用潜在空间，完美的符合了DiffuseVAe 的思想，但是实际实现不出来，因为潜在空间不不是一个显式的数据集，是一个难啃的生数据，我们在 Diffusion 中用不了它，要想用，还是得在 VAE 中把 $z$ 转换成 $\hat{x}_0$ ，这样扩散模型接收的依旧是“图像形式”的条件，而不是抽象的潜在矢量。本质上是什么，本质上就是把 VAE 得到的结果输入给 DDPM 就是这么简单，DDPM中根本就没用潜在空间。

#### 两阶段训练的顺序化与独立性

原本的想法是联合训练 VAE 部分（负责 $z$ 的学习和 $y$ 的重构）以及扩散部分（负责对重构样本做细化），这样可以在理论上实现一个端到端训练：模型同时学会提取潜在表示和利用扩散模型从潜在表示中生成高质量图像。但是! 联合训练在实际实现中非常复杂，参数多，计算量大，互为条件环路，即VAE 的输出是扩散模型的输入，扩散模型的性能又影响 VAE 的参数搜索空间，非常容易导致训练不稳定

为了简化训练过程，我们采用了顺序的两阶段训练方法：

1. 第一阶段只训练 VAE，学会从 $x_0$ 中提取潜在表示 $z$ 并重构出 $\hat{x}_0$。这一步本质是一个标准的 VAE 训练任务，优化相对简单。
2. 第二阶段固定 VAE 的参数（不再训练），只使用 VAE 训练好的模型表示。然后再单独运行扩散模型，这样就不会被 VAE 的动态变化扰动。

说明了什么，说明了之前说的端对端联合优化没能实现，还是只能使用传统思想，固定一个，训练另一个，VAE 提供一个初步结果，而 DDPM 专注于细化这些结果，从而结合两者的优势。



总结一下这三点简化：

- 条件信号 $y$ 被简化为目标数据 $x_0$；

- 第二阶段扩散模型的输入是 VAE 重构结果 $\hat{x}_0$；

- 使用顺序的两阶段训练方式，固定一个，训练另一个。

从这里我们可以看出，理论很美好，实践很困难啊!





到这里 11号



### **VAE 参数化的选择**

在 VAE 的选择上，论文决定只使用最基本、最传统的 VAE 构型（单随机层的简单 VAE），而不是使用更复杂的多层变种：

- 使用复杂的 VAE（比如多层潜在变量、多级别特征抽取）可以潜在地获得更强大的表示能力，但也会使潜在空间更难解释、更不稳定，从而不利于对潜在表示 $z$ 的直观控制和理解。
- 简单的 VAE 虽然表达能力或许弱一些，但潜在空间更易于理解和利用，符合 DiffuseVAE 利用潜在空间来控制全局特性的初衷。

### **DDPM参数化 条件扩散模型两种设计公式**

在将 VAE 重构的 $\hat{x}_0$ 与扩散模型相结合时，有两种主要的设计思路（即本文中所称的两种公式假设），分别在正向与逆向过程的依赖关系上做出了不同的简化。

#### 第一种简化假设：

1. **正向过程的条件独立性**：  

   假设正向加噪过程 $q(x_{1:T}|x_0,z)$ 对 $z$ 和 $\hat{x}_0$ 条件独立，也就是正向过程不需要使用 $z$ 或 $\hat{x}_0$的信息，只从原始数据 $x_0$ 通过标准扩散加噪就行：
   $$
   q(x_{1:T}|x_0, z) \approx q(x_{1:T}|x_0)
   $$
   这样正向过程与潜在表示、VAE 重构结果无关，计算更简单。

2. **逆向过程的依赖性**：  

   逆向去噪过程中，扩散模型仅依赖 VAE 的重构结果 $\hat{x}_0$，而不是直接依赖潜在表示 $z$，即：
   $$
   p(x_{0:T}|z) \approx p(x_{0:T}|\hat{x}_0)
   $$
   这意味着逆向去噪时，每个时间步都使用 $\hat{x}_0$ 作为参考信息，引导从噪声还原出接近 $x_0$ 的图像。  
   
   实现上，可将当前时间步的去噪输入（即 $x_t$）与 $\hat{x}_0$ 拼接，输入给扩散模型的神经网络，让模型在有参照的情况下预测 $x_{t-1}$，从而逐步接近真实图像。
   
   

#### 第二种简化假设：

1. **正向过程的依赖性**：  

   在公式1中，正向过程假设与 $z$ 和 $\hat{x}_0$ 条件独立。而在公式2中，正向过程被设计为显式依赖于 VAE 重构 $\hat{x}_0$：
   $$
   q(x_{1:T}|x_0, z) \approx q(x_{1:T}|\hat{x}_0, x_0)
   $$
   这意味着在每一步加噪过程中，模型会直接结合 $x_0$ 和 $\hat{x}_0$ 的信息，从而让正向过程更加灵活。

2. **逆向过程的依赖性**：  

   和公式1一样，公式2的逆向过程依然假设仅依赖于 VAE 重构：
   $$
   p(x_{0:T}|z) \approx p(x_{0:T}|\hat{x}_0)
   $$
   

### **正向过程的具体设计**

在第二种假设中，正向过程的每一步都显式包含了 VAE 重构 $\hat{x}_0$ 的信息：

1. **第一步 $t = 1$ 的加噪**：
   $$
   q(x_1|x_0, \hat{x}_0) = \mathcal{N}(\sqrt{1-\beta_1}x_0 + \hat{x}_0, \beta_1 \mathbf{I})
   $$
   这一步的设计中，$\sqrt{1-\beta_1}x_0$ 保留了原始数据 $x_0$ 的主要信息，而 $\hat{x}_0$ 为正向过程提供了额外的全局结构信息。

2. **后续时间步 $t > 1$ 的加噪**：
   $$
   q(x_t|x_{t-1}, \hat{x}_0) = \mathcal{N}(\sqrt{1-\beta_t}x_{t-1} + (1-\sqrt{1-\beta_t})\hat{x}_0, \beta_t \mathbf{I})
   $$
   在每个时间步中，$x_t$ 的生成不仅依赖于前一时间步 $x_{t-1}$，还结合了 $\hat{x}_0$ 的全局信息，即 $x_t$ 由原始数据 $x_0$ 和 VAE 重构 $\hat{x}_0$ 的加权和决定的。  

   - $\sqrt{1-\beta_t}x_{t-1}$ 是来自前一时间步的逐步加噪结果；
   - $(1-\sqrt{1-\beta_t})\hat{x}_0$ 是从 VAE 重构注入的额外信息，用于确保每一步都与 $\hat{x}_0$ 保持一致。



### **正向条件边缘分布**

在这种设计下，可以证明，正向过程的条件边缘分布为：
$$
q(x_t|x_0, \hat{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t}x_0 + \hat{x}_0, (1-\bar{\alpha}_t)\mathbf{I})
$$

- 这里的 $\bar{\alpha}_t$ 是正向过程的累积噪声调度系数：

$$
\bar{\alpha}_t = \prod_{i=1}^t (1-\beta_i)
$$



当时间步 $t = T$ 且 $\bar{\alpha}_T \approx 0$（通过合理的噪声调度 $\beta_t$ 实现），正向过程的输出 $x_T$ 的分布接近：
$$
q(x_T|x_0, \hat{x}_0) \approx \mathcal{N}(\hat{x}_0, \mathbf{I})
$$
这意味着最终的 $x_T$ 是以 $\hat{x}_0$ 为均值、单位方差的高斯分布。也就是说，在正向过程中，VAE 重构 $\hat{x}_0$ 成为了加噪数据的核心信息中心。在逆向过程中，其目标是从高斯分布 $\mathcal{N}(\hat{x}_0, \mathbf{I})$ 逐步还原到高质量样本。在这种设计中，$\hat{x}_0$ 的作用非常重要，因为它不仅提供了全局信息，还成为整个分布的基准。

















结果图片显示

Good

| ![output_gpu_0_0_147_12](/Users/zehua/pytorch/DiffuseVAE-main/results_hq/25/images/output_gpu_0_0_147_12.png)![output_gpu_0_0_144_14](/Users/zehua/pytorch/DiffuseVAE-main/results_hq/25/images/output_gpu_0_0_144_14.png) |      |
| ------------------------------------------------------------ | ---- |
| ![output_gpu_0_0_145_13](/Users/zehua/pytorch/DiffuseVAE-main/results_hq/25/images/output_gpu_0_0_145_13.png)![output_gpu_0_0_143_9](/Users/zehua/pytorch/DiffuseVAE-main/results_hq/25/images/output_gpu_0_0_143_9.png) |      |
|                                                              |      |

bad

| ![output_gpu_0_0_156_3](/Users/zehua/pytorch/DiffuseVAE-main/results_hq/25/images/output_gpu_0_0_156_3.png)![output_gpu_0_0_155_11](/Users/zehua/pytorch/DiffuseVAE-main/results_hq/25/images/output_gpu_0_0_155_11.png) |      |
| ------------------------------------------------------------ | ---- |
| ![output_gpu_0_0_155_14](/Users/zehua/pytorch/DiffuseVAE-main/results_hq/25/images/output_gpu_0_0_155_14.png)![output_gpu_0_0_152_7](/Users/zehua/pytorch/DiffuseVAE-main/results_hq/25/images/output_gpu_0_0_152_7.png) |      |
|                                                              |      |











