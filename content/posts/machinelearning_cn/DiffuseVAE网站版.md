---
title: "论文阅读 - DiffuseVAE 详解"
author: "zehua"
date: "2024-11-27T16:25:17+01:00"
lastmod: "2024-11-27T17:12:35+08:00"
draft: false
summary: ""
description: ""
tags: ["机器学习","生成模型","统计分析"]
lang: "zh"
# categories: "posts"
# cover:
#     image: "images/.jpg"
# comments: true
# hideMeta: false
# searchHidden: false
# ShowBreadCrumbs: true
# ShowReadingTime: false
---

# DiffuseVAE 

## **背景问题**：

- **扩散模型**：生成质量高，但存在两个缺点：① 缺乏低维、可解释的潜在空间；② 生成速度较慢。
- **VAE**：具有低维潜在空间，可以解释和操控生成，但生成图像质量较差。

因此提出了 **DiffuseVAE**，将VAE嵌入到扩散模型框架中，并设计了新的条件参数化方法，使扩散模型能够利用VAE推导的低维潜在编码。在标准基准（如 CIFAR-10 和 CelebA-64）上，生成质量优于大多数基于 VAE 的方法，并与最先进模型接近，并独特地保留了对低维潜在空间的访问能力，这是其他模型没有的。



## **VAE和扩散模型的优缺点** 

- **VAE 的特点**：
  - **显式似然生成模型**：以概率分布的方式描述数据生成过程。
  - **低维潜在表示**：通过学习隐空间表示，可以捕捉数据的结构和特征。
  - **灵活性**：适用于多种任务，包括：
    - 解耦表示学习（分离数据中的不同特征）。
    - 半监督学习（结合少量有标注数据进行学习）。
    - 异常检测（识别偏离正常分布的样本）。

- **VAE 的局限性**：
  - **模糊的生成样本**：生成图像往往缺乏高频细节和清晰度。
  - **结构复杂性**：最近的一些改进（如层次化潜在结构）尽管提高了生成质量，但增加了模型复杂度（需要大规模的潜在代码层次化结构）。
  - **与GAN的差距**：相比隐式生成模型（如GAN），VAE在样本质量上仍显逊色，特别是在视觉感知方面。



- **扩散模型（DDPM）的优势和局限性**：
  - **生成质量高**：在多个图像合成任务中表现优异，甚至在某些基准上超越了GANs。
  - **高昂的计算成本**：生成样本时需要多次迭代的逐步采样过程，计算开销大，生成速度慢。
  - **缺乏低维潜在表示**：无法像VAE那样学习和利用低维潜在空间，这限制了其在某些下游任务（如表示解耦、异常检测、可控生成）中的应用。



## **VAE和DDPM复习回顾**

### **变分自编码器 (VAE )**

VAE  是一种利用深度神经网络进行变分推断的生成模型，使用编码器将数据 $x$ 映射到潜在空间表示 $z$，再通过解码器从 $z$ 重建 $x$。它通过最大化数据对数似然 $\log p(x)$ 来学习模型，但由于直接计算对数似然通常不可行，VAE 使用 **证据下界（ELBO）** 来近似优化。 

VAE 的损失函数（ELBO）定义如下：
$$
\mathcal{L}(\theta, \phi) = \underbrace{\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]}_{\text{重建误差}} - \underbrace{D_{KL}[q_\phi(z|x) \| p(z)]}_{\text{KL 散度正则化}}
$$
为了使模型的优化过程可微分，VAE 引入了重参数化技巧，将采样过程从随机变量 $z$ 中分离。例如，将随机变量 $z$ 表示为 $z = \mu + \sigma \cdot \epsilon$，其中 $\epsilon$ 是从标准正态分布采样的噪声。默认先验分布 $p(z)$ 通常设为标准高斯分布，但可以扩展到更复杂的分布（如混合分布或归一化流）以提升模型的表达能力。

更详细内容请看 [VAE](https://zehua.eu/zh/posts/machinelearning_cn/vae/)

### **DDPM**

DDPM 是一种基于潜变量的生成模型， 利用正向过程将数据逐步加噪到高斯分布，再通过逆向过程从噪声中逐步还原数据。包含两个过程：

#### **正向加噪过程**

通过一个固定的高斯马尔可夫链，逐步对数据 $x_0$ 加噪，使其最终接近各向同性高斯分布。正向过程的核心是逐步破坏数据结构，每一步的加噪过程使用高斯分布：
$$
q(x_{1:T}|x_0) = \prod_{t=1}^{T} q(x_t|x_{t-1})
$$
这意味着当前状态 $x_t$ 仅依赖于前一个状态 $x_{t-1}$，整个加噪过程的联合分布可以通过各步的条件分布相乘得到，每一步的加噪过程由高斯分布建模：
$$
q(x_t|x_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t}x_{t-1}, \beta_t\mathbf{I})
$$

- 其中 $\beta_t$ 是噪声调度参数，决定每一步加噪的程度。

通过数学推导，可以从原始数据 $x_0$ 直接生成第 $t$ 步数据：
$$
q(x_t|x_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t}x_0, (1-\bar{\alpha}_t)\mathbf{I})
$$

- 其中 $\bar{\alpha}_t = \prod_{i=1}^t (1-\beta_i)$ 是累积衰减系数。

正向过程是固定的，不需要学习，最终会将数据 $x_0$ 转化为接近高斯分布的 $x_T$。

#### **逆向去噪过程**

逆向过程试图逐步还原数据，采用可学习的高斯马尔可夫链：
$$
p_\theta(x_{t-1}|x_t) = \mathcal{N}(\mu_\theta(x_t, t), \Sigma_\theta(x_t, t)) 
$$

- 其中均值 $\mu_\theta$ 和协方差 $\Sigma_\theta$ 是通过神经网络学习的。

采样时，先从 $p(x_T)$（通常选为各向同性高斯分布）中采样一个噪声向量，再运行逆向过程逐步去噪，生成最终样本 $x_0$。

噪声调度参数 $\beta_t$ 是正向加噪过程的核心，影响生成样本的质量和采样速度。可以是固定值，也可以通过优化学习得到。整个模型通过变分推断优化，目标是最小化正向过程和逆向过程的分布差异。

DDPM 主要优势是生成样本质量高，但缺点是采样速度较慢（需要多次迭代）。这是许多扩散模型后续改进的基础，例如提高采样效率的 DDIM。



## **DiffuseVAE: VAE与扩散模型的结合**

提出的 **DiffuseVAE** 将 VAE 和 DDPM 结合，使生成过程既拥有 VAE 提供的低维潜在空间，又有 DDPM 生成高质量细节的能力。框架中通过两阶段建模，先用 VAE 生成粗略样本，再用 DDPM 细化。通过两阶段建模解决了 VAE 样本模糊和 DDPM 缺乏低维表示的问题。

### **算法框架**：

- **第一阶段**：利用标准 VAE 对任意条件信号 $y$ 进行建模，生成低维潜在表示。
- **第二阶段**：使用条件 DDPM 对训练数据 $x$ 进行建模，条件是 $y$ 和 VAE 生成的低维潜在代码。

该框架可归纳为生成器-细化器框架

- **生成器**：第一阶段通过 VAE 对训练数据 $x$ 拟合，生成重构样本 $\hat{x}$。
- **细化器**：第二阶段通过条件 DDPM 对 VAE 的重构样本 $\hat{x}$ 进行细化建模，从而生成更高质量的样本。

DiffuseVAE 的低维潜在空间能够捕捉生成样本的全局特征(内容、形状或结构)，这就意味着用户可以通过修改这个潜在空间中的变量来直接影响生成样本的主要特性。并且 DiffuseVAE 在速度和质量之间取得了更好的平衡。例如，仅需 10 步采样就能生成质量合理的样本，比传统 DDPM 的效率更高，且生成效果优于相同采样次数的其他方法（如 DDIM）。DiffuseVAE 经过预训练后，对条件信号中的不同噪声类型表现出较好的泛化能力，证明其框架不仅适合特定场景，还能应对多种不确定性条件。

### 

### **训练目标**

首先，我们需要描述 DiffuseVAE 的**联合分布**，它包括以下几个元素：

- $x_0$：我们想生成的高分辨率图像；
- $y$：通过 VAE 建模的辅助条件信号；
- $z$：与 $y$ 相关联的潜在表示；
- $x_{1:T}$：扩散模型学习到的 $T$ 个加噪表示。

联合分布可以分解为：
$$
p(x_{0:T}, y, z) = p(z)p_\theta(y|z)p_\phi(x_{0:T}|y, z)
$$

- **$p(z)$** 是潜在变量的先验分布，通常设为标准高斯分布；

- **$p_\theta(y|z)$** 表示在潜在变量 $z$ 下生成条件信号 $y$ 的概率，这是由 VAE 的解码器学习到的；

- **$p_\phi(x_{0:T}|y, z)$** 是扩散模型的逆向去噪过程，用来从条件信号 $y$ 和潜在表示 $z$ 逐步恢复数据。

### **后验分布**

接下来，我们需要讨论后验分布，也就是在给定 $y$ 和 $x_0$ 的情况下推断 $z$ 和 $x_{1:T}$ 的分布：
$$
p(x_{1:T}, z|y, x_0)
$$
这个后验分布在解析上是不可计算的，因此我们用一个近似分布 $q(x_{1:T}, z|y, x_0)$ 来替代：
$$
q(x_{1:T}, z|y, x_0) = q_\psi(z|y, x_0)q(x_{1:T}|y, z, x_0)
$$

- $q_\psi(z|y, x_0)$ 是 VAE 的识别网络，它负责从 $y$ 和 $x_0$ 中推断潜在变量 $z$

- $q(x_{1:T}|y, z, x_0)$ 是扩散模型的正向过程，通过固定的加噪机制从 $x_0$ 生成 $x_{1:T}$。

因此我们将复杂的后验分布分解成了两部分：一部分由 VAE 处理（$z$ 的推断），另一部分由扩散模型处理（$x_{1:T}$ 的生成）。

### **数据对数似然**

我们的最终目标是最大化数据 $(x_0, y)$ 的对数似然：
$$
\log p(x_0, y) = \log \int p(x_{0:T}, y, z) dx_{1:T} dz
$$
这表示，我们需要对联合分布 $p(x_{0:T}, y, z)$ 进行积分，边缘化掉 $x_{1:T}$ 和 $z$。但是，这个积分在实际中是非常困难的，无法直接计算。

### **证据下界（ELBO）**

为了优化数据的对数似然，  引入了证据下界（ELBO）来近似它：
$$
\begin{aligned}
\log p(x_0, y) \geq & \underbrace{\mathbb{E}_{q_\psi(z|y,x_0)} \big[ p_\theta(y|z) \big] - \mathcal{D}_{KL} \big(q_\psi(z|y,x_0) \| p(z)\big)}_{\mathcal{L}_{\text{VAE}}} 
+ \mathbb{E}_{z \sim q(z|y,x_0)} \left[\underbrace{ \mathbb{E}_{q(x_{1:T}|y,z,x_0)} \left[ \frac{p_\phi(x_{0:T}|y,z)}{q(x_{1:T}|y,z,x_0)} \right] }_{\mathcal{L}_{\text{DDPM}}}\right]
\end{aligned}
$$

##### 前半部分

$$
\underbrace{\mathbb{E}_{q_\psi(z|y,x_0)} \big[ p_\theta(y|z) \big] - \mathcal{D}_{KL} \big(q_\psi(z|y,x_0) \| p(z)\big)}_{\mathcal{L}_{\text{VAE}}}
$$


- **$\mathbb{E}_{q_\psi(z|y, x_0)}[\log p_\theta(y|z)]$**：衡量 VAE 解码器在条件 $z$ 下生成信号 $y$ 的准确性；

- **$-D_{KL}(q_\psi(z|y, x_0) || p(z))$**：通过 KL 散度，约束 VAE 的后验分布与先验分布 $p(z)$ 保持一致；

- **$\mathcal{L}_{VAE}$**：是 VAE 自身的重构损失，确保从 $z$ 和 $y$ 能生成逼真的样本。

##### 后半部分

$$
\mathbb{E}_{z \sim q(z|y,x_0)} \left[\underbrace{ \mathbb{E}_{q(x_{1:T}|y,z,x_0)} \left[ \frac{p_\phi(x_{0:T}|y,z)}{q(x_{1:T}|y,z,x_0)} \right] }_{\mathcal{L}_{\text{DDPM}}}\right]
$$

这个公式看起来复杂，但本质上就是衡量逆向去噪分布 $p_\phi$ 和正向加噪分布 $q$ 的匹配程度：

- **内层期望**：通过对数似然比 $\log \frac{p_\phi}{q}$ 衡量逆向过程和正向过程的分布差异；
- **外层期望**：考虑不同的潜在表示 $z$，确保扩散模型对各种潜在空间的情况都能处理得很好。

总的来说，DiffuseVAE 的训练目标通过联合分布、后验分布近似以及 ELBO 分解，将 VAE 和扩散模型的损失函数结合起来。这个设计不仅让模型能够生成高质量的样本，还利用了低维潜在空间，提升了生成的控制性和效率。训练过程中，VAE 负责潜在表示的学习，扩散模型负责生成过程的细节还原，两者相辅相成。



### **简化设计**

#### **条件信号 $y$ 的简化**

在原始 DiffuseVAE 的设计中，$y$ 是一个辅助信号，用于增强模型的灵活性。但这里做了一个重要简化，**假设 $y = x_0$**，也就是说，条件信号直接等于目标图像本身。

为什么这样做？

- $y$ 和 $x_0$ 之间有一个确定性的映射关系，这样设计简化了模型的条件化处理。
- 在实际中，这意味着我们不需要在扩散过程的逆向阶段（去噪阶段）对 $y$ 进行额外条件化。
- 因此，公式中的 $p_\phi(x_{0:T}|y, z)$ 简化为 $p_\phi(x_{0:T}|z)$，减少了计算复杂度。

#### **条件信号 $z$ 的简化**

潜在表示 $z$ 是通过 VAE 的编码器从数据 $x_0$ 中推导出来的。然而，为了进一步简化，我们**不直接对潜在代码 $z$ 进行条件化**，相反，**将扩散模型的条件信号设为 VAE 重构的 $x_0$**（即 $\hat{x}_0$），这样由于 $\hat{x}_0$ 是 $z$ 的一个确定性函数，因此它包含了 $z$ 所需的信息，同时减少了额外的复杂依赖。

这一设计让模型的第二阶段更加直观：扩散模型的目标是进一步细化 VAE 重构的结果，而不是直接使用潜在空间的表示。

#### **两阶段训练**

为了简化训练过程，我们采用了顺序的两阶段训练方法：

1. **第一阶段**：使用 VAE 对原始数据 $x_0$ 建模，优化 VAE 的损失函数 $\mathcal{L}_{VAE}$，完成对数据 $x_0$ 的潜在表示 $z$ 的学习，并训练解码器生成重构 $\hat{x}_0$。
2. **第二阶段**：固定 VAE 的编码器和解码器（即冻结参数 $\theta$ 和 $\psi$），优化扩散模型的损失函数 $\mathcal{L}_{DDPM}$，用扩散模型对 VAE 重构的结果 $\hat{x}_0$ 进行细化建模，生成最终的高质量样本。

换句话说，本质就是固定一个模型，训练另一个，VAE 提供一个快速生成全局结构的初步结果，而 DDPM 专注于细化这些结果，从而结合两者的优势。



总结一下这三点简化：

- 条件信号 $y$ 被简化为目标数据 $x_0$；

- 第二阶段扩散模型的输入是 VAE 重构结果 $\hat{x}_0$；

- 使用顺序的两阶段训练方式，固定一个，训练另一个。

从这里我们可以看出，理论很美好，实践很困难啊!



### **VAE 参数化的选择**

选择最基本的标准 VAE，即一个带有单一随机层的简单模型，当然也可以扩展到更复杂的 VAE 变种(多个随机层和更复杂的结构)，那么为什么原文没有使用更复杂的 VAE 变种呢？这样的效果不是更好吗？因为多阶段 VAE 的复杂结构可能导致潜在表示变得更难以解释，从而无法再有效利用低维潜在空间进行直接控制，要解决这个问题那就得继续研究了



### **DDPM参数化 条件扩散模型两种设计公式**

#### 第一种简化假设：

1. **正向过程的条件独立性**：  

   假设正向加噪过程与 VAE 重构的结果 $\hat{x}_0$ 和潜在表示 $z$ 是条件独立的，即：
   $$
   q(x_{1:T}|x_0, z) \approx q(x_{1:T}|x_0)
   $$
   这意味着正向加噪的每一步都仅基于原始数据 $x_0$ 进行，而不需要额外依赖 $z$ 或 $\hat{x}_0$ 的信息。这样的独立性假设简化了正向过程的设计和计算。

2. **逆向过程的依赖性**：  

   假设逆向去噪过程中，仅依赖 VAE 的重构结果 $\hat{x}_0$，而不是直接依赖潜在表示 $z$，即：
   $$
   p(x_{0:T}|z) \approx p(x_{0:T}|\hat{x}_0)
   $$
   这表明扩散模型的逆向过程会通过 VAE 提供的重构结果来指导数据还原，而无需直接操作潜在变量 $z$。

在实际实现中，扩散模型在每个时间步 $t$ 的逆向过程会将 VAE 的重构结果 $\hat{x}_0$ 与当前时间步 $t$ 的逆过程表示 $x_t$ 拼接在一起，生成用于计算下一步 $x_{t-1}$ 的输入。这种拼接方法结合了 VAE 提供的全局信息和扩散模型逐步还原的细节，使得模型能够在保留低维表示的同时，逐步提升生成质量。



#### 第二种简化假设：

1. **正向过程的依赖性**：  

   在公式1中，正向过程假设与 $z$ 和 $\hat{x}_0$ 条件独立。而在公式2中，正向过程被设计为显式依赖于 VAE 重构 $\hat{x}_0$：
   $$
   q(x_{1:T}|x_0, z) \approx q(x_{1:T}|\hat{x}_0, x_0)
   $$
   这意味着在每一步加噪过程中，模型会直接结合 $x_0$ 和 $\hat{x}_0$ 的信息，从而让正向过程更加灵活。

2. **逆向过程的依赖性**：  

   和公式1一样，公式2的逆向过程依然假设仅依赖于 VAE 重构：
   $$
   p(x_{0:T}|z) \approx p(x_{0:T}|\hat{x}_0)
   $$
   

### **正向过程的具体设计**

在第二种假设中，正向过程的每一步都显式包含了 VAE 重构 $\hat{x}_0$ 的信息：

1. **第一步 $t = 1$ 的加噪**：
   $$
   q(x_1|x_0, \hat{x}_0) = \mathcal{N}(\sqrt{1-\beta_1}x_0 + \hat{x}_0, \beta_1 \mathbf{I})
   $$
   这一步的设计中，$\sqrt{1-\beta_1}x_0$ 保留了原始数据 $x_0$ 的主要信息，而 $\hat{x}_0$ 为正向过程提供了额外的全局结构信息。

2. **后续时间步 $t > 1$ 的加噪**：
   $$
   q(x_t|x_{t-1}, \hat{x}_0) = \mathcal{N}(\sqrt{1-\beta_t}x_{t-1} + (1-\sqrt{1-\beta_t})\hat{x}_0, \beta_t \mathbf{I})
   $$
   在每个时间步中，$x_t$ 的生成不仅依赖于前一时间步 $x_{t-1}$，还结合了 $\hat{x}_0$ 的全局信息，即 $x_t$ 由原始数据 $x_0$ 和 VAE 重构 $\hat{x}_0$ 的加权和决定的。  

   - $\sqrt{1-\beta_t}x_{t-1}$ 是来自前一时间步的逐步加噪结果；
   - $(1-\sqrt{1-\beta_t})\hat{x}_0$ 是从 VAE 重构注入的额外信息，用于确保每一步都与 $\hat{x}_0$ 保持一致。



### **正向条件边缘分布**

在这种设计下，可以证明，正向过程的条件边缘分布为：
$$
q(x_t|x_0, \hat{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t}x_0 + \hat{x}_0, (1-\bar{\alpha}_t)\mathbf{I})
$$

- 这里的 $\bar{\alpha}_t$ 是正向过程的累积噪声调度系数：

$$
\bar{\alpha}_t = \prod_{i=1}^t (1-\beta_i)
$$



当时间步 $t = T$ 且 $\bar{\alpha}_T \approx 0$（通过合理的噪声调度 $\beta_t$ 实现），正向过程的输出 $x_T$ 的分布接近：
$$
q(x_T|x_0, \hat{x}_0) \approx \mathcal{N}(\hat{x}_0, \mathbf{I})
$$
这意味着最终的 $x_T$ 是以 $\hat{x}_0$ 为均值、单位方差的高斯分布。也就是说，在正向过程中，VAE 重构 $\hat{x}_0$ 成为了加噪数据的核心信息中心。在逆向过程中，其目标是从高斯分布 $\mathcal{N}(\hat{x}_0, \mathbf{I})$ 逐步还原到高质量样本。在这种设计中，$\hat{x}_0$ 的作用非常重要，因为它不仅提供了全局信息，还成为整个分布的基准。





























