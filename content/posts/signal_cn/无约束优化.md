---
title: "无约束优化"
# author: "Zehua"
date: "2024-06-05T16:25:17+01:00"
lastmod: "2024-11-20T17:12:35+08:00"
lang: "zh"
draft: false
summary: "旨在应用迭代优化方法，特别是梯度下降法和牛顿法，通过Matlab来评估这些方法的性能"
description: "本内容为Optimisation实验内容，所有代码和结果是和Darwin Wang Cheou 共同完成的"
tags: ["信号处理","Optimisation"]
# categories: "posts"
#cover:
    #image: "img/signal.png"
# comments: true
# hideMeta: false
searchHidden: true
# ShowBreadCrumbs: true
# ShowReadingTime: false

---

## 问题描述

我们将先研究Rosenbrock函数的最小化问题，其定义如下：

$$x = (x_1, x_2) \in \mathbb{R}^2\quad  \rightarrow f(x) = (1 - x_1)^2 + 100(x_2 - x_1^2)^2$$



涉及的具体内容如下

 (a)  **创建和可视化Rosenbrock函数的二维和三维图像。**

 (b)  **分析函数的性质，特别是其凸性和优化难度。**

 (c)  **计算并实现函数的梯度和Hessian矩阵。**

 (d)  **利用梯度下降法以寻找函数的最小值。**



```matlab
% Creation de la fonction de Rosenbrock
f = @ ( x1 , x2 ) (1 - x1 ) .^2 + 100*( x2 - x1 .^2) .^2;

% Evaluation de la fonction sur une grille de points
x1 = linspace ( -2 ,2 ,150) ;
x2 = linspace ( -.5 ,3 ,150) ;
[X2,X1] = meshgrid(x2,x1) ;
F = f( X1 , X2 ) ;

% Affichage 3D
figure , surf ( x2 , x1 , F ) ;
xlabel ('X1') , ylabel ('X2') , zlabel ('f') ,
shading interp ;
camlight ;
axis tight ;
colormap jet ;

% Af fichage 2 D
figure ,
imagesc ( F ) ;
colormap jet (256) ;
```



​	<img src="/img/optimisation/TP_2.png" alt="Optimisation TP 1" width="85%" />

<p align="center">图1：Rosenbrock函数凸性的3D显示</p>

可以注意到，Rosenbrock函数并非严格凸的，有点类似局部凸的感觉，其形状类似倒置的抛物线。随便在这个范围内两个点之间画一条直线，该直线会超出集合的界限。

因此，优化Rosenbrock函数具有一定难度。下降法可能会很容易陷入局部极小值，或在其周围振荡而无法收敛到全局极小值。



```matlab
figure ,
contour ( x2 , x1 , F ,10)
xlabel ('X1') , ylabel ('X2')
```

​	<img src="/img/optimisation/TP_3.png" alt="Optimisation TP 1" width="85%" />

<p align="center">图2：Rosenbrock函数的等高线图</p>

可以观察到曲线并非均匀分布。它们表示函数的梯度，等高线的密集区域表明梯度更陡（函数值变化更快），而等高线较稀疏的区域表明梯度更缓（函数值变化较慢）。



## 前置计算

我们下面计算函数$f$的梯度

$$f(x_1, x_2) = (1 - x_1)^2 + 100(x_2 - x_1^2)^2$$

梯度的表达方式如下：



$$\nabla f(x) = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2} \right)$$



**计算$\frac{\partial f}{\partial x_1}$：**



$$\frac{\partial f}{\partial x_1} = -2 + 2x_1 - 400x_1x_2 + 400x_1^3$$



**计算$\frac{\partial f}{\partial x_2}$：**



$$\frac{\partial f}{\partial x_2} = 200x_2 - 200x_1^2$$



因此，函数$f$的梯度为：



$$\nabla f(x) = \left( -2 + 2x_1 - 400x_1x_2 + 400x_1^3, ; 200x_2 - 200x_1^2 \right)$$



可以进一步计算Hessian矩阵：


$$
H_f(x) = \begin{pmatrix}2 - 400x_2 + 1200x_1^2 & -400x_1 \\\\ -400x_1 & 200 \end{pmatrix}
$$


下面我们验证函数在$x^* = (1, 1)$处是否有极值，使其梯度为零：

$$\nabla f(x^*) = 0$$

因此需要解以下系统方程：

<div>$$-2 + 2x _1^* - 400x _1^*x_2^* + 400x _1^{*3} = 0$$</div>

$$
200x_2^* - 200x_1^{*2} = 0
$$

可得到:

<div>$$x_1^{*2} = x_2^*$$</div>



**重新排列项后：**

<div>$$-2 + 2x_1^* - 400x_1^*x_2^* + 400x_1^{*3} = 0$$</div>




**得到平衡点：**
$$
x_1^* = x_2^* = 1
$$


在平衡点 $x^* = (1, 1)$ 处计算Hessian矩阵：




$$
H_f(x) = \begin{bmatrix} 
802 & -400 \\\\ 
-400 & 200 
\end{bmatrix}
$$



**我们利用Matlab求解Hessian矩阵的特征值，如果特征值为正，那么可以说明点 $x^{*} = (1, 1)$是一个最小值**

```matlab
Hj = [802 -400; -400 200];
eig(Hj) % lambda i > 0 donc définie positive donc min local
```

$$
\lambda_1 = 0.4
$$


$$
\lambda_2 = 1001.6
$$


因此，两个特征值均为严格正值，表明Hessian矩阵是正定的，点$x^* = (1, 1)$是一个局部最小值。

编程实现梯度 和 Hessian矩阵 

```matlab
% Le gradient de la fonction f
gradf = @ ( x1 , x2 ) [-2 + 2*x1 - 400*x1*x2 + 400*x1^3; 200*x2 - 200*x1^2];
Gradf = @ ( x ) gradf ( x (1) ,x (2) ) ;
% La matrice Hessienne de la fonction f
hessf = @ ( x1 , x2 ) [2 - 400*x2 + 1200*x1^2, -400*x1; -400*x1, 200];
Hessf = @ ( x ) hessf ( x (1) ,x (2) ) ;
```





后续我们将比较两种优化方法：一种是经典梯度下降法，另一种是牛顿法。

## **梯度下降法**



梯度下降法是一种迭代优化技术，用于寻找可微函数的最小值。其基本思想是沿着目标函数梯度的反方向移动，因为这一方向可以使函数值下降最快。其过程如下：

 (a) **选择初始点$x_0$，该点位于函数定义域内。**

 (b) **在每次迭代$k$中，计算函数的梯度$\nabla f(x_k)$。**

 (c) **根据以下规则更新当前点：**




$$
x_{k+1} = x_k - \alpha \cdot \nabla f(x_k)
$$

- 其中，$\alpha$是下降步长（学习率），为控制步长大小的正参数。



 (d) **迭代过程重复，直到梯度足够接近于零（或者达到最大迭代次数）。**

 (e) **在每次迭代中使用相同的$\alpha$。选择合适的$\alpha$至关重要：步长太大会导致算法震荡甚至发散，而步长太小会减慢收敛速度。**



梯度下降法的优点在于其简单易用，并且可以适用于广泛的可微函数和优化问题。然而，它也存在以下缺点：

- **收敛速度可能较慢，特别是对于类似于 Rosenbrock函数 的问题。**

- **不适当的步长选择可能会阻止收敛。**

- **对于非凸函数，梯度下降可能陷入局部最小值。**



**接下来我们将通过编程，在Rosenbrock函数上实现梯度下降法，以确定函数$f$的最小值。**

```matlab
figure, hold on;
contour ( x2 , x1 , F ,10)
alpha = 1e-3;
x0 = [1.5;2.5];
N = 1e4; %10e4;

[x, fx] = descente_gradient(f, Gradf, N, alpha, x0);

plot(x(2,:), x(1,:), 'k', 'linewidth', 1.5);
axis ([ -.5 3 -2 2]) ;
```

```matlab
function [x, fx] = descente_gradient(f, Gradf, N, alpha, x0)
    x = zeros(length(x0), N+1);
    fx = zeros(1, N+1);
    x(:,1) = x0;
    fx(1) = f(x0(1),x0(2));
    for i = 1:N
        x(:,i+1) = x(:,i) - alpha*Gradf(x(:,i));
        fx(i+1) = f(x(1,i+1), x(2,i+1));
    end
end
```

**得到以下图像：**

​	<img src="/img/optimisation/TP_4.png" alt="Optimisation TP 1" width="85%" />

<p align="center">图3：Rosenbrock函数的梯度下降轨迹（$N = 10^4$）</p>

通过图3可以观察到，轨迹一开始出现了明显的震荡，并向最小值方向下移。下降初期的震荡表明在初始几步中很难找到最佳下降方向，这是由于Rosenbrock函数的特性所致。然而，随着轨迹的推进，震荡逐渐减小，路径变得更加规则。这意味着算法开始逐步收敛至全局最小值，该最小值位于等高线较密集的中心区域。



**为了说明迭代次数和步长的重要性，我们尝试不同参数下的下降过程。**

首先，将迭代次数从10000减少到1000。

​	<img src="/img/optimisation/TP_5.png" alt="Optimisation TP 1" width="85%" />

**<p align="center">图4：Rosenbrock函数的梯度下降轨迹（$N = 10^3$）</p>**



正如预期的那样，将迭代次数从$10^4$减少到$10^3$，明显表明算法没有足够的时间显著接近全局最小值。有限的轨迹强调了迭代次数的重要性。足够的迭代次数是保证梯度下降算法能够正确收敛的关键。也就是说，正确设置迭代次数和下降步长对于确保算法适当的收敛非常重要。





​	<img src="/img/optimisation/TP_6.png" alt="Optimisation TP 1" width="85%" />

<p align="center">图5：Rosenbrock函数的梯度下降轨迹（$N = 10^5$）</p>

几乎没有变化。因为迭代次数过大，算法早就稳定并收敛了，即达到极值了。过高的迭代次数是没有意义的。



现在对于梯度下降轨迹已经分析完了，下面调步长$\alpha$，看看它对算法性能的影响，并理解如何影响优化的稳定性、收敛速度以及精度。



​	<img src="/img/optimisation/TP_7.png" alt="Optimisation TP 1" width="85%" />

<p align="center">图6：Rosenbrock函数的梯度下降轨迹（$\alpha = 10^{-2}$）</p>

轨迹不再收敛，开始震荡并发散。步长$\alpha = 10^{-2}$ 过大，远离最优解。

​	<img src="/img/optimisation/TP_8.png" alt="Optimisation TP 1" width="85%" />

<p align="center">图7：Rosenbrock函数的梯度下降轨迹（$\alpha = 10^{-4}$）</p>



步长$\alpha = 10^{-4}$非常小，算法向最小值的推进极慢，和乌龟一样。



**给出最优解**

​	<img src="/img/optimisation/TP_9.png" alt="Optimisation TP 1" width="85%" />

<p align="center">图8：Rosenbrock函数的梯度下降轨迹（$\alpha = 10^{-4}, N = 10^5$）</p>

非常漂亮，不需要解释了



然而，我们也可以从另一个角度解释结果，认为需要在步长和迭代次数之间找到一个良好的平衡，以使算法在有限的迭代次数和适当的步长下收敛到最小值，同时避免过多的震荡，从而节省计算成本和时间。



改进方法:

每次迭代中，不仅考虑当前梯度，还要考虑先前迭代的方向，这可以减少震荡并加速收敛。或者结合其他优化方法，例如牛顿法，其利用了 Hessian矩阵来提高收敛速度和精度。







## **牛顿法**



牛顿法是一种迭代优化技术，用于寻找可导函数的驻点。与梯度下降法不同，牛顿法不仅利用梯度信息，还使用函数曲率信息（Hessian矩阵）来确定下降方向。



其过程如下：

- ​	**选择一个初始点$x_0$，位于函数定义域内。**

- ​	**在每次迭代$k$中，计算函数的梯度$\nabla f(x_k)$和Hessian矩阵$H_f(x_k)$。**

- ​	**根据以下规则更新当前点：**

  $$x_{k+1} = x_k - H_f(x_k)^{-1} \cdot \nabla f(x_k)$$

  - 其中，$H_f(x_k)^{-1}$是函数在点$x_k$处的Hessian矩阵的逆矩阵。

- **重复该过程，直到梯度足够接近于零或更新步长的范数小于预设阈值。**



牛顿法通过利用梯度和曲率信息，让更新更高效、收敛更快。但是，计算成本高，且需要反转Hessian矩阵，并且在函数条件差或非凸时不是很好用。



继续对Rosenbrock函数动手，利用Matlab实现，首先选择$N = 10$和$x_0 = [1.7; 2.7]$

```matlab
figure, hold on;
contour ( x2 , x1 , F ,10)
x0 = [1.7;2.7];
N = 10;

[x, fx] = descente_Newton(f, Gradf, Hessf, N, x0);

plot(x(2,:), x(1,:), 'k', 'linewidth', 1.5);
axis ([ -.5 3 -2 2]);
```

```matlab
function [x, fx] = descente_Newton(f, Gradf, Hessf, N, x0)
    x = zeros(length(x0), N+1);
    fx = zeros(1, N+1);
    x(:,1) = x0;
    fx(1) = f(x0(1),x0(2));
    for i = 1:N
        x(:,i+1) = x(:,i) - inv(Hessf(x(:,i)))*Gradf(x(:,i));
        fx(i+1) = f(x(1,i+1), x(2,i+1));
    end
end
```

​	<img src="/img/optimisation/TP_10.png" alt="Optimisation TP 1" width="85%" />

<p align="center">图9：$f$的变化与范数$E$关于迭代次数的演变</p>

与梯度下降法类似，轨迹从右上角 向 最优点靠近。



牛顿法显示了快速的收敛，其轨迹表现出其步长较大，这就是使用Hessian矩阵的特性，最终收敛到最小值。

```matlab
x_star = [1; 1];
E = sqrt(sum((x - x_star).^2, 1));

% for   i = 1:N
% E(:,i)= norm(x(:,i)-x_star);
% end 

figure;
subplot(2,1,1);
plot(log10(1:N+1), log10(fx));
xlabel('Log10(Iteration)');
ylabel('Log10(Valeur de la fonction)');
title('Congergence de la fonction de Rosenbrock');

subplot(2,1,2);
plot(log10(1:N+1), log10(E));
xlabel('Log10(Iteration)');
ylabel('Log10(Norme de x - x*)');
title('Congergence de la fonction de Newton');

% Affichage en echelle logarithmique
% figure , plot (log10(E(E > eps)));
```

​	<img src="/img/optimisation/TP_11.png" alt="Optimisation TP 1" width="85%" />

**图9：$f$的变化与范数$E$关于迭代次数的演变**

<p align="center">图10：不同初始化条件下解 $x$ 的演变</p>

我们绘制了两个图，用于描述函数$f$和范数$E$随迭代次数的变化。



对于Rosenbrock函数，纵轴表示函数值的对数刻度$\log_{10}$，横轴表示迭代次数的对数刻度$\log_{10}$。可以看到，在大约$10^{0.6}$次迭代后，函数值迅速下降（约4次迭代）。然后曲线趋于稳定，接近$10^{-30}$，表明函数基本收敛到全局最小值。



对于牛顿算法，纵轴表示误差范数$|x - x^*|$的对数刻度（$\log_{10}$），横轴表示迭代次数的对数刻度（$\log_{10}$）。可见误差范数随着迭代次数迅速下降，显示出算法的 快速收敛 性，因为通过利用Hessian矩阵加速了向最优解的下降。最后稳定在极低的值（$10^{-15}$），这表明$x$与$x^*$之间的误差几乎为零，显示牛顿法精确地收敛到最优解。

可见牛顿法在优化Rosenbrock函数时非常不错，快速且精确



​	<img src="/img/optimisation/TP_12.png" alt="Optimisation TP 1" width="85%" />

<p align="center">图10：不同初始化条件下解 $x$ 的演变</p>

我们从三个不同初始点（分别用红色、蓝色和绿色表示）来出发并显示它的轨迹。它们最终收敛到不同的点，但是距离非常接近，特别是绿色和蓝色。这表明牛顿法到达局部最小值或非常接近全局最小值的能力。即使初始条件不同，但是我们可能获得同样的最优解结果，这让初始点的策略选择变得不那么紧张了。



为增强牛顿法的鲁棒性，可以使用技术来稳定Hessian矩阵，或者将算法与线搜索方法结合，不过还是那句话，时代变了，当模型又高纬又复杂，直接上神经网络，近似 Hessian 矩阵或其逆 $H^{-1}$。或者动态学习每一步的最优步长 $\alpha_k$，不用自己纠结选步长。

或者直接暴力一点，传统优化算法都不要了，直接学习一个端到端的优化器，它通过输入 函数的梯度或样本点，直接输出下一步的迭代结果，比如用RNN（递归神经网络）学习迭代规则：
$$
x_{k+1} = \text{RNN}(x_k, \nabla f(x_k), H_f(x_k))
$$
但是一般没有足够的数据。



综上所述，梯度下降法可能收敛速度慢，步长选择要求高。此外，对于局部凸函数，梯度下降法容易陷入局部极小值。因此，牛顿法更适合用于优化Rosenbrock这样的函数，收敛快速且精确，并且对局部凸函数表现良好。然而，牛顿法也有局限性，即计算Hessian矩阵逆的高昂成本。