---
title: "VAE"
author: "zehua"
date: "2024-11-05T16:25:17+01:00"
lastmod: "2024-11-06T17:12:35+08:00"
draft: false
summary: "VAE是一种生成式模型，其核心思想就是概率密度估计问题，属于无监督学习。"
description: ""
tags: ["tst"]
lang: "zh"
# categories: "posts"
# cover:
#     image: "images/.jpg"
# comments: true
# hideMeta: false
# searchHidden: false
# ShowBreadCrumbs: true
# ShowReadingTime: false
---

# VAE

## 1. 无监督学习概述

### 1.1 有监督学习

有监督学习，就是数据集有样本有标签，其目标是学习 $x$ 到 $y$ 的映射，相当于告诉正确答案，如果训练中做错了就根据答案调整一下。

### 1.2 无监督学习

无监督学习，数据没有类标，那就不是映射了，其目标就是找到隐含在数据里的模式和结构，常见任务有：

- **聚类任务**
- **降维**:  三维到二维，且同时保留有用信息。最常用的降维就是主成分分析，是一种线性降维方式
- **特征学习**:  也是一种降维，使用了自编码器结构，将一个输入图像通过神经网络映射到一个低维空间，同时注意，保留的主要信息还得能够重构自己，区别在于这是非线性方法
- **密度估计**：这就是我们下面要重点讲的方面——概率密度函数估计



## 2. 生成模型与概率密度建模

### 2.1 生成模型

**生成模型的定义**

我们给机器一个训练样本集，希望他能产生与训练样本集**同分布**的**新**样本。

比如说有一组训练数据，服从其概率密度分布为 $p_{\text{data}}(x)$，我们希望找到一个模型，其概率密度分布为 $p_{\text{model}}(x)$  ，并且和训练样本的概率密度分布相近（即概率密度函数一样），因此可以从这个 (新构造的) 模型中采样数据从而产生新的样本，并且新样本服从 $p_{\text{model}}(x)$这个概率密度分布。

换句话说，我们用了一个模型去逼近这个原始真实输入数据分布 $p_{\text{data}}(x)$ ，但是注意，真实分布 $p_{\text{data}}(x)$ 是没有办法知道的，因为它太高维了，只能用一个模型去逼近。有了这个模型我们就可以从其中采样了。

总而言之，生成模型通过学习输入数据的的联合概率密度分布，学习数据的生成过程，来产生新的且合理的数据样本

**生成模型分类**

![Generative_Models_en](/img/MachineLearning/VAE/Generative_Models_en.png)

根据概率密度估计方法的不同分为两类：

- **显式的**：估计的分布方程是可以写出来的。换句话说，模型可见。
  - **RNN** 的概率密度函数可定义、也可解。
  - **VAE** 密度函数能定义但是不可解。
- **隐式的**：不要问模型长什么样子，也不要问概率分布是什么样的，反正我能生成样本，你要什么我就给你什么样的新样本。模型不可见。
  - **GAN** 完全不需要密度函数。



### 2.2 概率密度估计建模

**机器学习中的不确定性建模**

机器学习，特别是生成模型中的核心理念之一：**世界上的一切都是不确定的，任何事实发生的背后都有一个对应的概率分布**。机器学习模型（如深度神经网络、生成模型）试图去**建模真实世界中的不确定性**。换句话说，我们认为数据（无论是图像、文本、音频等）都是由某种潜在的概率分布生成的。在生成模型中，我们希望通过学习这个分布来进行新的数据生成。

**任务目标**

因此这也引出了无监督模型里的一个核心问题，概率密度估计问题。

初始的输入数据是有限的样本集，我们的目标是利用这些样本来构建一个生成模型，该模型能够生成与原始数据分布相似的无限新数据。这相当于用一个模型来近似原始数据分布。

因此，我们面临的核心问题是：如何通过有限的样本去近似这个未知的、且可能是高维的原始数据分布。这也是密度估计问题的本质：在已知样本的前提下，估计出样本所属的概率密度函数（PDF）。**即 密度估计的任务就是近似原始数据的概率密度分布。**

**概率建模定义**

对于给定的输入数据，我们通过构建模型来推断出数据背后的潜在生成机制，并推断其最可能的输出数据。

**基本组成部分**

1. **随机变量**：
   - 是一个不确定的变量，离散或连续都可以，它们的值可以通过**概率分布**来描述。
2. **概率分布**：
   - 概率分布定义了随机变量取值的可能性。
   - 对于随机变量，其概率分布是未知的，但是我们可以先入为主 假设其概率分布，例如**正态分布**、**离散分布**或**指数分布**。这种假定的分布就是我们构建的假设空间。
3. **联合分布和条件分布**：
   - **联合分布 **表示多个随机变量同时出现的概率。例如，如果有两个变量 $X$ 和 $Y$，那么联合分布 $P(X, Y)$ 描述了 $X$ 和 $Y$ 同时发生的可能性。
   - **条件分布** 描述在已知某些变量取值的情况下，其他变量的概率分布。比如，$P(X|Y)$ 表示在已知 $Y$ 的情况下，$X$ 的分布。

**监督学习中的概率建模**

在监督学习中，我们希望通过已知的数据样本 $D = {(x_i, y_i)}$ 来学习 $x$ 和 $y$ 之间的条件概率分布 $P(y|x)$。因此在给定新数据 $x$ 时，可以进行预测 $y$ (比如说给定一个猫的图片$x$ ，然后判断是 +1 的概率是多少  0 的概率是多少)。典型的基于概率密度建模的监督学习方法有**逻辑回归**、**朴素贝叶斯**和**贝叶斯网络**。



**非监督学习中的概率建模**

没有标签 y 与数据 x 的对应关系。因此我们需要从 未标注的数据样本 $D = \{x_i\}$  中挖掘数据的潜在结构、分布或者规律。因此概率建模的目标是估计 $P(x)$ ，而不再是条件概率。典型的基于概率密度建模的非监督学习方法有 **高斯混合模型 (GMM)**、 **核密度估计 (KDE)** 和 **变分自动编码器 (VAE)**



**概率建模的步骤**

1. **定义问题**：找到随机变量，并且确定随机变量之间的相关性 (独立与否)，同时找到可观测的随机变量及不可观测的随机变量。
2. **选择模型**：选择一个合适描述这些随机变量分布的概率模型。比如，线性回归、逻辑回归、高斯分布、混合高斯模型等。
3. **推断与学习**：根据观测数据 $X$ 来估计模型参数$\theta$ ，可以通过贝叶斯推断或最大似然估计等方法。
4. **预测与决策**：用训练好的模型对新数据进行预测、分类，或生成与数据分布相似的新样本。



**贝叶斯推断**

**贝叶斯推断**是基于概率建模的重要推断方法。贝叶斯推断方法通过先验分布 $P(\theta)$ 和似然函数 $P(X | \theta)$ 来更新对参数的相信度：
$$
P(\theta | X) = \frac{P(X|\theta)P(\theta)}{P(X)}
$$
其中，$\theta$ 是模型的参数，$X$ 是观测数据。贝叶斯推断是一种动态的过程：**从“先验假设”出发，结合“观测证据”，更新“后验认知”。**



**朴素贝叶斯分类器 (监督学习中的分类问题)**

在朴素贝叶斯模型中，$Y$  是目标变量，表示分类标签 (对于二分类是 0 或 1；或者离散值集合$ \{C_1, C_2, \dots, C_k\} $ 的多分类)。 $X = (x_1, x_2, \dots, x_n)$ ：输入特征，假设互相自独立。我们希望通过学习**条件概率**  $P(Y|X)$ 来对样本 X 进行分类。根

据贝叶斯定理 即条件概率：
$$
P(Y | X) = \frac{P(X | Y) P(Y)}{P(X)}
$$
由于假设特征独立，我们可以将 $P(X|Y)$ 拆解为：
$$
P(X | Y) = P(x_1, x_2, \dots, x_n | Y) = \prod_{i=1}^{n} P(x_i | Y)
$$
将多维特征联合分布的计算转化为单个特征的条件概率的乘积。现在只需要分别计算每个特征 $x_i$ 的条件概率 $P(x_i|Y)$，简化计算。

最后选择能够最大后验概率的分类标签 Y 
$$
\hat{Y} = \arg\max_Y P(Y) \prod_{i=1}^n P(x_i | Y)
$$
**独立性假设** 是朴素贝叶斯的核心假设，现实中不可能完全独立。



**变分自编码器（VAE）**

VAE 是生成模型的一个典型例子，它通过概率建模来学习数据的隐含表示和生成过程。VAE 使用变分推断近似后验分布 Q(Z|X) 来计算对数似然的下界（Evidence Lower Bound，ELBO）：
$$
\log P(X) \geq \mathbb{E}_{Q(Z|X)} \left[\log P(X|Z)\right] - D_{\text{KL}}(Q(Z|X) \| P(Z))
$$
​	•	**第一项：重构损失（**$ \mathbb{E}_{Q(Z|X)}[\log P(X|Z)]$）

表示使用潜在变量 Z  重构原始数据 X 的好坏，衡量生成数据与真实数据的接近程度。

​	•	**第二项：KL 散度项** （$ D_{\text{KL}}(Q(Z|X) \| P(Z)$ )

表示近似后验分布 Q(Z|X) 与先验分布 P(Z) 的差异。通过最小化它，使潜在变量的分布接近设定的先验（通常是标准正态分布）。

先不做展开，后续会极大篇幅讲解VAE原理及数学背景以及必要的证明。



**概率密度建模**

由于后续会大规模讲解潜在变量，因此概率建模会细致到概率密度建模

VAE 的目标之一是学习在给定潜在变量 **Z** 的条件下观测数据 **X** 的概率密度，即 $P(X|Z$ ，因此如何从潜在变量生成观测数据，从而实现对数据分布的近似建模。这就是**概率密度建模** 问题

VAE使用了**变分推断**，由于后验分布 $P(Z|X)$ 复杂，我们要使用优化方法来近似它，其核心思想是 在潜在变量 **Z** 的概率空间中寻找一个最优的近似分布$ Q(Z|X)$，使其尽可能接近真实的后验分布 $P(Z|X)$ 。这同样属于概率密度建模的范畴 (复杂概率分布的近似和优化)

概率密度建模的任务是找到数据的概率密度函数 $P_{\theta}(x)$，但是它必须满足以下两个基本约束条件。

- $P_{\theta}(x) \geq 0, \forall x \in \mathcal{X}$：概率密度函数必须为非负的。
- $\int P_{\theta}(x) dx = 1, \forall \theta \in \Theta$：概率密度函数需要归一化，使得其积分为1（确保总概率为1）。

**概率密度建模的困难**

在概率密度建模中，往往需要对概率密度函数的对数求梯度，即$\nabla_{\theta} \log P_{\theta}(x)$  不容易计算，其具体解释如下

- **高维积分的不可解析性**
  $$
  P(x) = \int P(x, z) dz=\int P(x|z) P(Z) \, dz
  $$
  边缘概率表示通过积分消去潜在变量 $z$ 的影响来得到 $P(x)$。在实际应用中，求解这个积分往往没有解析解 (数据维度高、模型复杂)，直接计算 $P(x)$ 是不可行的

- **梯度的复杂性**

  真实的后验分布 **P(Z|X)** 是难以计算甚至无法表示的，这使得直接基于后验分布进行梯度计算是难实现的

$$
\nabla_\theta \log P_\theta(x) = \frac{\nabla_\theta P_\theta(x)}{P_\theta(x)} 
= \frac{\int \nabla_\theta P_\theta(x|z) P(Z) \, dz}{\int P_\theta(x|z) P(Z) \, dz}
$$

- **高维数据采样生成的复杂性**

  在图像生成任务中，像素空间可能具有数十万维度（ 例如 256×256×3 ），而真实数据分布可能集中在一个复杂的低维流形上。因此从高维分布直接采样既困难又不准确。

### **2.3 假设概率分布**

**2.3.1 高斯分布**

我们通常对概率分布的形式做出某种假设（因为我们本身不知道它的概率分布）。最常用的是 **假设其概率分布是高斯分布**：

- 假设概率分布是多维高斯分布 $\mathcal{N}(\mu_{\theta}(x), \Sigma_{\theta}(x))$，其中均值 $\mu_{\theta}(x)$ 和协方差 $\Sigma_{\theta}(x)$ 是数据 $x$ 的函数，并且由参数 $\theta$ 控制。这个假设简化了模型的计算，因为对于高斯分布，许多操作都有解析解，比如极大似然估计。

- 多维正态分布的概率密度函数（PDF）为：

  $f(z) = \frac{1}{\sqrt{(2\pi)^d |\Sigma_\theta(x)|}} \exp\left( -\frac{(z - \mu_\theta(x))^T (z - \mu_\theta(x))}{2\Sigma_\theta(x)} \right)$

  

  对比一维正态分布的概率密度函数(PDF):

  $f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left( -\frac{(x - \mu)^2}{2\sigma^2} \right)$

**2.3.2 极大似然估计（MLE）**

详细内容见极大似然估计文章

经常使用极大似然估计（MLE）来推断高斯分布的参数 $\mu_{\theta}, \Sigma_{\theta}$ 。其目标是：**给定一个数据 X ，找到能够最大化数据发生概率的模型参数**$\theta$ 。

假设我们有一个概率模型 $P(X|\theta)$，其中 $X$ 是观测到的数据，$\theta$ 是我们希望估计的参数。MLE 的目标是找到一个参数估计值 $\hat{\theta}$，使得在参数  $\hat{\theta}$ 下，数据 $X$ 的似然函数最大化

**2.3.3  VAE 中的概率分布假设**

VAE中有两个重要的概率分布：$Q_{\phi}(Z|X)$ 和 $P_{\theta}(Z|X)$，它们分别表示从数据 $X$ 到潜在变量 $Z$ 的近似后验分布，以及真实后验分布

在后续变分推断的步骤中，VAE将有大量的假设概率分布的步骤

### **2.4 序列建模与自回归模型**

**序列建模**，特别是 **自回归模型（Autoregressive Models）** 是通过条件概率展开来建模高维数据的分布。

2.4.1 条件概率链式法则

对于一个序列化的数据 $X = (x_1, x_2, \dots, x_n)$，我们可以通过 **条件概率链式法则** 来表达整个序列的联合概率分布：

$$
P(x_1, x_2, \dots, x_n) = P(x_1) P(x_2 | x_1) \cdots P(x_n | x_1, \dots, x_{n-1})
$$

序列中所有变量的联合概率分布等于每个变量在给定前面所有变量条件下的条件概率的连乘。

2.4.2 自回归模型的原理和实现

自回归模型核心思想是通过**条件概率分解**来生成序列中的每一个元素。基于条件概率链式法则，每次生成当前元素时都会依赖之前已经生成的元素 ( 每一步的输出用于指导下一步的生成)。

2.4.3 图像生成中自回归模型的挑战

图像**二维空间**数据，而自回归模型本质上是为一维序列设计的。因此，在应用自回归模型于图像生成时，必须将二维像素展平成一维序列。例如生成一张分辨率为 $1920 \times 1080$ 的图片，其像素总数为 $1920 \times 1080 = 2,073,600 $。这意味着模型需要依次生成长度为 207 万的序列。

### 2.5 基于隐变量的生成模型类型

2.5.1 **隐变量** 

隐变量（Latent Variable）是模型中引入的一类**未被直接观测到的变量**。可以捕获数据的潜在结构并且其模型能够实现降维操作。

2.5.2 **基于隐变量模型的核心思想**

基于隐变量的模型会假设观测数据 **X** 是由某些隐变量 **Z** 通过某种概率分布生成的，其中结构包括

- **先验分布** $P(Z) $：

  通常假设隐变量 **Z** 服从某个先验分布，例如标准正态分布 P(Z) \sim \mathcal{N}(0, I) 。即隐变量的初始状态分布。

- **生成过程**$ P(X|Z) $

  在给定隐变量 **Z** 的条件下，观测数据 **X** 的生成过程被建模为条件分布 P(X|Z) 。这个分布可以由神经网络、混合高斯模型等方法建模。

- **后验分布**$ P(Z|X) $

  给定观测数据 **X**，隐变量 **Z** 的分布可以通过贝叶斯公式推断为 P(Z|X) 。这个后验分布基本上解不出来，变分自编码器 VAE 通过引入近似方法来估计。

  

2.5.3 **常见的基于隐变量的模型**

- **混合高斯模型（GMM）**
- **隐马尔可夫模型（HMM）**
- **变分自编码器（VAE**
- **生成对抗网络（GAN）**

2.5.4 **变分自编码器（VAE）基于隐变量的思想**

VAE通过学习数据的潜在表示（隐变量）来生成新的样本，使用了变分推断的思想，将复杂的后验分布近似为简单的高斯分布。其过程简单概述为:

- 编码器将输入数据 $X$ 压缩到隐变量 $Z$ 的概率分布。

- 解码器从隐变量 $Z$ 生成数据 $X$。

- 通过优化损失函数（包括重构误差和KL散度），模型学习到如何从潜在空间生成真实数据。

**2.5.5 扩散模型 (Diffusion Model) 基于隐变量的思想**

扩散模型（Diffusion Model）基于概率和隐变量的思想，利用逐步添加噪声和去噪的过程实现数据生成。这种生成模型通过建模从数据分布到噪声分布的**正向过程**，然后学习其反向**去噪过程**来生成数据。

在这个过程中，**隐变量**是扩散过程中的中间状态  $x_1, x_2, \dots, x_T $，这些状态表示逐步添加噪声后的数据分布，换句话说，正向扩散过程通过向数据逐步添加噪声，将观测变量 $x_0$ 转化为纯噪声 $x_T$ 。这一过程可以视为构造了一组隐变量 $x_1, x_2, \dots, x_T  $  。

详细 内容看 扩散模型  待做

## **第二章：生成模型的数学基础**

详细内容见 生成模型的数学基础



## **第三章：基于隐变量的生成模型**

**3.1 潜在变量的概念**

**1.定义**

**潜在变量**（Latent Variables），在统计学和机器学习中，是指那些**未被直接观测到**的变量，它们隐含在数据背后，控制着可观测变量的行为或模式。好比韩国的财阀，美国的背后集团，看不见老板，但是控制着一切

在很多生成模型中，观测到的数据 X 通常是通过某些潜在的生成过程产生的，而生成这些数据的因素可以通过潜在变量 Z 来表示。

- **观测变量 X**：我们可以直接测量或观测到的数据。
- **潜在变量 Z**：隐含在观测数据背后的未知变量，这些变量控制着数据的生成过程，但我们无法直接观测到它们。

举个简单的例子：在天气预测模型中，我们只能观测到温度、湿度、风速等变量，但得到了这些观测变量 X 的用处不大，也就多穿件衣服，我们真正想知道的是导致它这样的 背后规律，即我们无法直接测量的潜在变量 Z（比如说 大气层的复杂状态、地球表面气流的动态等），它们才是关键因素，找到这些决定气候的潜在因素，就可以控制气候

**2. 高维空间与低维子空间的投影**

我们观察到的世界可能是一个**高维空间**到**低维子空间**的投影。我们观测到的信息 X 并不总是完整的，而只是一些维度的投影。因此，观测到的 X 是潜在变量 Z 的某种反映或表现。



**3. 图像中的潜在变量**

尽管一张图片包含了所有的视觉信息，但很多信息是人脑在处理时通过反馈分析得出的。例如，给定一张图片，计算机看到的全是一堆数字，但是人类会自然而然地分析出图片中的颜色、特征等信息，因为大脑通过识别图片中的像素值从而推测出 潜在特征，这些特征就是潜在变量。

- **在人脸识别任务中**：给定一张人脸图片，潜在变量 Z 可能是皮肤颜色、脸型、发型、眼睛、鼻子的形状等。这些特征不能直接从像素中得知，但通过分析，我们可以推测这些特征控制了图片的生成过程。

- 



**3.2 基于潜在变量的生成模型**

**1. VAE**

在生成模型（如 VAE）中，潜在变量的应用非常重要。VAE 通过**隐变量 Z** 来生成观测数据 X，并通过对潜在变量进行建模来捕获数据的生成机制。具体来说，VAE 中会引入一个潜在变量空间，通过学习 Z 的分布来生成与数据分布相似的样本。

- **Z 是潜在变量**：Z 控制了数据 X 的生成过程。在训练过程中，VAE 会学习如何从一个简单的分布（如标准正态分布）中生成潜在变量 Z，然后通过解码器将 Z 映射到观测数据空间 X 中。
- **生成模型**：通过学习 P(X|Z)，VAE 可以从潜在变量 Z 中生成新的观测样本 X，这使得我们能够生成与真实数据相似的新样本。

**2. 对潜在变量的假设**

- **样本 $(X, Z)$**：

  完整的样本由可观测的变量 $X$ 和不可观测的潜在变量 $Z$ 构成。每一个观测样本 $X$ 对应一个特定的潜在变量 $Z$。潜在变量 $Z$ 控制了生成 $X$ 的过程。

  

- **难以直接优化 $P_\theta(X)$**：

  我们的目标是生成模型中优化 $P_\theta(X)$，但直接优化这个目标很困难，因为它包含积分的形式：

  $$
  P(X) = \int_Z P_\theta(X, Z) dZ = \int_Z P_\theta(X|Z) P_\theta(Z) dZ
  $$

  这个积分在高维空间上难以求解析解，因此通过直接优化 $P(X)$ 来做是不可能的。

- **优化 $P(X|Z)$**：

  如果我们知道了潜在变量 $Z$，那么优化 $P(X, Z)$ 或者说 $P(X|Z)$ 就很容易可，因为在 $Z$ 给定的条件下，$P(X|Z)$ 是直接可优化的。

**3. 基于潜在变量的生成模型**

从上面分析我们可以得出结论，在生成模型中，潜在变量是关键，新数据的生成过程依赖于潜在变量 $Z$ 的表达，找到 Z 我们就找到了一切，即:

- **$Z \rightarrow X$**：

  潜在变量 $Z$ 中隐藏的特征对于生成 $X$ 非常重要。

- **$Z$ 是 $X$ 的编码**：

  在生成模型中，我们通常把潜在变量 $Z$ 当作是观测数据 $X$ 的编码向量。通过对 $Z$ 进行采样并基于条件概率 $P(X|Z)$ 来生成对应的观测数据 $X$。

- **生成过程流程**：

  (a) 首先根据先验分布 (预先假设) 对潜在变量 $Z$ 进行采样。

  (b) 然后根据条件概率 $P(X|Z)$ 对观测数据 $X$ 进行采样。

  (c) 最后，整个过程表示为

  $$
  P(X, Z) = P(Z) P(X|Z)
  $$

### **3.3 高斯混合模型 (GMM) 与 VAE 的联系**

**1. 高斯混合模型 (GMM)**

高斯混合模型是一个生成模型，它假设观测数据是由多个高斯分布的线性组合生成的。模型中每个数据点 $X$ 都可能来自 $K$ 个不同的高斯分布中的某一个。GMM 的概率密度函数 $P(X)$ 为：

$$
P(X) = \sum_{Z}P(Z)P(X|Z) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)
$$

其中：

- $\pi_k$ 是每个高斯成分的**混合系数**，满足 $\sum_{k=1}^{K} \pi_k = 1$。
- $\mathcal{N}(x|\mu_k, \Sigma_k)$ 是第 $k$ 个高斯成分的概率密度函数，$\mu_k$ 为均值，$\Sigma_k$ 为协方差矩阵。
- $Z$ 是潜在变量，表示每个数据点属于哪个高斯成分。

**生成过程：**

1. **选择一个高斯分量**：根据先验 $P(Z)$ 决定 $X$ 来自哪个高斯成分 $k$，这个过程根据混合系数 $\pi_k$ 进行采样。
2. **从该分量采样**：在选择了某个高斯成分 $k$ 之后，接下来基于 $P(X|Z)$ 从该分量的高斯分布 $\mathcal{N}(x|\mu_k, \Sigma_k)$ 中生成 $X$。

**2. GMM 的优化与 one-hot 形式的 $Z$**

直接优化 $\log P(X)$ 是困难的，因为它涉及对多个成分的积分和求和。但我们可以将联合概率 $P(X, Z)$ 写成一个便于优化的形式：

$$
P(X,Z) = \prod_{k=1}^{K} \pi_k^{Z_k} \mathcal{N}(X|\mu_k, \Sigma_k)^{Z_k}
$$

其中 $Z = (Z_1, Z_2, \dots, Z_K)$ 是一个 one-hot 形式的向量，表示样本属于哪个高斯成分 $k$，即对于某个 $Z_k = 1$，其余为 0。

在这种情况下，联合概率的对数形式为：

$$
\log P(X, Z) = \sum_{k=1}^{K} Z_k \left[\log \pi_k + \log \mathcal{N}(X|\mu_k, \Sigma_k)\right]
$$

这个对数似然函数的形式变得容易优化，因为通过引入潜在变量 $Z$，我们可以将复杂的积分和求积简化为求和的形式，便于基于潜在变量的优化。

**3. 变分自编码器 (VAE) 与 GMM 的类比**

VAE 可以被视为一种特殊形式的高斯混合模型，但有一些关键的不同。VAE 也是一种生成模型，其目的是通过学习潜在变量的分布来生成与真实数据相似的样本。VAE 的生成过程如下：

- **潜在变量 $Z$ 的先验分布**：VAE 假设潜在变量 $Z \sim \mathcal{N}(0, I)$，这表示潜在变量是从标准正态分布中采样的。

- **条件分布 $P(X|Z)$**：VAE 假设给定潜在变量 $Z$ 后，观测数据 $X$ 服从高斯分布：
  $$
  P(X|Z) \sim \mathcal{N}(\mu(X), \Sigma(X))
  $$

  这里 $\mu(X)$ 和 $\Sigma(X)$ 是通过神经网络来学习的参数。

- **数据的边缘分布 $P(X)$**：VAE 的目标是近似数据的边缘分布 $P(X)$，这可以表示为：

  $$
  P(X) = \int P(X|Z) P(Z) dZ
  $$

  这个积分表示我们从潜在变量 $Z$ 中生成数据 $X$，而 $Z$ 的分布是标准正态分布 $\mathcal{N}(0, I)$。这种表示是通过无穷多个高斯分布的混合来表示的。

### 







