---
title: "VAE"
author: "zehua"
date: "2024-11-05T16:25:17+01:00"
lastmod: "2024-11-06T17:12:35+08:00"
draft: false
summary: "VAE是一种生成式模型，其核心思想就是概率密度估计问题，属于无监督学习。"
description: ""
tags: ["tst"]
lang: "zh"
# categories: "posts"
# cover:
#     image: "images/.jpg"
# comments: true
# hideMeta: false
# searchHidden: false
# ShowBreadCrumbs: true
# ShowReadingTime: false
---

# VAE

## 1. 无监督学习概述

### 1.1 有监督学习

有监督学习，就是数据集有样本有标签，其目标是学习 $x$ 到 $y$ 的映射，相当于告诉正确答案，如果训练中做错了就根据答案调整一下。

### 1.2 无监督学习

无监督学习，数据没有类标，那就不是映射了，其目标就是找到隐含在数据里的模式和结构，常见任务有：

- **聚类任务**
- **降维**:  三维到二维，且同时保留有用信息。最常用的降维就是主成分分析，是一种线性降维方式
- **特征学习**:  也是一种降维，使用了自编码器结构，将一个输入图像通过神经网络映射到一个低维空间，同时注意，保留的主要信息还得能够重构自己，区别在于这是非线性方法
- **密度估计**：这就是我们下面要重点讲的方面——概率密度函数估计



## 2. 生成模型与概率密度建模

### 2.1 生成模型

**生成模型的定义**

我们给机器一个训练样本集，希望他能产生与训练样本集**同分布**的**新**样本。

比如说有一组训练数据，服从其概率密度分布为 $p_{\text{data}}(x)$，我们希望找到一个模型，其概率密度分布为 $p_{\text{model}}(x)$  ，并且和训练样本的概率密度分布相近（即概率密度函数一样），因此可以从这个 (新构造的) 模型中采样数据从而产生新的样本，并且新样本服从 $p_{\text{model}}(x)$这个概率密度分布。

换句话说，我们用了一个模型去逼近这个原始真实输入数据分布 $p_{\text{data}}(x)$ ，但是注意，真实分布 $p_{\text{data}}(x)$ 是没有办法知道的，因为它太高维了，只能用一个模型去逼近。有了这个模型我们就可以从其中采样了。

总而言之，生成模型通过学习输入数据的的联合概率密度分布，学习数据的生成过程，来产生新的且合理的数据样本

**生成模型分类**

![Generative_Models_en](/img/MachineLearning/VAE/Generative_Models_en.png)

根据概率密度估计方法的不同分为两类：

- **显式的**：估计的分布方程是可以写出来的。换句话说，模型可见。
  - **RNN** 的概率密度函数可定义、也可解。
  - **VAE** 密度函数能定义但是不可解。
- **隐式的**：不要问模型长什么样子，也不要问概率分布是什么样的，反正我能生成样本，你要什么我就给你什么样的新样本。模型不可见。
  - **GAN** 完全不需要密度函数。



### 2.2 概率密度估计建模

**机器学习中的不确定性建模**

机器学习，特别是生成模型中的核心理念之一：**世界上的一切都是不确定的，任何事实发生的背后都有一个对应的概率分布**。机器学习模型（如深度神经网络、生成模型）试图去**建模真实世界中的不确定性**。换句话说，我们认为数据（无论是图像、文本、音频等）都是由某种潜在的概率分布生成的。在生成模型中，我们希望通过学习这个分布来进行新的数据生成。

**任务目标**

因此这也引出了无监督模型里的一个核心问题，概率密度估计问题。

初始的输入数据是有限的样本集，我们的目标是利用这些样本来构建一个生成模型，该模型能够生成与原始数据分布相似的无限新数据。这相当于用一个模型来近似原始数据分布。

因此，我们面临的核心问题是：如何通过有限的样本去近似这个未知的、且可能是高维的原始数据分布。这也是密度估计问题的本质：在已知样本的前提下，估计出样本所属的概率密度函数（PDF）。**即 密度估计的任务就是近似原始数据的概率密度分布。**

**概率建模定义**

对于给定的输入数据，我们通过构建模型来推断出数据背后的潜在生成机制，并推断其最可能的输出数据。

**基本组成部分**

1. **随机变量**：
   - 是一个不确定的变量，离散或连续都可以，它们的值可以通过**概率分布**来描述。
2. **概率分布**：
   - 概率分布定义了随机变量取值的可能性。
   - 对于随机变量，其概率分布是未知的，但是我们可以先入为主 假设其概率分布，例如**正态分布**、**离散分布**或**指数分布**。这种假定的分布就是我们构建的假设空间。
3. **联合分布和条件分布**：
   - **联合分布 **表示多个随机变量同时出现的概率。例如，如果有两个变量 $X$ 和 $Y$，那么联合分布 $P(X, Y)$ 描述了 $X$ 和 $Y$ 同时发生的可能性。
   - **条件分布** 描述在已知某些变量取值的情况下，其他变量的概率分布。比如，$P(X|Y)$ 表示在已知 $Y$ 的情况下，$X$ 的分布。

**监督学习中的概率建模**

在监督学习中，我们希望通过已知的数据样本 $D = {(x_i, y_i)}$ 来学习 $x$ 和 $y$ 之间的条件概率分布 $P(y|x)$。因此在给定新数据 $x$ 时，可以进行预测 $y$ (比如说给定一个猫的图片$x$ ，然后判断是 +1 的概率是多少  0 的概率是多少)。典型的基于概率密度建模的监督学习方法有**逻辑回归**、**朴素贝叶斯**和**贝叶斯网络**。



**非监督学习中的概率建模**

没有标签 y 与数据 x 的对应关系。因此我们需要从 未标注的数据样本 $D = \{x_i\}$  中挖掘数据的潜在结构、分布或者规律。因此概率建模的目标是估计 $P(x)$ ，而不再是条件概率。典型的基于概率密度建模的非监督学习方法有 **高斯混合模型 (GMM)**、 **核密度估计 (KDE)** 和 **变分自动编码器 (VAE)**



**概率建模的步骤**

1. **定义问题**：找到随机变量，并且确定随机变量之间的相关性 (独立与否)，同时找到可观测的随机变量及不可观测的随机变量。
2. **选择模型**：选择一个合适描述这些随机变量分布的概率模型。比如，线性回归、逻辑回归、高斯分布、混合高斯模型等。
3. **推断与学习**：根据观测数据 $X$ 来估计模型参数$\theta$ ，可以通过贝叶斯推断或最大似然估计等方法。
4. **预测与决策**：用训练好的模型对新数据进行预测、分类，或生成与数据分布相似的新样本。



**贝叶斯推断**

**贝叶斯推断**是基于概率建模的重要推断方法。贝叶斯推断方法通过先验分布 $P(\theta)$ 和似然函数 $P(X | \theta)$ 来更新对参数的相信度：
$$
P(\theta | X) = \frac{P(X|\theta)P(\theta)}{P(X)}
$$
其中，$\theta$ 是模型的参数，$X$ 是观测数据。贝叶斯推断是一种动态的过程：**从“先验假设”出发，结合“观测证据”，更新“后验认知”。**



**朴素贝叶斯分类器 (监督学习中的分类问题)**

在朴素贝叶斯模型中，$Y$  是目标变量，表示分类标签 (对于二分类是 0 或 1；或者离散值集合$ \{C_1, C_2, \dots, C_k\} $ 的多分类)。 $X = (x_1, x_2, \dots, x_n)$ ：输入特征，假设互相自独立。我们希望通过学习**条件概率**  $P(Y|X)$ 来对样本 X 进行分类。根

据贝叶斯定理 即条件概率：
$$
P(Y | X) = \frac{P(X | Y) P(Y)}{P(X)}
$$
由于假设特征独立，我们可以将 $P(X|Y)$ 拆解为：
$$
P(X | Y) = P(x_1, x_2, \dots, x_n | Y) = \prod_{i=1}^{n} P(x_i | Y)
$$
将多维特征联合分布的计算转化为单个特征的条件概率的乘积。现在只需要分别计算每个特征 $x_i$ 的条件概率 $P(x_i|Y)$，简化计算。

最后选择能够最大后验概率的分类标签 Y 
$$
\hat{Y} = \arg\max_Y P(Y) \prod_{i=1}^n P(x_i | Y)
$$
**独立性假设** 是朴素贝叶斯的核心假设，现实中不可能完全独立。



**变分自编码器（VAE）**

VAE 是生成模型的一个典型例子，它通过概率建模来学习数据的隐含表示和生成过程。VAE 使用变分推断近似后验分布 Q(Z|X) 来计算对数似然的下界（Evidence Lower Bound，ELBO）：
$$
\log P(X) \geq \mathbb{E}_{Q(Z|X)} \left[\log P(X|Z)\right] - D_{\text{KL}}(Q(Z|X) \| P(Z))
$$
​	•	**第一项：重构损失（**$ \mathbb{E}_{Q(Z|X)}[\log P(X|Z)]$）

表示使用潜在变量 Z  重构原始数据 X 的好坏，衡量生成数据与真实数据的接近程度。

​	•	**第二项：KL 散度项** （$ D_{\text{KL}}(Q(Z|X) \| P(Z)$ )

表示近似后验分布 Q(Z|X) 与先验分布 P(Z) 的差异。通过最小化它，使潜在变量的分布接近设定的先验（通常是标准正态分布）。

先不做展开，后续会极大篇幅讲解VAE原理及数学背景以及必要的证明。



**概率密度建模**

由于后续会大规模讲解潜在变量，因此概率建模会细致到概率密度建模

VAE 的目标之一是学习在给定潜在变量 **Z** 的条件下观测数据 **X** 的概率密度，即 $P(X|Z$ ，因此如何从潜在变量生成观测数据，从而实现对数据分布的近似建模。这就是**概率密度建模** 问题

VAE使用了**变分推断**，由于后验分布 $P(Z|X)$ 复杂，我们要使用优化方法来近似它，其核心思想是 在潜在变量 **Z** 的概率空间中寻找一个最优的近似分布$ Q(Z|X)$，使其尽可能接近真实的后验分布 $P(Z|X)$ 。这同样属于概率密度建模的范畴 (复杂概率分布的近似和优化)

概率密度建模的任务是找到数据的概率密度函数 $P_{\theta}(x)$，但是它必须满足以下两个基本约束条件。

- $P_{\theta}(x) \geq 0, \forall x \in \mathcal{X}$：概率密度函数必须为非负的。
- $\int P_{\theta}(x) dx = 1, \forall \theta \in \Theta$：概率密度函数需要归一化，使得其积分为1（确保总概率为1）。

**概率密度建模的困难**

在概率密度建模中，往往需要对概率密度函数的对数求梯度，即$\nabla_{\theta} \log P_{\theta}(x)$  不容易计算，其具体解释如下

- **高维积分的不可解析性**
  $$
  P(x) = \int P(x, z) dz=\int P(x|z) P(Z) \, dz
  $$
  边缘概率表示通过积分消去潜在变量 $z$ 的影响来得到 $P(x)$。在实际应用中，求解这个积分往往没有解析解 (数据维度高、模型复杂)，直接计算 $P(x)$ 是不可行的

- **梯度的复杂性**

  真实的后验分布 **P(Z|X)** 是难以计算甚至无法表示的，这使得直接基于后验分布进行梯度计算是难实现的

$$
\nabla_\theta \log P_\theta(x) = \frac{\nabla_\theta P_\theta(x)}{P_\theta(x)} 
= \frac{\int \nabla_\theta P_\theta(x|z) P(Z) \, dz}{\int P_\theta(x|z) P(Z) \, dz}
$$

- **高维数据采样生成的复杂性**

  在图像生成任务中，像素空间可能具有数十万维度（ 例如 256×256×3 ），而真实数据分布可能集中在一个复杂的低维流形上。因此从高维分布直接采样既困难又不准确。

### **2.3 假设概率分布**

**2.3.1 高斯分布**

我们通常对概率分布的形式做出某种假设（因为我们本身不知道它的概率分布）。最常用的是 **假设其概率分布是高斯分布**：

- 假设概率分布是多维高斯分布 $\mathcal{N}(\mu_{\theta}(x), \Sigma_{\theta}(x))$，其中均值 $\mu_{\theta}(x)$ 和协方差 $\Sigma_{\theta}(x)$ 是数据 $x$ 的函数，并且由参数 $\theta$ 控制。这个假设简化了模型的计算，因为对于高斯分布，许多操作都有解析解，比如极大似然估计。

- 多维正态分布的概率密度函数（PDF）为：

  $f(z) = \frac{1}{\sqrt{(2\pi)^d |\Sigma_\theta(x)|}} \exp\left( -\frac{(z - \mu_\theta(x))^T (z - \mu_\theta(x))}{2\Sigma_\theta(x)} \right)$

  

  对比一维正态分布的概率密度函数(PDF):

  $f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left( -\frac{(x - \mu)^2}{2\sigma^2} \right)$

**2.3.2 极大似然估计（MLE）**

详细内容见极大似然估计文章

经常使用极大似然估计（MLE）来推断高斯分布的参数 $\mu_{\theta}, \Sigma_{\theta}$ 。其目标是：**给定一个数据 X ，找到能够最大化数据发生概率的模型参数**$\theta$ 。

假设我们有一个概率模型 $P(X|\theta)$，其中 $X$ 是观测到的数据，$\theta$ 是我们希望估计的参数。MLE 的目标是找到一个参数估计值 $\hat{\theta}$，使得在参数  $\hat{\theta}$ 下，数据 $X$ 的似然函数最大化

**2.3.3  VAE 中的概率分布假设**

VAE中有两个重要的概率分布：$Q_{\phi}(Z|X)$ 和 $P_{\theta}(Z|X)$，它们分别表示从数据 $X$ 到潜在变量 $Z$ 的近似后验分布，以及真实后验分布

在后续变分推断的步骤中，VAE将有大量的假设概率分布的步骤

### **2.4 序列建模与自回归模型**

**序列建模**，特别是 **自回归模型（Autoregressive Models）** 是通过条件概率展开来建模高维数据的分布。

2.4.1 条件概率链式法则

对于一个序列化的数据 $X = (x_1, x_2, \dots, x_n)$，我们可以通过 **条件概率链式法则** 来表达整个序列的联合概率分布：

$$
P(x_1, x_2, \dots, x_n) = P(x_1) P(x_2 | x_1) \cdots P(x_n | x_1, \dots, x_{n-1})
$$

序列中所有变量的联合概率分布等于每个变量在给定前面所有变量条件下的条件概率的连乘。

2.4.2 自回归模型的原理和实现

自回归模型核心思想是通过**条件概率分解**来生成序列中的每一个元素。基于条件概率链式法则，每次生成当前元素时都会依赖之前已经生成的元素 ( 每一步的输出用于指导下一步的生成)。

2.4.3 图像生成中自回归模型的挑战

图像**二维空间**数据，而自回归模型本质上是为一维序列设计的。因此，在应用自回归模型于图像生成时，必须将二维像素展平成一维序列。例如生成一张分辨率为 $1920 \times 1080$ 的图片，其像素总数为 $1920 \times 1080 = 2,073,600 $。这意味着模型需要依次生成长度为 207 万的序列。

### 2.5 基于隐变量的生成模型类型

2.5.1 **隐变量** 

隐变量（Latent Variable）是模型中引入的一类**未被直接观测到的变量**。可以捕获数据的潜在结构并且其模型能够实现降维操作。

2.5.2 **基于隐变量模型的核心思想**

基于隐变量的模型会假设观测数据 **X** 是由某些隐变量 **Z** 通过某种概率分布生成的，其中结构包括

- **先验分布** $P(Z) $：

  通常假设隐变量 **Z** 服从某个先验分布，例如标准正态分布 P(Z) \sim \mathcal{N}(0, I) 。即隐变量的初始状态分布。

- **生成过程**$ P(X|Z) $

  在给定隐变量 **Z** 的条件下，观测数据 **X** 的生成过程被建模为条件分布 P(X|Z) 。这个分布可以由神经网络、混合高斯模型等方法建模。

- **后验分布**$ P(Z|X) $

  给定观测数据 **X**，隐变量 **Z** 的分布可以通过贝叶斯公式推断为 P(Z|X) 。这个后验分布基本上解不出来，变分自编码器 VAE 通过引入近似方法来估计。

  

2.5.3 **常见的基于隐变量的模型**

- **混合高斯模型（GMM）**
- **隐马尔可夫模型（HMM）**
- **变分自编码器（VAE**
- **生成对抗网络（GAN）**

2.5.4 **变分自编码器（VAE）基于隐变量的思想**

VAE通过学习数据的潜在表示（隐变量）来生成新的样本，使用了变分推断的思想，将复杂的后验分布近似为简单的高斯分布。其过程简单概述为:

- 编码器将输入数据 $X$ 压缩到隐变量 $Z$ 的概率分布。

- 解码器从隐变量 $Z$ 生成数据 $X$。

- 通过优化损失函数（包括重构误差和KL散度），模型学习到如何从潜在空间生成真实数据。

**2.5.5 扩散模型 (Diffusion Model) 基于隐变量的思想**

扩散模型（Diffusion Model）基于概率和隐变量的思想，利用逐步添加噪声和去噪的过程实现数据生成。这种生成模型通过建模从数据分布到噪声分布的**正向过程**，然后学习其反向**去噪过程**来生成数据。

在这个过程中，**隐变量**是扩散过程中的中间状态  $x_1, x_2, \dots, x_T $，这些状态表示逐步添加噪声后的数据分布，换句话说，正向扩散过程通过向数据逐步添加噪声，将观测变量 $x_0$ 转化为纯噪声 $x_T$ 。这一过程可以视为构造了一组隐变量 $x_1, x_2, \dots, x_T  $  。

详细 内容看 扩散模型  待做

## **第二章：生成模型的数学基础**









