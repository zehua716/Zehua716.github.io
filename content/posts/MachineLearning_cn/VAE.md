---
title: "VAE"
author: "zehua"
date: "2024-11-05T16:25:17+01:00"
lastmod: "2024-11-06T17:12:35+08:00"
draft: false
summary: "VAE是一种生成式模型，其核心思想就是概率密度估计问题，属于无监督学习。"
description: ""
tags: ["tst"]
lang: "zh"
# categories: "posts"
# cover:
#     image: "images/.jpg"
# comments: true
# hideMeta: false
# searchHidden: false
# ShowBreadCrumbs: true
# ShowReadingTime: false
---

# VAE

## 1. 无监督学习概述

### 1.1 有监督学习

有监督学习，就是数据集有样本有标签，其目标是学习 $x$ 到 $y$ 的映射，相当于告诉正确答案，如果训练中做错了就根据答案调整一下。

### 1.2 无监督学习

无监督学习，数据没有类标，那就不是映射了，其目标就是找到隐含在数据里的模式和结构，常见任务有：

- **聚类任务**
- **降维**:  三维到二维，且同时保留有用信息。最常用的降维就是主成分分析，是一种线性降维方式
- **特征学习**:  也是一种降维，使用了自编码器结构，将一个输入图像通过神经网络映射到一个低维空间，同时注意，保留的主要信息还得能够重构自己，区别在于这是非线性方法
- **密度估计**：这就是我们下面要重点讲的方面——概率密度函数估计



## 2. 生成模型与概率密度建模

### 2.1 生成模型

**生成模型的定义**

我们给机器一个训练样本集，希望他能产生与训练样本集**同分布**的**新**样本。

比如说有一组训练数据，服从其概率密度分布为 $p_{\text{data}}(x)$，我们希望找到一个模型，其概率密度分布为 $p_{\text{model}}(x)$  ，并且和训练样本的概率密度分布相近（即概率密度函数一样），因此可以从这个 (新构造的) 模型中采样数据从而产生新的样本，并且新样本服从 $p_{\text{model}}(x)$这个概率密度分布。

换句话说，我们用了一个模型去逼近这个原始真实输入数据分布 $p_{\text{data}}(x)$ ，但是注意，真实分布 $p_{\text{data}}(x)$ 是没有办法知道的，因为它太高维了，只能用一个模型去逼近。有了这个模型我们就可以从其中采样了。

总而言之，生成模型通过学习输入数据的的联合概率密度分布，学习数据的生成过程，来产生新的且合理的数据样本

**生成模型分类**

![Generative_Models_en](/img/MachineLearning/VAE/Generative_Models_en.png)

根据概率密度估计方法的不同分为两类：

- **显式的**：估计的分布方程是可以写出来的。换句话说，模型可见。
  - **RNN** 的概率密度函数可定义、也可解。
  - **VAE** 密度函数能定义但是不可解。
- **隐式的**：不要问模型长什么样子，也不要问概率分布是什么样的，反正我能生成样本，你要什么我就给你什么样的新样本。模型不可见。
  - **GAN** 完全不需要密度函数。



### 2.2 概率密度估计建模

**机器学习中的不确定性建模**

机器学习，特别是生成模型中的核心理念之一：**世界上的一切都是不确定的，任何事实发生的背后都有一个对应的概率分布**。机器学习模型（如深度神经网络、生成模型）试图去**建模真实世界中的不确定性**。换句话说，我们认为数据（无论是图像、文本、音频等）都是由某种潜在的概率分布生成的。在生成模型中，我们希望通过学习这个分布来进行新的数据生成。

**任务目标**

因此这也引出了无监督模型里的一个核心问题，概率密度估计问题。

初始的输入数据是有限的样本集，我们的目标是利用这些样本来构建一个生成模型，该模型能够生成与原始数据分布相似的无限新数据。这相当于用一个模型来近似原始数据分布。

因此，我们面临的核心问题是：如何通过有限的样本去近似这个未知的、且可能是高维的原始数据分布。这也是密度估计问题的本质：在已知样本的前提下，估计出样本所属的概率密度函数（PDF）。**即 密度估计的任务就是近似原始数据的概率密度分布。**

**概率建模定义**

对于给定的输入数据，我们通过构建模型来推断出数据背后的潜在生成机制，并推断其最可能的输出数据。

**基本组成部分**

1. **随机变量**：
   - 是一个不确定的变量，离散或连续都可以，它们的值可以通过**概率分布**来描述。
2. **概率分布**：
   - 概率分布定义了随机变量取值的可能性。
   - 对于随机变量，其概率分布是未知的，但是我们可以先入为主 假设其概率分布，例如**正态分布**、**离散分布**或**指数分布**。这种假定的分布就是我们构建的假设空间。
3. **联合分布和条件分布**：
   - **联合分布 **表示多个随机变量同时出现的概率。例如，如果有两个变量 $X$ 和 $Y$，那么联合分布 $P(X, Y)$ 描述了 $X$ 和 $Y$ 同时发生的可能性。
   - **条件分布** 描述在已知某些变量取值的情况下，其他变量的概率分布。比如，$P(X|Y)$ 表示在已知 $Y$ 的情况下，$X$ 的分布。

**监督学习中的概率建模**

在监督学习中，我们希望通过已知的数据样本 $D = {(x_i, y_i)}$ 来学习 $x$ 和 $y$ 之间的条件概率分布 $P(y|x)$。因此在给定新数据 $x$ 时，可以进行预测 $y$ (比如说给定一个猫的图片$x$ ，然后判断是 +1 的概率是多少  0 的概率是多少)。典型的基于概率密度建模的监督学习方法有**逻辑回归**、**朴素贝叶斯**和**贝叶斯网络**。



**非监督学习中的概率建模**

没有标签 y 与数据 x 的对应关系。因此我们需要从 未标注的数据样本 $D = \{x_i\}$  中挖掘数据的潜在结构、分布或者规律。因此概率建模的目标是估计 $P(x)$ ，而不再是条件概率。典型的基于概率密度建模的非监督学习方法有 **高斯混合模型 (GMM)**、 **核密度估计 (KDE)** 和 **变分自动编码器 (VAE)**



**概率建模的步骤**

1. **定义问题**：找到随机变量，并且确定随机变量之间的相关性 (独立与否)，同时找到可观测的随机变量及不可观测的随机变量。
2. **选择模型**：选择一个合适描述这些随机变量分布的概率模型。比如，线性回归、逻辑回归、高斯分布、混合高斯模型等。
3. **推断与学习**：根据观测数据 $X$ 来估计模型参数$\theta$ ，可以通过贝叶斯推断或最大似然估计等方法。
4. **预测与决策**：用训练好的模型对新数据进行预测、分类，或生成与数据分布相似的新样本。



**贝叶斯推断**

**贝叶斯推断**是基于概率建模的重要推断方法。贝叶斯推断方法通过先验分布 $P(\theta)$ 和似然函数 $P(X | \theta)$ 来更新对参数的相信度：
$$
P(\theta | X) = \frac{P(X|\theta)P(\theta)}{P(X)}
$$
其中，$\theta$ 是模型的参数，$X$ 是观测数据。贝叶斯推断是一种动态的过程：**从“先验假设”出发，结合“观测证据”，更新“后验认知”。**



**朴素贝叶斯分类器 (监督学习中的分类问题)**

在朴素贝叶斯模型中，$Y$  是目标变量，表示分类标签 (对于二分类是 0 或 1；或者离散值集合$ \{C_1, C_2, \dots, C_k\} $ 的多分类)。 $X = (x_1, x_2, \dots, x_n)$ ：输入特征，假设互相自独立。我们希望通过学习**条件概率**  $P(Y|X)$ 来对样本 X 进行分类。根

据贝叶斯定理 即条件概率：
$$
P(Y | X) = \frac{P(X | Y) P(Y)}{P(X)}
$$
由于假设特征独立，我们可以将 $P(X|Y)$ 拆解为：
$$
P(X | Y) = P(x_1, x_2, \dots, x_n | Y) = \prod_{i=1}^{n} P(x_i | Y)
$$
将多维特征联合分布的计算转化为单个特征的条件概率的乘积。现在只需要分别计算每个特征 $x_i$ 的条件概率 $P(x_i|Y)$，简化计算。

最后选择能够最大后验概率的分类标签 Y 
$$
\hat{Y} = \arg\max_Y P(Y) \prod_{i=1}^n P(x_i | Y)
$$
**独立性假设** 是朴素贝叶斯的核心假设，现实中不可能完全独立。



**变分自编码器（VAE）**

VAE 是生成模型的一个典型例子，它通过概率建模来学习数据的隐含表示和生成过程。VAE 使用变分推断近似后验分布 Q(Z|X) 来计算对数似然的下界（Evidence Lower Bound，ELBO）：
$$
\log P(X) \geq \mathbb{E}_{Q(Z|X)} \left[\log P(X|Z)\right] - D_{\text{KL}}(Q(Z|X) \| P(Z))
$$
​	•	**第一项：重构损失（**$ \mathbb{E}_{Q(Z|X)}[\log P(X|Z)]$）

表示使用潜在变量 Z  重构原始数据 X 的好坏，衡量生成数据与真实数据的接近程度。

​	•	**第二项：KL 散度项** （$ D_{\text{KL}}(Q(Z|X) \| P(Z)$ )

表示近似后验分布 Q(Z|X) 与先验分布 P(Z) 的差异。通过最小化它，使潜在变量的分布接近设定的先验（通常是标准正态分布）。

先不做展开，后续会极大篇幅讲解VAE原理及数学背景以及必要的证明。

到这里

**概率密度建模**

由于后续会大规模讲解潜在变量，因此概率建模会细致到概率密度建模

- 比如在VAE中，假设潜在变量 $Z$ 服从某个概率密度函数（例如标准正态分布 $P(Z)$），VAE 需要学习 $P(Z)$ 的条件下观测数据 $X$ 的概率密度，即学习 $P(X|Z)$，这属于**概率密度建模**的部分
- VAE使用了**变分推断**，这是一种通过优化来近似复杂后验分布 $P(Z|X)$ 的方法。这个过程实际上是在隐变量 $Z$ 的概率空间中寻找最优的概率密度分布 $Q(Z|X)$ 来逼近真实后验分布。这也属于**概率密度建模**的部分

- **概率密度函数的约束条件**

  概率密度建模的任务是找到数据的概率密度函数 $P_{\theta}(x)$，但是概率密度函数必须满足以下两个基本条件。

  - **约束条件**：
    - $P_{\theta}(x) \geq 0, \forall x \in \mathcal{X}$：概率密度函数必须为非负的。
    - $\int P_{\theta}(x) dx = 1, \forall \theta \in \Theta$：概率密度函数需要归一化，使得其积分为1（确保总概率为1）。

- **计算对数概率密度的困难**

  - $\nabla_{\theta} \log P_{\theta}(x)$ **不容易计算**：

    - 当我们使用生成模型时，往往需要对概率密度函数的对数进行求导（如用于梯度优化），但是计算复杂，尤其当模型复杂时。

    

- **计算边缘概率的困难**

  - $P(x) = \int P(x, z) dz$ 不容易计算：
    - 边缘概率表示通过积分消去潜在变量 $z$ 的影响来得到 $P(x)$。这一步非常困难，因为在实际应用中，求解这个积分往往没有解析解，特别是当数据维度高、模型复杂时（如深度神经网络）。因此，直接计算 $P(x)$ 是不可行的。

  

- **高维数据采样生成的复杂性**

  1. **采样生成**：
     - 生成模型的重要任务之一是从学到的概率分布中采样生成新的数据点。在某些模型中，直接从分布中采样可能是困难的，尤其是在高维空间或复杂分布的情况下。
     - Autoregressive Model 不适合应用在图像像素方面。



